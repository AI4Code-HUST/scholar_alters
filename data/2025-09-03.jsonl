{"title": "Multimodal Fusion for Vulnerability Detection: Integrating Sequence and Graph-Based Analysis with LLM Augmentation", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Graph"], "data": "NDT Ngan, NH Khoa, VH Pham, PT Duy- 2025 International Conference on, 2025\nDetecting vulnerabilities in source code remains a challenging task due to the \ncomplex and diverse ways security flaws can manifest. This study investigates how to \neffectively combine sequential code semantics and graph-based structural features", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/11133716/11133719/11133833.pdf&hl=en&sa=X&d=18161264188626294235&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b_qzqk9xJ5xCaFpeq6KjmrR&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "2 new citations to articles by Richard Fang", "Bach Le - new related research", "Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Is code coverage of performance tests related to source code features? An empirical study on open-source Java systems", "first_label": ["Code", "Software Testing"], "second_label": [], "data": "M Imran, V Cortellessa, D Di Ruscio, R Rubei, L Traini- Empirical Software, 2025\nPerformance testing aims to ensure the operational efficiency of software systems. \nHowever, many factors influencing the efficacy and adoption of performance tests in \npractice are not yet fully understood. For instance, while code coverage is widely", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10712-3&hl=en&sa=X&d=4859854080979322054&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b9thlXAMwoU1FqGzi8cr7gY&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "Quang-Cuong Bui - new related research", "David Lo - new related research"]}
{"title": "Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization", "first_label": ["Vulnerabilities"], "second_label": [], "data": "C Gao, X Chen, J Wang, J Wang, G Yang- Engineering Applications of Artificial, 2025\nThe increasing complexity of software systems has led to a surge in cybersecurity \nvulnerabilities, necessitating efficient and scalable solutions for vulnerability \nassessment. However, the deployment of large pre-trained models in real-world\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.02840%3F&hl=en&sa=X&d=8585706311454434502&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b8gAdH4KkGQefk07ervBA2M&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Enhancing Fine-Grained Vulnerability Detection with Reinforcement Learning", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "Y Jiang, Z Qu, C Treude, X Su, T Wang- IEEE Transactions on Software Engineering, 2025\nThe rapid growth of vulnerabilities has significantly accelerated the development of \nautomated vulnerability detection methods, especially those based on data-driven \nmodels. However, most of them primarily focus on extracting accurate code \nrepresentations while overlooking the complex vulnerability patterns among \nvulnerable statements, thereby leaving room for improvement. To overcome this \nlimitation, we present a novel reinforcement learning framework (RLFD) for detecting\nCites: Multi-granularity detector for vulnerability fixes", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11145224/&hl=en&sa=X&d=6810495655654990921&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b_OjdD0in8cKtAZUwFCBn2Z&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "2 new citations to articles by Hong Jin Kang", "Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "Thanh Le-Cong - new related research", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "Towards Automatic Vulnerability Management in Open-Source Software", "first_label": ["Vulnerabilities"], "second_label": [], "data": "X Yang - 2025\nOpen-source software (OSS) is the foundation of the digital world, yet its transparent \ndevelopment model exposes systems to security vulnerabilities. Effective OSS \nvulnerability management requires two fundamental capabilities: vulnerability \ndetection (VD), which identifies vulnerable code in repositories before exploitation, \nand vulnerability-fix detection (VFD), which monitors upstream commits to identify \nsecurity patches. While VD prevents vulnerable code from reaching production, VFD\nCites: Multi-granularity detector for vulnerability fixes", "link": "https://scholar.google.com/scholar_url?url=https://mspace.lib.umanitoba.ca/server/api/core/bitstreams/8255ecc7-6cca-4302-b7df-0bdd6d4cf36d/content&hl=en&sa=X&d=2998770214157551647&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b83Aan4rzYfxaHuYDZyKpAs&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=1&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "2 new citations to articles by Richard Fang", "3 new citations to articles by Xin ZHOU", "2 new citations to articles by Hong Jin Kang", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "LLMS vs Agents: Repairing Inter-Procedural Vulnerabilities in Real-World Code", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": ["Repair", "Agent"], "data": "J De La Rosa - 2025\nAutomated vulnerability repair has emerged as a promising alternative to traditional \nrule-based and static-analysis approaches, which are often limited by high false-\npositive rates and incomplete coverage. This thesis investigates the comparative \neffectiveness of fine-tuned large language models (LLMs) and agentic systems for \nboth vulnerability detection and repair, with a particular focus on inter-procedural \nvulnerabilities. Given that commonly used datasets (eg, BigVul, CVEFixes) suffer\nCites: Comparison of static application security testing tools and large\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/ce4c2a94e7d3a81ffceec53535d12515/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=en&sa=X&d=8899563170682244215&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b9mBLioXp8dRdSaJzmp-C9v&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=2&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "3 new citations to articles by Xin ZHOU", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "Smart Contract Vulnerability Detection using Prompt Engineering with Reasoning Models", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection", "Reasoning"], "data": "DT Anh Duc, HG Le, HP Chu-Nguyen, VH Pham- International Conference on, 2025\nThe increasing deployment of smart contracts has drawn significant attention to the \nurgent need for robust and scalable vulnerability detection techniques to mitigate \nsubstantial financial risks associated with their immutable nature on blockchain \nplatforms. This paper introduces structured reasoning prompts using agent-role \nchaining for vulnerability detection that utilizes model capacity to enhance smart \ncontract security through zero-shot and structured prompt engineering without fine\nCites: Large language model for vulnerability detection: Emerging results", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/11133716/11133719/11133797.pdf&hl=en&sa=X&d=4960414742317175043&ei=04i3aPmrBabT6rQPheXy8Aw&scisig=AAZF9b_IfZspn7oTQi1b8ULDalOG&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU"]}
{"title": "Breakpoint: Stress-testing systems-level reasoning in LLM agents", "first_label": ["LLM", "Software Testing"], "second_label": ["Agent", "Reasoning"], "data": "K Hariharan, U Girit, Z Wang, J Andreas- Second Conference on Language Modeling\nBenchmarks for large language models (LLMs) have predominantly assessed short-\nhorizon, localized reasoning. Existing long-horizon suites (eg SWE-lancer) rely on \nmanually curated issues, so expanding or tuning difficulty demands expensive\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DGQNojroNCH&hl=en&sa=X&d=12717619982875316177&ei=0oi3aJX9J_voieoP_t6J0Qg&scisig=AAZF9b-kI0RWDA2GmhCVc8jMru93&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "LLM-based Multi-Agents System Attack via Continuous Optimization with Discrete Efficient Search", "first_label": ["LLM"], "second_label": ["Agent", "Search"], "data": "W Yu, K Hu, T Pang, C Du, M Lin, M Fredrikson- Second Conference on Language Modeling\nLarge Language Model (LLM)-based Multi-Agent Systems (MAS) have demonstrated \nremarkable capability in complex tasks. However, emerging evidence indicates \nsignificant security vulnerabilities within these systems. In this paper, we introduce", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DED5diyzc1C&hl=en&sa=X&d=6490174810296450350&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b8Kua5VPBNgaEu8Asmv2qhJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage", "first_label": ["LLM"], "second_label": ["Agent"], "data": "Y Nie, Z Wang, Y Yu, X Wu, X Zhao, ND Bastian- Second Conference on Language\nRecent studies have discovered that large language models (LLM) may be``fooled''to \noutput private information, including training data, system prompts, and personally \nidentifiable information, under carefully crafted adversarial prompts. Existing red", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DWIfns41MAb&hl=en&sa=X&d=10259244939618973360&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_hZGCbTyeMBbRwykX_3qB9&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Cost of Thinking: Increased Jailbreak Risk in Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "F Yang- arXiv preprint arXiv:2508.10032, 2025\nThinking mode has always been regarded as one of the most valuable modes in \nLLMs. However, we uncover a surprising and previously overlooked phenomenon: \nLLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate 9", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.10032%3F&hl=en&sa=X&d=228728027978274532&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_8cp_f6WJxImUVOGsK56O4&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Gradient Surgery for Safe LLM Fine-Tuning", "first_label": ["LLM"], "second_label": [], "data": "B Yi, J Li, B Zhang, L Nie, T Li, T Huang, Z Liu- arXiv preprint arXiv:2508.07172, 2025\nFine-tuning-as-a-Service introduces a critical vulnerability where a few malicious \nexamples mixed into the user's fine-tuning dataset can compromise the safety \nalignment of Large Language Models (LLMs). While a recognized paradigm frames", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.07172&hl=en&sa=X&d=10352052812613128560&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_-UmhxxnT9CjoTuoEHfdfM&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers", "first_label": [], "second_label": [], "data": "Q Long, Y Deng, L Gan, W Wang, SJ Pan- Second Conference on Language Modeling\nDense retrieval systems have been widely used in various NLP applications. \nHowever, their vulnerabilities to potential attacks have been underexplored. This \npaper investigates a novel attack scenario where the attackers aim to mislead the", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DRsnxggqW4l&hl=en&sa=X&d=12899846075017282411&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b-mEhIe0IKyrwlGeXONAF8S&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Rethinking Safety in LLM Fine-tuning: An Optimization Perspective", "first_label": ["LLM"], "second_label": [], "data": "M Kim, JM Kwak, L Alssum, B Ghanem, P Torr- arXiv preprint arXiv, 2025\nFine-tuning language models is commonly believed to inevitably harm their safety, \nie, refusing to respond to harmful user requests, even when using harmless datasets, \nthus requiring additional safety measures. We challenge this belief through", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.12531&hl=en&sa=X&d=5765344796588629336&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b824RRmgPcbwax6O3_MzYe2&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "first_label": ["LLM"], "second_label": [], "data": "U Maskey, S Yadav, M Dras, U Naseem- arXiv preprint arXiv:2508.11290, 2025\nLLMs increasingly exhibit over-refusal behavior, where safety mechanisms cause \nmodels to reject benign instructions that superficially resemble harmful content. This \nphenomena diminishes utility in production applications that repeatedly rely on", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.11290&hl=en&sa=X&d=4418065858919549735&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_590TBu9nutdVdOeEXPNS5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "M Phute, R Balakrishnan- arXiv preprint arXiv:2508.08521, 2025\nVision Language Models (VLMs) are increasingly being used in a broad range of \napplications, bringing their security and behavioral control to the forefront. While \nexisting approaches for behavioral control or output redirection, like system", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.08521%3F&hl=en&sa=X&d=3389705349380814363&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_q3Y2_Sm63cdpM1VXa1B78&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Unlearning Mirage: A Dynamic Framework for Evaluating LLM Unlearning", "first_label": ["LLM"], "second_label": [], "data": "RS Shah, J Huang, K Murugesan, N Baracaldo- Second Conference on Language\nUnlearning in Large Language Models (LLMs) aims to enhance safety, mitigate \nbiases, and comply with legal mandates, such as the right to be forgotten. However, \nexisting unlearning methods are brittle: minor query modifications, such as multi-hop", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DexW2SFJK4H&hl=en&sa=X&d=15889298358367187024&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b91gDuml9z_8j5vYaea6CHM&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement", "first_label": ["LLM"], "second_label": ["Reasoning"], "data": "Y Shen, Z Huang, Z Guo, Y Liu, G Chen, R Yin- arXiv preprint arXiv, 2025\nThe rapid advancement of large language models (LLMs) has driven their adoption \nacross diverse domains, yet their ability to generate harmful content poses significant \nsafety challenges. While extensive research has focused on mitigating harmful\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20151&hl=en&sa=X&d=341990395345011620&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_juyWDx8SfrGpexsWLbpK6&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Proof Engineering in Logika: Synergistically Integrating Automated and Semi-automated Program Verification", "first_label": ["Verification"], "second_label": [], "data": "S Hallerstedel, JH Robby, J Belt- Formal Methods for Industrial Critical Systems: 30th, 2025\nRecent work on industry-capable program verification tech-nology has emphasized \nthe need for greater predictability in the per-formance of SMT-based automated \nverification approaches. Moreover, foundational limitations of SMT necessitate some\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DfaSBEQAAQBAJ%26oi%3Dfnd%26pg%3DPA39%26ots%3DDrL0CvAkUA%26sig%3DnAGspZVrV0FKadLi2qKIcB7oTL8&hl=en&sa=X&d=16976827903832918058&ei=0oi3aPWRGoTXieoP4q2S-QU&scisig=AAZF9b-ObQYOvjtr5uyfYtNAsjl_&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=3&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Cutting the Root of Hallucination: Structural Trimming for Vulnerability Mitigation in Code LLMs", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": [], "data": "Y Zhang- Second Conference on Language Modeling\nWe introduce a structural perspective on hallucinations in code-generating language \nmodels, framing them as causality anchors in syntax graphs that trigger cascading \nsemantic errors and latent security flaws. This work is the first to systematically\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DdU4Y2sNfJ2&hl=en&sa=X&d=5135073847180578167&ei=04i3aNjJHoXR6rQP27rbgAs&scisig=AAZF9b9_JbzLhczFealkFDO90_BW&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "CALLME: Call Graph Augmentation with Large Language Models for Javascript", "first_label": ["LLM", "Static Analysis"], "second_label": ["Graph"], "data": "M Wang, K Pei, A Solar-Lezama- Second Conference on Language Modeling\nBuilding precise call graphs for Javascript programs is a fundamental build-ing block \nfor many important software engineering and security applications such as bug \ndetection, program repair, and refactoring. However, resolving dynamic calls using\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DxZi2rMUcAO&hl=en&sa=X&d=14269582805361277416&ei=0Yi3aKCoOrOk6rQPk-uVkAE&scisig=AAZF9b86fszM_QdUkoZEP6__MxBh&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Overcoming model optimization bugs in machine learning frameworks", "first_label": ["Bug"], "second_label": [], "data": "H Guan - 2025\nWith the widespread adoption of machine learning models on resource-constrained \nplatforms such as mobile and IoT devices, ensuring efficient and reliable model \noptimization has become increasingly critical. However, optimization processes can \nintroduce model optimization bugs (MOBs), which threaten system reliability and \nremain largely unexplored and under-addressed in current research. This thesis \nprovides the first comprehensive study of MOBs, characterizing their patterns and\nCites: Large language model guided protocol fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://espace.library.uq.edu.au/view/UQ:04c9239/s4643454_phd_thesis.pdf&hl=en&sa=X&d=2154320929638342570&ei=0oi3aML0C4fC6rQPgLuo8Qc&scisig=AAZF9b8KaFUKNMIry9_lIY9kGkYK&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Solidifying AI-enabled Systems via Verification, Testing, and Monitoring", "first_label": ["Verification", "Software Testing"], "second_label": [], "data": "X Xie - 2025\nIn the information era, Artificial Intelligence (AI)-enabled systems are ubiquitous due \nto their scalability, cost-effectiveness, and the availability of diverse operation \nenvironments. They can empower the automation of tasks, analysis of data, and \nrecognition of diverse patterns. However, their trustworthiness, eg, fairness, safety, \nand truthfulness, also encounter enormous challenges when deployed in the real-\nworld environment. Therefore, how to ensure and enhance the trustworthiness of the\nCites: Coverage-based greybox fuzzing as markov chain\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://ualberta.scholaris.ca/bitstreams/136ecc1c-1190-4ced-a982-58e0747c0a8a/download&hl=en&sa=X&d=10164661893200334674&ei=0oi3aML0C4fC6rQPgLuo8Qc&scisig=AAZF9b_hnlld3YIOkPfTdkSRdBYi&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Multi-Agent Penetration Testing AI for the Web", "first_label": ["Software Testing"], "second_label": ["Agent"], "data": "I David, A Gervais- arXiv preprint arXiv:2508.20816, 2025\nAI-powered development platforms are making software creation accessible to a \nbroader audience, but this democratization has triggered a scalability crisis in \nsecurity auditing. With studies showing that up to 40% of AI-generated code contains", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20816&hl=en&sa=X&d=4844919501821999836&ei=yc21aJngLKHN6rQPma25gQI&scisig=AAZF9b_I_7Wp_hGE7c4tW6dSSGgR&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Combining Code Generating Large Language Models and Self-Play to Iteratively Refine Strategies in Games", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Bachrach, E Toledo, K Hambardzumyan, D Magka\nWe propose a self-play approach to generating strategies for playing in multi-player \ngames, where strategies are represented as computer code. We use large language \nmodels (LLMs) to generate pieces of code to play in the game, which we refer to as", "link": "https://scholar.google.com/scholar_url?url=https://ijcai-preprints.s3.us-west-1.amazonaws.com/2025/DM24.pdf&hl=en&sa=X&d=11145484786866310576&ei=yc21aJngLKHN6rQPma25gQI&scisig=AAZF9b9UJ3FFk636jgXvcvyaqXUq&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "first_label": ["LLM", "Fuzzing"], "second_label": [], "data": "M Sun, Y Yang, Y Zhou- arXiv preprint arXiv:2508.20340, 2025\nSatisfiability Modulo Theory (SMT) solvers are foundational to modern systems and \nprogramming languages research, providing the foundation for tasks like symbolic \nexecution and automated verification. Because these solvers sit on the critical path\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20340&hl=en&sa=X&d=1695679758284148071&ei=yc21aJngLKHN6rQPma25gQI&scisig=AAZF9b-bzdShNjYbG_DukyZB6diB&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research", "David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Patch Generation in APR: A Survey from the Perspectives of Utilizing LLMs and Using APR-Specific Information", "first_label": ["LLM"], "second_label": ["Generation"], "data": "Y Yang, C Li, Z Han, R Li, K Xu, Q Li, W Zhong, Z Shen- ACM Transactions on Software\nAutomated Program Repair (APR) is a crucial task in software development and \nmaintenance, aiming to patch software bugs automatically without human \nintervention. The rise of Large Language Models (LLMs) has significantly advanced", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/abs/10.1145/3764584&hl=en&sa=X&d=12095139928200092084&ei=yc21aPLRHYvWieoPpLaliQQ&scisig=AAZF9b9FdXwyjqtbiPQo7S-egQ-4&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=0&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research", "3 new citations to articles by Hong Jin Kang", "2 new citations to articles by Thanh Le-Cong", "5 new citations to articles by Abhik Roychoudhury", "3 new citations to articles by Bach Le", "3 new citations to articles by Xin ZHOU"]}
{"title": "Effect of Deep Recurrent Architectures on Code Vulnerability Detection: Performance Evaluation for SQL Injection in Python", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection"], "data": "A Slotkien, A Poka, P Stefanovi, S Ramanauskait- Electronics, 2025\nSecurity defects in software code can lead to situations that compromise web-based \nsystems, data security, service availability, and the reliability of functionality. \nTherefore, it is crucial to detect code vulnerabilities as early as possible. During the", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2079-9292/14/17/3436&hl=en&sa=X&d=17611221135334497072&ei=yc21aPLRHYvWieoPpLaliQQ&scisig=AAZF9b8E-DzHzklagNrUxwZtKN6z&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=1&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning", "first_label": ["Vulnerabilities"], "second_label": ["Agent", "Reasoning"], "data": "A Lbath, MR Amini, A Delaitre, V Okun- arXiv preprint arXiv:2508.20866, 2025\nThe increasing complexity of software systems and the sophistication of cyber-\nattacks have underscored the critical need for effective automated vulnerability \ndetection and repair systems. Traditional methods, such as static program analysis", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20866&hl=en&sa=X&d=3544592540780461734&ei=yc21aPLRHYvWieoPpLaliQQ&scisig=AAZF9b8p3vPmDSTC1680AMCVumV8&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=2&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Machine Learning Techniques for Automatic Program Repair: A Systematic Literature Mapping", "first_label": ["APR"], "second_label": ["Repair"], "data": "S Domnguez-Isidro, J Snchez-Garca- New Challenges in Software, 2025\nProgram repair involves identifying and fixing issues in a program's source code, \nencompassing error correction, performance improvement, code optimization, and \neven addressing security problems. However, this process can address issues\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-90310-6_32&hl=en&sa=X&d=2492969672266113366&ei=yc21aPLRHYvWieoPpLaliQQ&scisig=AAZF9b-JH-gkbMbo94UsYfG8n1JR&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=3&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research", "5 new citations to articles by Abhik Roychoudhury", "3 new citations to articles by Bach Le"]}
{"title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "first_label": ["LLM", "Code"], "second_label": [], "data": "Z Li, W Chen, J Yu, Z Lu- Expert Systems with Applications, 2025\nEmbedding models have demonstrated strong performance in tasks like clustering, \nretrieval, and feature extraction while offering computational advantages over \ngenerative models and cross-encoders. Benchmarks such as MTEB have shown that", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.19558&hl=en&sa=X&d=15100132484853175061&ei=ys21aN7DJ_voieoP_t6J0Qg&scisig=AAZF9b-2AWIbN3jfOfpV42M5folV&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=0&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Towards Better Correctness and Efficiency in Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "Y Feng, Y Xu, X Xu, B Hui, J Lin- arXiv preprint arXiv:2508.20124, 2025\nWhile code large language models have demonstrated remarkable progress in code \ngeneration, the generated code often exhibits poor runtime efficiency, limiting its \npractical application in performance-sensitive scenarios. To address this limitation", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20124&hl=en&sa=X&d=3240288768383467512&ei=ys21aN7DJ_voieoP_t6J0Qg&scisig=AAZF9b_SzzdGQdI-yKlU8IydAnQ7&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "3 new citations to articles by Hong Jin Kang", "David Lo - new related research"]}
{"title": "Improving Source Code Security: A Novel Approach Using Spectrum of Prompts and Automated State Machine", "first_label": ["Code"], "second_label": [], "data": "CAS Lelis, CAC Marcondes, K Fealey- Brasileiro de Segurana da Informao e de, 2025\nAs software security becomes increasingly vital, automating source code \nvulnerability remediation is essential for enhancing system reliability. This research \npresents an integrated framework that combines large language models (LLMs) with\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://sol.sbc.org.br/index.php/sbseg/article/download/36641/36428/&hl=en&sa=X&d=17468143338563062292&ei=ys21aN7DJ_voieoP_t6J0Qg&scisig=AAZF9b-OBcGLjdqgkLuluUieJGA8&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=3&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "1 new citation to articles by Quang-Cuong Bui", "5 new citations to articles by Abhik Roychoudhury"]}
{"title": "-", "first_label": [], "second_label": [], "data": ",  ,  -   , 2025\n      \n         \n.          \n     \n     \n.       \nCites: Semantic patches for java program transformation\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://ispranproceedings.elpub.ru/jour/article/view/1824&hl=en&sa=X&d=8625546604190736434&ei=yM21aPTBB4fC6rQPgLuo8Qc&scisig=AAZF9b81YrFyoX04UmWZbhGcTdAm&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=2&folt=cit", "author": ["Hong Jin Kang"], "ref": ["3 new citations to articles by Hong Jin Kang"]}
{"title": "Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM", "first_label": ["LLM"], "second_label": ["Generation"], "data": "J Beason, R Cheng, E Schoop, J Nichols- arXiv preprint arXiv:2508.20263, 2025\nIt is challenging to generate the code for a complete user interface using a Large \nLanguage Model (LLM). User interfaces are complex and their implementations often \nconsist of multiple, inter-related files that together specify the contents of each\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20263&hl=en&sa=X&d=16100333742832142127&ei=yM21aOWeO6HN6rQPma25gQI&scisig=AAZF9b-L7XT2cvqAW9ETfxogVnSD&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Interleaving Large Language Models for Compiler Testing", "first_label": ["LLM", "Software Testing"], "second_label": [], "data": "Y Ni, S Li- arXiv preprint arXiv:2508.18955, 2025\nTesting compilers with AI models, especially large language models (LLMs), has \nshown great promise. However, current approaches struggle with two key problems: \nThe generated programs for testing compilers are often too simple, and extensive", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.18955&hl=en&sa=X&d=487666916091373873&ei=ys21aNTINIDXieoPk73huQQ&scisig=AAZF9b-AIHxzif6Q1-QX1fiyTuBb&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "CODE COVERAGE", "first_label": ["Code"], "second_label": [], "data": "-  , 2025\n   ,    \n,  :  ,   .  \n   ,      ,  \n,    .     \n ,     ,    \n.      \nCites: Refining chatgpt-generated code: Characterizing and mitigating\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://cyberleninka.ru/article/n/code-coverage-i-stabilnost-klyuchevye-metriki-uspeha-tehnologicheskoy-komandy&hl=en&sa=X&d=10133539486544816960&ei=x821aIq7HvrUieoPmfaJ2Q4&scisig=AAZF9b__oOw2ALmBoOkA-WmZeuhF&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=1&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["2 new citations to articles by Thanh Le-Cong", "3 new citations to articles by Bach Le"]}
{"title": "JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring", "first_label": [], "second_label": [], "data": "J Chu, M Li, Z Yang, Y Leng, C Lin, C Shen, M Backes- arXiv preprint arXiv, 2025\nAccurately determining whether a jailbreak attempt has succeeded is a fundamental \nyet unresolved challenge. Existing evaluation methods rely on misaligned proxy \nindicators or naive holistic judgments. They frequently misinterpret model responses", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20848&hl=en&sa=X&d=3354477192234789053&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b_OUtU5TwY5gFe7t_EPP97t&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks", "first_label": ["LLM"], "second_label": [], "data": "B Han, F Zhao, D Zhao, G Shen, P Wu, Y Shi, Y Zeng- arXiv preprint arXiv, 2025\nFine-tuning as service injects domain-specific knowledge into large language \nmodels (LLMs), while challenging the original alignment mechanisms and \nintroducing safety risks. A series of defense strategies have been proposed for the", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.09190%3F&hl=en&sa=X&d=4598373852349660026&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b-FE-O4pS9mJhwptehdW8I-&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Automatic LLM Red Teaming", "first_label": ["LLM"], "second_label": [], "data": "R Belaire, A Sinha, P Varakantham- arXiv preprint arXiv:2508.04451, 2025\nRed teaming is critical for identifying vulnerabilities and building trust in current \nLLMs. However, current automated methods for Large Language Models (LLMs) rely \non brittle prompt templates or single-turn attacks, failing to capture the complex", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.04451%3F&hl=en&sa=X&d=2257827261327917536&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b_g0uCpzSGEd0WDaskjDUbX&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "When AIOps Become\" AI Oops\": Subverting LLM-driven IT Operations via Telemetry Manipulation", "first_label": ["LLM"], "second_label": [], "data": "D Pasquini, EM Kornaropoulos, G Ateniese, O Akgul- arXiv preprint arXiv, 2025\nAI for IT Operations (AIOps) is transforming how organizations manage complex \nsoftware systems by automating anomaly detection, incident diagnosis, and \nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.06394&hl=en&sa=X&d=3305610682736444973&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b9VIvh1YIWaVZu12q2YDMRD&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "In-Training Defenses against Emergent Misalignment in Language Models", "first_label": ["LLM"], "second_label": [], "data": "D Kaczr, M Jrgenvg, C Vetter, L Flek, F Mai- arXiv preprint arXiv:2508.06249, 2025\nFine-tuning lets practitioners repurpose aligned large language models (LLMs) for \nnew domains, yet recent work reveals emergent misalignment (EMA): Even a small, \ndomain-specific fine-tune can induce harmful behaviors far outside the target", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.06249%3F&hl=en&sa=X&d=2848574234538730362&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b_Lcm2qHmGdjcK978cplr7k&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "R Wang, X Wang, Y Yao, X Tong, X Ma- arXiv preprint arXiv:2508.01741, 2025\nFine-tuning open-source Vision-Language Models (VLMs) creates a critical yet \nunderexplored attack surface: vulnerabilities in the base VLM could be retained in \nfine-tuned variants, rendering them susceptible to transferable jailbreak attacks. To", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.01741%3F&hl=en&sa=X&d=13634573132391004205&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b-WD8yrAEjxqk5AYRgNWSj1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs", "first_label": ["LLM"], "second_label": [], "data": "W Xing, M Li, C Hu, HXN Zhang, B Lin, M Han- arXiv preprint arXiv:2508.10029, 2025\nLarge language models (LLMs) demonstrate impressive capabilities in various \nlanguage tasks but are susceptible to jailbreak attacks that circumvent their safety \nalignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.10029&hl=en&sa=X&d=15395219468843460694&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b8rU6ujBAfNkA_M5tKiNjJ6&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units", "first_label": ["LLM"], "second_label": [], "data": "C Hao, Z Wang, Y Huang, R Xu, W Niu, X Liu, Z Yu- arXiv preprint arXiv:2508.18763, 2025\nThis paper investigates the enhancement of reasoning capabilities in language \nmodels through token-level multi-model collaboration. Our approach selects the \noptimal tokens from the next token distributions provided by multiple models to", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.18763&hl=en&sa=X&d=7978340676637382802&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b8qsff6JIq9FXQxpoyH5zNr&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach", "first_label": ["LLM"], "second_label": [], "data": "S Liang, Z Xu, J Tao, H Xue, X Wang- arXiv preprint arXiv:2508.09201, 2025\nDespite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain \nvulnerable to jailbreak attacks, posing serious safety risks. Although recent detection \nworks have shifted to internal representations due to their rich cross-modal", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.09201%3F&hl=en&sa=X&d=18315359548988601437&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b-JER6kReVLcbFPLmFoxVC_&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Towards Evaluation for Real-World LLM Unlearning", "first_label": ["LLM"], "second_label": [], "data": "K Miao, Y Hu, X Li, W Bao, Z Liu, Z Qin, K Ren- arXiv preprint arXiv:2508.01324, 2025\nThis paper analyzes the limitations of existing unlearning evaluation metrics in terms \nof practicality, exactness, and robustness in real-world LLM unlearning scenarios. To \novercome these limitations, we propose a new metric called Distribution Correction\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.01324%3F&hl=en&sa=X&d=3951001180431648386&ei=ys21aPbJHbOk6rQPk-uVkAE&scisig=AAZF9b_75D0a0mqlX1m7HkWmVpF1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Understanding the potentially confounding effect of test suite size in test effectiveness evaluation", "first_label": ["Software Testing"], "second_label": [], "data": "Z Lu, Y Wang, Y Rong, Y Huang, X Wang, P Zhang- ACM Transactions on, 2025\nBackground: Code coverage and mutation score serve as pivotal test effectiveness \nmetrics used to assess a test suite's ability to uncover actual defects. However, prior \nresearch has produced inconsistent or even conflicting findings regarding their \ncorrelation with defect detection capability, particularly concerning the impact of test \nsuite size. Problem: The extent of the potentially confounding effect of test suite size \nin test effectiveness evaluation context is not clear, nor is the method to remove the\nCites: Smart greybox fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3748504&hl=en&sa=X&d=17755438009705655820&ei=yc21aJCQDqPWieoPhpqLqAo&scisig=AAZF9b-uFChQan2FjV7k8DKKYbE5&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=3&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "PANGOLIN: a Comprehensive Testing Framework for Configuration-Rich Key-Value Stores", "first_label": ["Software Testing"], "second_label": [], "data": "S Duan, S Kannan, AC Arpaci-Dusseau- Proceedings of the 18th, 2025\nIn this paper, we present Pangolin, a comprehensive testing framework for \nconfiguration-rich key-value stores. To better understand bugs in modern key-value \nstores and explore domain knowledge for efficiently identifying new ones, we first \ncomprehensively study historical bugs in five mature key-value stores during the last \neight years. Then, we design and implement Pangolin, which is motivated by insights \nfrom our bug study, which indicated most bugs could be identified by systematically\nCites: Directed greybox fuzzing\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/abs/10.1145/3757347.3759132&hl=en&sa=X&d=15440488289208500222&ei=yc21aJCQDqPWieoPhpqLqAo&scisig=AAZF9b8qr39eiGYwEq4-hXUPlShh&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=4&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics", "first_label": ["LLM"], "second_label": ["Agent"], "data": "S Fumero, K Huang, M Boffa, D Giordano, M Mellia- arXiv preprint arXiv, 2025\nLarge Language Model (LLM) agents are powerful tools for automating complex \ntasks. In cybersecurity, researchers have primarily explored their use in red-team \noperations such as vulnerability discovery and penetration tests. Defensive uses for \nincident response and forensics have received comparatively less attention and \nremain at an early stage. This work presents a systematic study of LLM-agent design \nfor the forensic investigation of realistic web application attacks. We propose\nCites: CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20643&hl=en&sa=X&d=4157175005797422882&ei=x821aJrwMeDM6rQPycS0-AI&scisig=AAZF9b-EyN-S69V8M8QzOl1x6TfM&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning", "first_label": ["LLM"], "second_label": [], "data": "W Feng, L Wang, T Wei, J Zhang, C Gao, S Zhan, P Lv- arXiv preprint arXiv, 2025\nAs large language models (LLMs) continue to grow in capability, so do the risks of \nharmful misuse through fine-tuning. While most prior studies assume that attackers \nrely on supervised fine-tuning (SFT) for such misuse, we systematically demonstrate \nthat reinforcement learning (RL) enables adversaries to more effectively break safety \nalignment and facilitate advanced harmful task assistance, under matched \ncomputational budgets. To counter this emerging threat, we propose TokenBuncher\nCites: Removing rlhf protections in gpt-4 via fine-tuning", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20697&hl=en&sa=X&d=13446977088965088347&ei=x821aJrwMeDM6rQPycS0-AI&scisig=AAZF9b-dJ0ZBodvXmZXfW2lgSXa0&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=1&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection", "first_label": [], "second_label": [], "data": "HA Shairah, HAAK Hammoud, G Turkiyyah, B Ghanem- arXiv preprint arXiv, 2025\nSafety alignment in Large Language Models (LLMs) often involves mediating \ninternal representations to refuse harmful requests. Recent research has \ndemonstrated that these safety mechanisms can be bypassed by ablating or \nremoving specific representational directions within the model. In this paper, we \npropose the opposite approach: Rank-One Safety Injection (ROSI), a white-box \nmethod that amplifies a model's safety alignment by permanently steering its\nCites: Removing rlhf protections in gpt-4 via fine-tuning", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20766&hl=en&sa=X&d=14789718056790263300&ei=x821aJrwMeDM6rQPycS0-AI&scisig=AAZF9b_KNkaHt4hXMV-7TFQBdUZm&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=2&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Rethinking the Safety Landscape for Foundation Models: A Multi-Modal Perspective", "first_label": [], "second_label": [], "data": "X Li, S Zhao, F Zhao, R Yu\nWith the rise of multi-modal foundation models in domains such as autonomous \ndriving, healthcare, and virtual assistants, safety concerns have become increasingly \nimportant. Unlike uni-modal learning, these models rely on modality alignment and \nfusion to integrate cross-modal informationintroducing novel threats that existing \nsafety frameworks fail to address. Current safety solutions often assume prior \nknowledge of compromised modalities and overlook complex cross-modal\nCites: Removing rlhf protections in gpt-4 via fine-tuning\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://lixi1994.github.io/assets/publications/2025_ICCV_T2FM/paper.pdf&hl=en&sa=X&d=14370337437498939819&ei=x821aJrwMeDM6rQPycS0-AI&scisig=AAZF9b_lE_G7h2U1Gzm2mJozkDiP&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=3&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "BePilot: An AI Programming Assistant for Compiler Backend Development", "first_label": [], "second_label": [], "data": "M Zhong, X Sun, F Lv, L Wang, H Geng, L Qiu, H Cui- ACM Transactions on, 2025\nCompiler backends are tasked with generating executable machine code for various \nprocessors. As the diversity of processors continues to grow, it is imperative for \nprogrammers to tailor specific compiler backends to accommodate each one. \nHowever, compiler backend development remains a labor-intensive and time-\nconsuming process, with limited automation tools available. Although large language \nmodels (LLMs) have demonstrated strong abilities in code completion and\nCites: An LLM-as-Judge Metric for Bridging the Gap with Human", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3764585&hl=en&sa=X&d=57737463069935228&ei=ys21aMy4DIfC6rQPgLuo8Qc&scisig=AAZF9b874V1LRkqdD8Xy7ICAnnvp&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU"]}
{"title": "The Impact of Class Noise-handling on the Effectiveness of Machine Learning-based Methods for Build Outcome and Code Change Request Predictions", "first_label": ["Code", "Code Change"], "second_label": [], "data": "K Al-Sabbagh, M Staron, R Hebig- ACM Transactions on Software Engineering and\nMachine learning-based methods for optimizing build processes and code review \nprocesses are commonly used to accelerate feature delivery to end-users in modern \nsoftware engineering. These methods leverage large volumes of historical code \nchanges to train models on predicting and preventing issues in the code-base that \ncould delay code integrations. The objective of this paper is to improve these \nmethods by reducing the impact of noise on their predictive performance. In this\nCites: Assessing generalizability of codebert\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/abs/10.1145/3764864&hl=en&sa=X&d=6929809996551122379&ei=ys21aMy4DIfC6rQPgLuo8Qc&scisig=AAZF9b-BYbCkanzPUyztydvBsSVR&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=2&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU"]}

{"title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs", "first_label": ["LLM", "Bug"], "second_label": [], "data": "X Li, J Pu, Y Wu, X Zou, S Zhu, Q Wu, Z Zhang, J Hsu- arXiv preprint arXiv, 2025\nOpen-source software projects are foundational to modern software ecosystems, with \nthe Linux kernel standing out as a critical exemplar due to its ubiquity and \ncomplexity. Although security patches are continuously integrated into the Linux", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22796&hl=en&sa=X&d=5877456260761011502&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b_K5bVx20Ft-L-epPjLrGqv&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research", "David Lo - new related research", "Bach Le - new related research"]}
{"title": "Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair", "first_label": ["APR", "LLM", "Bug"], "second_label": ["Repair"], "data": "P Przymus, A Happe, J Cito- arXiv preprint arXiv:2509.05372, 2025\nLarge Language Model (LLM)-based Automated Program Repair (APR) systems are \nincreasingly integrated into modern software development workflows, offering \nautomated patches in response to natural language bug reports. However, this", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.05372&hl=en&sa=X&d=17856431439942945890&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b9iNtxXJTJlEQXG_3JYQbs1&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Richard Fang - new related research"]}
{"title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "K Shehada, Y Wu, WD Feng, A Iyer, G Kumfert, Y Ding- NeurIPS 2025 Workshop on\nLarge Language Models (LLMs) have revolutionized automated program repair \n(APR) but current benchmarks like SWE-Bench predominantly focus on userspace \napplications and overlook the complexities of kernel-space debugging and repair", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DNY4wv5C39G&hl=en&sa=X&d=13284020585308537468&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b8qiQW2WOt5dhxoK0fl5lFh&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Abhik Roychoudhury - new related research", "Quang-Cuong Bui - new related research", "David Lo - new related research", "Bach Le - new related research"]}
{"title": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "H Chang, G Meng, S Xiao, K Chen, K Sun, Y Li- arXiv preprint arXiv:2509.06504, 2025\nWith the growing demand for cross-language codebase migration, evaluating LLMs' \nsecurity implications in translation tasks has become critical. Existing evaluations \nprimarily focus on syntactic or functional correctness at the function level, neglecting", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.06504&hl=en&sa=X&d=15964686870607645440&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b9rJmR42d8IPErh15fFE3f8&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "DeepCodeProbe: Evaluating Code Representation Quality in Models Trained on Code", "first_label": ["Code"], "second_label": [], "data": "V Majdinasab, A Nikanjam, F Khomh- Empirical Software Engineering, 2025\nAbstract Machine Learning models trained on code and artifacts extracted from them \n(eg, version control histories, code differences, etc.), provide invaluable assistance \nfor software engineering tasks. Despite their good performance, there exists a lack of", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10731-0&hl=en&sa=X&d=4894546859526238222&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b9PrlAWMKjBTBOwWDVqSOtc&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=4&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Xin ZHOU - new related research"]}
{"title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "first_label": ["Code"], "second_label": ["Generation"], "data": "Z Sun, C Yang, C Peng, P Gao, X Du, L Li, D Lo- arXiv preprint arXiv:2509.24637, 2025\nLarge Language Models (LLMs) have significantly advanced code completion, yet \nthey often fail when the developer's intent is underspecified in the code context. To \naddress this, developers usually add natural language instructions (eg, comments)", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24637&hl=en&sa=X&d=17305794744039020422&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b-qDU6QAC4PPz9qG-_owbNR&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=5&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research"]}
{"title": "From Cryptic to Clear-Training on LLM Explanations to Detect Smart Contract Vulnerabilities", "first_label": ["Vulnerabilities", "Smart Contracts", "LLM"], "second_label": [], "data": "Y Chen, Z Sun, G Wang, Q Liang, X Yu, D Hao- ACM Transactions on Software, 2025\nSmart contracts have revolutionized the way transactions are executed, offering \ndecentralized and immutable frameworks. The immutability of smart contracts poses \nsignificant risks when vulnerabilities exist in their code, leading to financial losses", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3765753&hl=en&sa=X&d=10696251976758637118&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b9ZBf4NQYTpgMU9yATSPdqf&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=6&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "first_label": ["LLM"], "second_label": ["Agent"], "data": "YA Xiao, P Gao, C Peng, Y Xiong- arXiv preprint arXiv:2509.23586, 2025\nMulti-turn agent systems based on Large Language Models (LLMs) have been \nincreasingly popular for software engineering tasks. While LLM agents show decent \neffectiveness, the high computational cost of input tokens due to the ever-growing", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23586&hl=en&sa=X&d=12596444921036280365&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b-Nsxb9BzRYoIftNViBPFg1&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=7&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Abhik Roychoudhury - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research", "David Lo - new related research", "Bach Le - new related research", "4 new citations to articles by Xin ZHOU"]}
{"title": "Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer", "first_label": ["LLM", "Code"], "second_label": ["Detection"], "data": "T Racharak, C Ragkhitwetsagul, C Junplong- arXiv preprint arXiv, 2025\nRecent studies highlight various machine learning (ML)-based techniques for code \nclone detection, which can be integrated into developer tools such as static code \nanalysis. With the advancements brought by ML in code understanding, ML-based", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22978&hl=en&sa=X&d=13116222657243354012&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b-CrNTuw-vA-xt9R-PWj2rN&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=8&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "GTVD: a multi-level aggregation vulnerability detection method based on full-dependency program graph", "first_label": ["Vulnerabilities"], "second_label": ["Detection", "Graph"], "data": "H He, S Li, Y Li, Y Li- Cluster Computing, 2025\nIn modern software development life cycles, proactive vulnerability discovery and \nremediation play crucial roles in ensuring application security. However, current \ndeep learning-based vulnerability detection methods frequently face limitations due\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10586-025-05506-7&hl=en&sa=X&d=9365356712912853263&ei=OWffaJLDHMnXieoP-o3JmQ0&scisig=AAZF9b_DdZ4fyL1mLj90wo4wpBRq&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=9&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F", "first_label": [], "second_label": ["Reasoning"], "data": "Y He, L Yang, CCG Gonzalo, H Chen- arXiv preprint arXiv:2509.23686, 2025\nLarge Language Models (LLMs) are increasingly integrated into the software \nengineering ecosystem. Their test-time compute (TTC) reasoning capabilities show \nsignificant potential for understanding program logic and semantics beyond mere \ntoken recognition. However, current benchmarks for code reasoning lack a formal, \nprogram-centric deductive framework to ensure sound evaluation, and are incapable \nof assessing whether models genuinely reason about program semantics or merely\nCites: Can LLMs Reason About Program Semantics? A Comprehensive", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23686&hl=en&sa=X&d=7812027363893781818&ei=OGffaNCwHZycieoPgK6aoQo&scisig=AAZF9b921YNdLGMQG7M6UBXqYA2i&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["4 new citations to articles by Thanh Le-Cong", "6 new citations to articles by Bach Le"]}
{"title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "first_label": ["LLM", "Fuzzing", "Software Testing"], "second_label": [], "data": "Y Zhao, M Wu, X Hu, X Xia- arXiv preprint arXiv:2509.23835, 2025\nLarge Language Models (LLMs) are widely used for code generation, but they face \ncritical security risks when applied to practical production due to package \nhallucinations, in which LLMs recommend non-existent packages. These \nhallucinations can be exploited in software supply chain attacks, where malicious \nattackers exploit them to register harmful packages. It is critical to test LLMs for \npackage hallucinations to mitigate package hallucinations and defend against\nCites: Refining chatgpt-generated code: Characterizing and mitigating", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23835&hl=en&sa=X&d=16917587561815760773&ei=OGffaNCwHZycieoPgK6aoQo&scisig=AAZF9b8Mk-rnOTcKAAGqO9tIlO4J&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=1&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["4 new citations to articles by Thanh Le-Cong", "6 new citations to articles by Bach Le", "Abhik Roychoudhury - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research", "David Lo - new related research", "10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Automated Vulnerability Validation and Verification: A Large Language Model Approach", "first_label": ["Vulnerabilities", "Verification", "LLM"], "second_label": [], "data": "A Lotfi, C Katsis, E Bertino- arXiv preprint arXiv:2509.24037, 2025\nSoftware vulnerabilities remain a critical security challenge, providing entry points for \nattackers into enterprise networks. Despite advances in security practices, the lack of \nhigh-quality datasets capturing diverse exploit behavior limits effective vulnerability \nassessment and mitigation. This paper introduces an end-to-end multi-step pipeline \nleveraging generative AI, specifically large language models (LLMs), to address the \nchallenges of orchestrating and reproducing attacks to known software\nCites: Refining chatgpt-generated code: Characterizing and mitigating", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24037&hl=en&sa=X&d=7978409907240033186&ei=OGffaNCwHZycieoPgK6aoQo&scisig=AAZF9b8QoVaounWNCXofucwywvLP&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=2&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["4 new citations to articles by Thanh Le-Cong", "6 new citations to articles by Bach Le"]}
{"title": "Metamorphic Testing of Deep Code Models: A Systematic", "first_label": ["Code", "Software Testing"], "second_label": [], "data": "ALI ASGARI, M DE KONING, P DERAKHSHANFAR - 2025\nIn recent years, large language models for code (LLM4Code) have achieved \nremarkable performance across a range of software engineering tasks, reaching \naccuracy levels that make them increasingly viable for real-world adoption. These \ntasks include, but are not limited to, program repair [1], vulnerability detection [2, 3], \ncode completion [4, 5], and clone detection [6, 7], among others. However, the \npractical applicability of LLM4Code depends not only on their performance on\nCites: Evaluating program repair with semantic-preserving\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Annibale-Panichella/publication/395486459_Metamorphic_Testing_of_Deep_Code_Models_A_Systematic_Literature_Review/links/68d12186220a341aa14e4b74/Metamorphic-Testing-of-Deep-Code-Models-A-Systematic-Literature-Review.pdf&hl=en&sa=X&d=4283050785129156787&ei=OGffaNCwHZycieoPgK6aoQo&scisig=AAZF9b8CPVZTSsxHUaaieQPV0b6M&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=3&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["4 new citations to articles by Thanh Le-Cong", "6 new citations to articles by Bach Le", "David Lo - new related research", "7 new citations to articles by Hong Jin Kang", "4 new citations to articles by Xin ZHOU"]}
{"title": "Secure Motion Verification for Malicious Vehicles in Autonomous Driving", "first_label": ["Verification"], "second_label": [], "data": "W Yan, C Zhou, J Chen, Z Ning, Y Xie, N Yu- 2025 11th IEEE International, 2025\nIn autonomous driving and intelligent transportation systems, self-driving cars rely on \nmotion state information from surrounding vehicles for effective path planning. \nHowever, this reliance creates vulnerabilities, as malicious vehicles can inject false \ndata to compromise navigation and collision avoidance systems. To counter this \nthreat, we propose a holistic security framework that synergistically combines real-\ntime physical-layer verification with a long-term, decentralized reputation system built\nCites: Smart contract development: Challenges and opportunities", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/proceedings-article/pcds/2025/774600a192/2aplMLyyDIc&hl=en&sa=X&d=2291248112185761117&ei=OmffaKXTA_iu6rQPlOqwiQY&scisig=AAZF9b_rQqrk_uJRyylGw5QsJnts&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=4&folt=cit", "author": ["Bach Le"], "ref": ["6 new citations to articles by Bach Le"]}
{"title": "KGCompass: Knowledge Graph Enhanced Repository-Level Software Repair", "first_label": ["Repository-Level"], "second_label": ["Repair", "Graph"], "data": "B YANG, J REN, S JIN, Y LIU, F LIU, B LE, H TIAN - 2025\nAuthors' Contact Information: Boyang Yang; Jiadong Ren; Shunfu Jin, Yanshan \nUniversity, China; Yang Liu, Nanyang Technological University, Singapore; Feng \nLiu; Bach Le, University of Melbourne, Australia; Haoye Tian, Aalto University, \nFinland.\nCites: A Survey of LLM-based Automated Program Repair: Taxonomies\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Boyang-Yang-2/publication/390247861_KGCompass_Knowledge_Graph_Enhanced_Repository-Level_Software_Repair/links/68cf54cc11d348252ba68e04/KGCompass-Knowledge-Graph-Enhanced-Repository-Level-Software-Repair.pdf&hl=en&sa=X&d=15376757677641919947&ei=OmffaKXTA_iu6rQPlOqwiQY&scisig=AAZF9b9PZD1aKGcwRv3HDINJO4vm&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=5&folt=cit", "author": ["Bach Le"], "ref": ["6 new citations to articles by Bach Le", "Bach Le - new articles", "7 new citations to articles by Hong Jin Kang", "10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "first_label": [], "second_label": ["Agent"], "data": "Z Yang, S Wang, K Fu, W He, W Xiong, Y Liu, Y Miao- arXiv preprint arXiv, 2025\nLarge Language Models (LLMs) are increasingly applied to software engineering \n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent \nframeworks with multi-turn interactions and workflow-based Agentless methods with", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23045&hl=en&sa=X&d=2319009272467082410&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b9CT4ZER7as9GwLJ6RzCV2x&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Parallel Fuzzing based on Sub-tasks Scheduling", "first_label": ["Fuzzing"], "second_label": [], "data": "T Gu, T Wang, X Li, S Lu, Y Nie, Z Zhang, X Kuang- ACM Transactions on Software\nParallel fuzzing introduces two key steps: task division and task merging to improve \nefficiency and effectiveness. However, existing works lack effective analysis of task \nresults during task merging; they employ simple differential seed distribution", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3766542&hl=en&sa=X&d=2693574529684625139&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b9aadL5OeSioZ8HLhUivWic&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "first_label": [], "second_label": ["Agent", "Reasoning"], "data": "S Ouyang, J Yan, I Hsu, Y Chen, K Jiang, Z Wang- arXiv preprint arXiv, 2025\nWith the growing adoption of large language model agents in persistent real-world \nroles, they naturally encounter continuous streams of tasks. A key limitation, \nhowever, is their failure to learn from the accumulated interaction history, forcing", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.25140&hl=en&sa=X&d=12867737869979691806&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b-z7Jhe5lDHccQ4eyi-Wr3y&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=3&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage", "first_label": ["Fuzzing"], "second_label": [], "data": "K Feng, J Singer, AK Marnerides- arXiv preprint arXiv:2509.04967, 2025\nBinary-only fuzzing often struggles with achieving thorough code coverage and \nuncovering hidden vulnerabilities due to limited insight into a program's internal \ndataflows. Traditional grey-box fuzzers guide test case generation primarily using", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.04967&hl=en&sa=X&d=12793668697793724612&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b9DbASTknJLZ9Qw2muyThkA&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=5&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Fuzzing JavaScript JIT compilers with a high-quality differential test oracle", "first_label": ["Fuzzing", "Software Testing"], "second_label": [], "data": "J Li, H Xu, Y Wang, Z Jiang, H Chun, P Xie, Y Chen- Computers & Security, 2025\nAbstract Modern JavaScript engines use Just-In-Time (JIT) compilers to convert \nfrequently executed code into machine instructions, boosting performance for web \napplications and cross-platform systems. However, the optimizations in JIT compilers", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0167404825003499&hl=en&sa=X&d=7438620869129233397&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b8tN2mMloCI7htWicorKtpq&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=6&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets", "first_label": ["Fuzzing"], "second_label": [], "data": "C Cesarano, R Natella- arXiv preprint arXiv:2509.05643, 2025\nCoverage-guided fuzzing has been widely applied to address zero-day \nvulnerabilities in general-purpose software and operating systems. This approach \nrelies on instrumenting the target code at compile time. However, applying it to", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.05643&hl=en&sa=X&d=4585422960426806286&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b-p7mmDlvT3yh-ZhB_TgxIV&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=7&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?", "first_label": ["LLM", "Code", "Repository-Level"], "second_label": [], "data": "W Peng, Y Shi, Y Wang, X Zhang, B Shen, X Gu- arXiv preprint arXiv:2509.14635, 2025\nUnderstanding and reasoning about entire software repositories is an essential \ncapability for intelligent software engineering tools. While existing benchmarks such \nas CoSQA and CodeQA have advanced the field, they predominantly focus on small", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.14635&hl=en&sa=X&d=15006803551698078506&ei=O2ffaPPVFp6sieoP2PTggQg&scisig=AAZF9b-gcqnrJWuOC2cDSJR6nYNt&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=8&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "H Chang, Y Jun, H Lee- arXiv preprint arXiv:2509.22830, 2025\nThe growing deployment of large language model (LLM) based agents that interact \nwith external environments has created new attack surfaces for adversarial \nmanipulation. One major threat is indirect prompt injection, where attackers embed \nmalicious instructions in external environment output, causing agents to interpret and \nexecute them as if they were legitimate prompts. While previous research has \nfocused primarily on plain-text injection attacks, we find a significant yet\nCites: Adaptive attacks break defenses against indirect prompt injection", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22830&hl=en&sa=X&d=2407502886914257722&ei=OWffaPemCZXP6rQP9ZK58Q0&scisig=AAZF9b_zGZ5UZWwrYdojCUU9qDJ3&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "Y Meng, L Tang, F Yu, J Jia, G Yan, P Yang, Z Xi- arXiv preprint arXiv:2509.23573, 2025\nLarge Language Models (LLMs) are intensively used to assist security analysts in \ncounteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat \nintelligence (CTI) to support vulnerability assessment and incident response. While \nrecent work has shown that LLMs can support a wide range of CTI tasks such as \nthreat analysis, vulnerability detection, and intrusion defense, significant \nperformance gaps persist in practical deployments. In this paper, we investigate the\nCites: Llm agents can autonomously exploit one-day vulnerabilities", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23573&hl=en&sa=X&d=1491611886356611360&ei=OWffaPemCZXP6rQP9ZK58Q0&scisig=AAZF9b8YCCxz0SFkvu5MTtMUUAKx&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=1&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment", "first_label": ["LLM"], "second_label": [], "data": "J Kim, M Song, S Shin, S Son- arXiv preprint arXiv:2509.22745, 2025\nRecent large language models (LLMs) have increasingly adopted the Mixture-of-\nExperts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a \nsuperficial safety mechanism in which harmful inputs are routed safety-critical \nexperts. However, our analysis reveals that routing decisions for harmful inputs drift \nsignificantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning \n(HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less\nCites: Removing rlhf protections in gpt-4 via fine-tuning", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22745&hl=en&sa=X&d=11610053878383523095&ei=OWffaPemCZXP6rQP9ZK58Q0&scisig=AAZF9b9Ryy9lVeBzgEokbZ-QAxYp&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=2&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Generalization of Variadic Structures with Binders: A Tool for Structural Code Comparison", "first_label": ["Code"], "second_label": [], "data": "A Baumgartner, T Kutsia- arXiv preprint arXiv:2509.25023, 2025\nThis paper introduces a novel anti-unification algorithm for the generalization of \nvariadic structures with binders, designed as a flexible tool for structural code \ncomparison. By combining nominal techniques for handling variable binding with \nsupport for variadic expressions (common in abstract syntax trees and programming \nlanguages), the approach addresses key challenges such as overemphasis on \nbound variable names and difficulty handling insertions or deletions in code\nCites: Vgx: Large-scale sample generation for boosting learning-based\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.25023&hl=en&sa=X&d=11988250968567186176&ei=OWffaPemCZXP6rQP9ZK58Q0&scisig=AAZF9b8EJp72QJZ8HS6IBckT92RA&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=3&folt=cit", "author": ["Richard Fang"], "ref": ["4 new citations to articles by Richard Fang"]}
{"title": "Evaluating SAP Joule for Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "J Heisler, J Reisinger, A Fischer- arXiv preprint arXiv:2509.24828, 2025\nSAP has released its own proprietary generative model SAP Joule, intended for \nvarious generative tasks, including serving as a code assistant for software \nengineers. While Joule is yet not focused on SAP-specific ABAP code generation, it", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24828&hl=en&sa=X&d=3752269403016204218&ei=PGffaKrdEMyR6rQP8M_xgAM&scisig=AAZF9b99WxxwInMhTJnAhqJdbCXJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "first_label": ["LLM", "Code"], "second_label": [], "data": "Q Wang, Z Sun, R Wang, T Huang, Z Jin, G Li, C Lyu- arXiv preprint arXiv, 2025\nLarge Language Models (LLMs) can translate natural language requirements into \ncode, yet empirical analyses of representative models reveal that semantic errors-\nprograms that compile but behave incorrectly-constitute the majority of observed", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24507&hl=en&sa=X&d=16752764936045731674&ei=PGffaKrdEMyR6rQP8M_xgAM&scisig=AAZF9b9KC1lKl2hTSKuhiiUc-Jar&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Hong Jin Kang - new related research", "David Lo - new related research"]}
{"title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "first_label": ["Code", "Commit Message", "Code Change"], "second_label": ["Generation"], "data": "H Kuang, N Zhang, H Gao, X Zhou, WKG Assuno- arXiv preprint arXiv, 2025\nCommit messages are valuable resources for describing why code changes are \ncommitted to repositories in version control systems (eg, Git). They effectively help \ndevelopers understand code changes and better perform software maintenance", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.15567&hl=en&sa=X&d=3566966376596200560&ei=PGffaKrdEMyR6rQP8M_xgAM&scisig=AAZF9b-vZA0Uf808DqbPbNRX2qRx&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=3&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Generation", "Agent"], "data": "J Chen, H Huang, Y Lyu, J An, J Shi, C Yang, T Zhang- arXiv preprint arXiv, 2025\nLarge language model (LLM) powered code agents are rapidly transforming \nsoftware engineering by automating tasks such as testing, debugging, and repairing, \nyet the security risks of their generated code have become a critical concern. Existing", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22097&hl=en&sa=X&d=17349871534881250756&ei=PGffaKrdEMyR6rQP8M_xgAM&scisig=AAZF9b8kYbQbluttLz4z1MJU_2OD&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=7&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "7 new citations to articles by Hong Jin Kang", "10 new citations to articles by Abhik Roychoudhury"]}
{"title": "ImportSnare: Directed\" Code Manual\" Hijacking in Retrieval-Augmented Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "K Ye, L Su, C Qian- arXiv preprint arXiv:2509.07941, 2025\nCode generation has emerged as a pivotal capability of Large Language Models \n(LLMs), revolutionizing development efficiency for programmers of all skill levels. \nHowever, the complexity of data structures and algorithmic logic often results in\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.07941%3F&hl=en&sa=X&d=12960283547209180326&ei=PGffaKrdEMyR6rQP8M_xgAM&scisig=AAZF9b-ZDtajHNi1jL3oUGb-sWCh&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=9&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "LLM-SZZ: Novel Vulnerability-Inducing Commit Identification Driven by Large Language Model and CVE Description", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "S Fan, X Liu, Y Zhang, Y Tan, L Yin, Z Chen, S Li\nThe SZZ method and its variants are widely employed to identify vulnerability-\naffected ranges by analyzing vulnerability-fixing commits to trace back vulnerability-\ninducing commits. However, these methods generally suffer from low precision due", "link": "https://scholar.google.com/scholar_url?url=https://songli.io/papers/LLM-SZZ.pdf&hl=en&sa=X&d=11360329303053337607&ei=PGffaOrUHZ2N6rQP3enmuQg&scisig=AAZF9b-llm-iUuaPic6SJcWwOQlc&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Enhancing Semantic Clone Detection with Siamese Bi-LSTM for Multi-Level Source Code Representations", "first_label": ["Code"], "second_label": ["Detection"], "data": "FHA Quradaa, S Shahzad, R Saeed, MH Al-Hakimi- Arabian Journal for Science and, 2025\nSource code clone detectors are fundamental techniques in Software Engineering. \nDespite significant research efforts in the last ten years, current methods still need to \nbe improved to detect semantic clones. The advancement of machine learning", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s13369-025-10675-z&hl=en&sa=X&d=7647209439385932584&ei=PGffaOrUHZ2N6rQP3enmuQg&scisig=AAZF9b8dwaU-JUy_Zn_LXzeDc-Vd&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=4&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Automated Repair of C Programs Using Large Language Models", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "M Farzandway, F Ghassemi- arXiv preprint arXiv:2509.01947, 2025\nThis study explores the potential of Large Language Models (LLMs) in automating \nthe repair of C programs. We present a framework that integrates spectrum-based \nfault localization (SBFL), runtime feedback, and Chain-of-Thought-structured", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.01947&hl=en&sa=X&d=1317846922093490120&ei=PGffaOrUHZ2N6rQP3enmuQg&scisig=AAZF9b9xJ1n4DpSkY2MDDEUWo_Wk&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=6&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "D Liao, X Yin, S Pan, C Ni, Z Xing, X Sun- arXiv preprint arXiv:2509.23812, 2025\nUnit testing is essential for software quality assurance, yet writing and maintaining \ntests remains time-consuming and error-prone. To address this challenge, \nresearchers have proposed various techniques for automating unit test generation", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23812&hl=en&sa=X&d=17547399089676270182&ei=PGffaOrUHZ2N6rQP3enmuQg&scisig=AAZF9b_KQEA7UTv7yqv3sXHg6AI3&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=7&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "David Lo - new related research"]}
{"title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "Z Xue, P Ma, Z Wang, S Wang- arXiv preprint arXiv:2509.11708, 2025\nZero-knowledge proofs (ZKPs) are increasingly deployed in domains such as privacy-\npreserving authentication, blockchain scalability, and secure finance. However, \nauthoring ZK programs remains challenging: unlike mainstream programming, ZK", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.11708&hl=en&sa=X&d=10349220616421543327&ei=PGffaOrUHZ2N6rQP3enmuQg&scisig=AAZF9b9VxdZ6a1w1kBGuJtv-cEtA&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=8&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Large Language Models for Software Testing: A Research Roadmap", "first_label": ["LLM", "Software Testing"], "second_label": ["Search"], "data": "C Augusto, A Bertolino, G De Angelis, F Lonetti- arXiv preprint arXiv, 2025\nLarge Language Models (LLMs) are starting to be profiled as one of the most \nsignificant disruptions in the Software Testing field. Specifically, they have been \nsuccessfully applied in software testing tasks such as generating test code, or", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.25043&hl=en&sa=X&d=1208330022631398164&ei=OmffaIuYFZycieoPgK6aoQo&scisig=AAZF9b9b4upJnsQek3OFr6wbVQSd&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=5&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Binary Diff Summarization using Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "M Udeshi, VSC Putrevu, P Krishnamurthy- arXiv preprint arXiv, 2025\nSecurity of software supply chains is necessary to ensure that software updates do \nnot contain maliciously injected code or introduce vulnerabilities that may \ncompromise the integrity of critical infrastructure. Verifying the integrity of software", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23970&hl=en&sa=X&d=10519039045997772347&ei=OmffaIuYFZycieoPgK6aoQo&scisig=AAZF9b9YEARwInBvwF2248v01rbj&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=8&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective", "first_label": [], "second_label": [], "data": "P Chen, H Liang, T Chen- arXiv preprint arXiv:2509.21945, 2025\nTo efficiently tune configuration for better system performance (eg, latency), many \ntuners have leveraged a surrogate model to expedite the process instead of solely \nrelying on the profoundly expensive system measurement. As such, it is naturally \nbelieved that we need more accurate models. However, the fact of accuracy can lie-a \nsomewhat surprising finding from prior work-has left us many unanswered questions \nregarding what role the surrogate model plays in configuration tuning. This paper\nCites: Greening large language models of code", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.21945&hl=en&sa=X&d=17783382616808622371&ei=OWffaJSAL63M6rQPuqaCiA0&scisig=AAZF9b9Ntb7LpDt7lJtxwzrDfIOk&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=1&folt=cit", "author": ["Hong Jin Kang"], "ref": ["7 new citations to articles by Hong Jin Kang"]}
{"title": "A Computational Perspective on NeuroAI and Synthetic Biological Intelligence", "first_label": [], "second_label": [], "data": "D Patel, MS Tanveer, J Gonzalez-Ferrer, A Loeffler- arXiv preprint arXiv, 2025\nNeuroAI is an emerging field at the intersection of neuroscience and artificial \nintelligence, where insights from brain function guide the design of intelligent \nsystems. A central area within this field is synthetic biological intelligence (SBI), \nwhich combines the adaptive learning properties of biological neural networks with \nengineered hardware and software. SBI systems provide a platform for modeling \nneural computation, developing biohybrid architectures, and enabling new forms of\nCites: Surveying neuro-symbolic approaches for reliable artificial", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23896&hl=en&sa=X&d=12339767377315408012&ei=OWffaJSAL63M6rQPuqaCiA0&scisig=AAZF9b-3yKT04XVUgn97AxgCGVSV&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=2&folt=cit", "author": ["Hong Jin Kang"], "ref": ["7 new citations to articles by Hong Jin Kang"]}
{"title": "Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering", "first_label": [], "second_label": [], "data": "V De Martino, MA Zadenoori, X Franch, A Ferrari- arXiv preprint arXiv:2509.22320, 2025\nLanguage Models are increasingly applied in software engineering, yet their \ninference raises growing environmental concerns. Prior work has examined \nhardware choices and prompt length, but little attention has been paid to linguistic \ncomplexity as a sustainability factor. This paper introduces Green Prompt \nEngineering, framing linguistic complexity as a design dimension that can influence \nenergy consumption and performance. We conduct an empirical study on\nCites: Greening large language models of code", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22320&hl=en&sa=X&d=1877254333866732556&ei=OWffaJSAL63M6rQPuqaCiA0&scisig=AAZF9b8K7TcENwRtapw434alUIMi&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=3&folt=cit", "author": ["Hong Jin Kang"], "ref": ["7 new citations to articles by Hong Jin Kang", "4 new citations to articles by Xin ZHOU"]}
{"title": "CCWise: CarbonCost Aware Regional LLM Orchestration for Next-Gen Sustainable AI", "first_label": ["LLM"], "second_label": [], "data": "RK Saha, D Chahal, R Singhal, M Nambiar- NeurIPS 2025 Workshop on Evaluating the\nThis paper presents a comprehensive orchestration for evaluating the sustainability \nof Large Language Models (LLMs) lifecycle by integrating carbon emissions, energy \nconsumption, and cost-efficiency metrics across diverse geographic regions. We \nintroduce two novel indicesCarbon-Cost Tradeoff Index (CCTI) and Green Cost \nEfficiency (GCE)to quantify the environmental and economic trade-offs inherent in \ntoken generation of LLM deployment. Through extensive experimental analysis\nCites: Greening large language models of code", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DVf4Gu3dtHC&hl=en&sa=X&d=9759993333755148706&ei=OWffaJSAL63M6rQPuqaCiA0&scisig=AAZF9b-jslubAsKcLm9kTJLUQZHj&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=4&folt=cit", "author": ["Hong Jin Kang"], "ref": ["7 new citations to articles by Hong Jin Kang"]}
{"title": "SoK: Potentials and Challenges of Large Language Models for Reverse Engineering", "first_label": ["LLM"], "second_label": [], "data": "X Hu, Z Fu, S Xie, SHH Ding, P Charland- arXiv preprint arXiv:2509.21821, 2025\nReverse Engineering (RE) is central to software security, enabling tasks such as \nvulnerability discovery and malware analysis, but it remains labor-intensive and \nrequires substantial expertise. Earlier advances in deep learning start to automate \nparts of RE, particularly for malware detection and vulnerability classification. More \nrecently, a rapidly growing body of work has applied Large Language Models (LLMs) \nto similar purposes. Their role compared to prior machine learning remains unclear\nCites: Large language model guided protocol fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.21821&hl=en&sa=X&d=14198233320746930427&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b8v8Q19cqHXP_WqEX1O7DfW&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=2&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "GUIFUZZ++: Unleashing Grey-box Fuzzing on Desktop Graphical User Interfacing Applications", "first_label": ["Fuzzing"], "second_label": ["Graph"], "data": "D Otto, T Rowlett, S Nagy\nDesktop applications represent one of today's largest software ecosystems, \naccounting for over 96% of workplace computing and supporting essential \noperations across critical sectors such as healthcare, commerce, industry, and \ngovernment. Though modern software is increasingly being vetted through fuzzing\nan automated testing technique for large-scale bug discoverya major component \nof desktop applications remains universally under-vetted: the Graphical User\nCites: Program Environment Fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://futures.cs.utah.edu/papers/25ASE.pdf&hl=en&sa=X&d=18087380470427803104&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b-vDiq1Xxp9P-WJQXt0xNy_&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=4&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning", "first_label": [], "second_label": ["Generation", "Agent"], "data": "Z Lu, H Ren, Y Yang, K Wang, Z Zong, J Pan, M Zhan- arXiv preprint arXiv, 2025\nAgent systems powered by large language models (LLMs) have demonstrated \nimpressive performance on repository-level code-generation tasks. However, for \ntasks such as website codebase generation, which depend heavily on visual effects \nand user-interaction feedback, current code agents rely only on simple code \nexecution for feedback and verification. This approach fails to capture the actual \nquality of the generated code. In this paper, we propose WebGen-Agent, a novel\nCites: Autocoderover: Autonomous program improvement, 2024", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.22644&hl=en&sa=X&d=11672519558392686429&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b-ygX_LH_c9erux60Vcpblc&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=5&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Towards Reliable Generation of Executable Workflows by Foundation Models", "first_label": [], "second_label": ["Generation"], "data": "S Masoumzadeh, K Gallaba, D Lin, AE Hassan- arXiv preprint arXiv:2509.25117, 2025\nRecent advancements in Foundation Models (FMs) have demonstrated significant \nprogress in comprehending complex natural language to perform intricate tasks. \nSuccessfully executing these tasks often requires orchestrating calls to FMs \nalongside other software components. However, manually decomposing a task into a \ncoherent sequence of smaller, logically aggregated steps, commonly referred to as \nworkflows, demands considerable effort and specialized domain knowledge. While\nCites: SemFix: Program Repair via Semantic Analysis", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.25117&hl=en&sa=X&d=12196803753268532282&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b9gpQwKrK9_-LvXOoBr77eH&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=6&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Metamorphic Testing for Audio Content Moderation Software", "first_label": ["Software Testing"], "second_label": [], "data": "W Wang, Y Wu, J Zhang, S Li, Y Peng, W Chen- arXiv preprint arXiv, 2025\nThe rapid growth of audio-centric platforms and applications such as WhatsApp and \nTwitter has transformed the way people communicate and share audio content in \nmodern society. However, these platforms are increasingly misused to disseminate \nharmful audio content, such as hate speech, deceptive advertisements, and explicit \nmaterial, which can have significant negative consequences (eg, detrimental effects \non mental health). In response, researchers and practitioners have been actively\nCites: Fuzz Testing based Data Augmentation to Improve Robustness of", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24215&hl=en&sa=X&d=7285678392934041559&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b8CrQWe7kLi42ILTluvuvkp&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=7&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Multifuzz: a Collaborative Fuzzing Framework Based on Concolic Execution", "first_label": ["Fuzzing"], "second_label": [], "data": "Z Sun, X Li, F He, D Yu, Y Zhang- 2025 11th IEEE International Conference on, 2025\nFuzzing is an efficient vulnerability discovery technique, but it has limitations in \nexploring the depth of program paths. Concolic Execution performs outstandingly in \npath coverage, yet its execution speed is relatively slow. Hybrid fuzzing combines the \nadvantages of both, improving the comprehensiveness and efficiency of testing. \nCollaborative fuzzing further enhances the testing effectiveness by integrating \nmultiple testing engines. This paper studies the impact of concolic execution on the\nCites: Coverage-based greybox fuzzing as markov chain", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/proceedings-article/pcds/2025/774600a389/2aplMfRJflC&hl=en&sa=X&d=14208974081929903867&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b8CEUQNo3eDA7cUKfpOHM9m&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=8&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "LogicRepair: An Empirical Study on Automated Repair of Smart Contract Logic Vulnerabilities Based on Large Language Models", "first_label": ["Vulnerabilities", "Smart Contracts", "APR", "LLM"], "second_label": ["Repair"], "data": "M Ai, K Wang, H Wang, C Zheng, Y Zhang- on Privacy Computing and Data Security, 2025\nBlockchain technology has exploded in popularity, providing robust technical support \nfor decentralized finance (DeFi) and other domains. However, logical vulnerabilities \nin smart contracts can lead to serious security issues and economic losses. \nTraditional repair tools often fail to handle these logic flaws effectively. To address \nthis issue, our study proposes an automated repair framework for smart contract logic \nvulnerabilities, which takes advantage of large language models (LLM) to improve\nCites: Smart Contract Repair\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/proceedings-article/pcds/2025/774600a261/2aplQRfplja&hl=en&sa=X&d=4220323208835090567&ei=OmffaJO6NZycieoPgK6aoQo&scisig=AAZF9b8NMXTGJp2wIoWlEe8SuIpe&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=9&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning", "first_label": ["LLM"], "second_label": [], "data": "Z Wang, D He, Z Zhang, X Li, L Zhu, M Li, J Liu- arXiv preprint arXiv:2509.23558, 2025\nLarge language models (LLMs) have demonstrated remarkable capabilities, yet they \nalso introduce novel security challenges. For instance, prompt jailbreaking attacks \ninvolve adversaries crafting sophisticated prompts to elicit responses from LLMs that", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23558&hl=en&sa=X&d=2749826917926664463&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b8yPbrhQZJx7QFv8R1moIWX&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment", "first_label": ["LLM"], "second_label": [], "data": "L Yang, T Zheng, K Xiu, Y Chen, D Wang, P Zhao- arXiv preprint arXiv, 2025\nThe alignment of large language models (LLMs) with human values is critical for their \nsafe deployment, yet jailbreak attacks can subvert this alignment to elicit harmful \noutputs from LLMs. In recent years, a proliferation of jailbreak attacks has emerged", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24384&hl=en&sa=X&d=6954529287697046870&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b9uM0TGT71bAeDSUUl5_-VC&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey", "first_label": [], "second_label": [], "data": "BH Abbasi, Y Zhang, L Zhang, S Gao- arXiv preprint arXiv:2509.07504, 2025\nBackdoor (trojan) attacks embed hidden, controllable behaviors into machine-\nlearning models so that models behave normally on benign inputs but produce \nattacker-chosen outputs when a trigger is present. This survey reviews the rapidly", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.07504&hl=en&sa=X&d=3269854252125827141&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b9hUL9vDNoEXnWcZIb5H3Bf&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks", "first_label": ["LLM"], "second_label": ["Agent"], "data": "SM Hossain, RK Shayoni, MR Ameen, A Islam- arXiv preprint arXiv, 2025\nPrompt injection attacks represent a major vulnerability in Large Language Model \n(LLM) deployments, where malicious instructions embedded in user inputs can \noverride system prompts and induce unintended behaviors. This paper presents a", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.14285&hl=en&sa=X&d=913307193832282926&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b-cOLAwuLy9zQlk8KZ2kPLK&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Dual-Space Smoothness for Robust and Balanced LLM Unlearning", "first_label": ["LLM"], "second_label": [], "data": "H Yan, Z Liu, M Jiang- arXiv preprint arXiv:2509.23362, 2025\nWith the rapid advancement of large language models, Machine Unlearning has \nemerged to address growing concerns around user privacy, copyright infringement, \nand overall safety. Yet state-of-the-art (SOTA) unlearning methods often suffer from", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23362&hl=en&sa=X&d=1871730686130311929&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b8As51yVW8jz__d_c1PoJ_F&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models", "first_label": ["LLM"], "second_label": [], "data": "M Yu, Z Zhou, M Aloqaily, K Wang, B Huang, S Wang- arXiv preprint arXiv, 2025\nFine-tuned Large Language Models (LLMs) are vulnerable to backdoor attacks \nthrough data poisoning, yet the internal mechanisms governing these attacks remain \na black box. Previous research on interpretability for LLM safety tends to focus on", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.21761&hl=en&sa=X&d=2109015168868723800&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b8vuwf85b9luh6u5TdaGEpT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors", "first_label": ["LLM"], "second_label": [], "data": "B Cao, C Li, Y Cao, Y Ge, T Wang, J Chen- arXiv preprint arXiv:2509.21884, 2025\nLarge language models (LLMs) have been widely adopted across various \napplications, leveraging customized system prompts for diverse tasks. Facing \npotential system prompt leakage risks, model developers have implemented", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.21884&hl=en&sa=X&d=9049765669629367160&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b-kaJvFCE65fBfFjzILwtIc&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "When Your Reviewer is an LLM: Biases, Divergence, and Prompt Injection Risks in Peer Review", "first_label": ["LLM"], "second_label": [], "data": "C Zhu, J Xiong, R Ma, Z Lu, Y Liu, L Li- arXiv preprint arXiv:2509.09912, 2025\nPeer review is the cornerstone of academic publishing, yet the process is \nincreasingly strained by rising submission volumes, reviewer overload, and expertise \nmismatches. Large language models (LLMs) are now being used as\" reviewer aids,\"", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.09912&hl=en&sa=X&d=559709989728729115&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b_J-eLdccTA9UluyGqY5GIL&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models", "first_label": ["LLM"], "second_label": [], "data": "W Jeung, S Yoon, Y Cho, D Jeon, S Shin, H Hong- arXiv preprint arXiv, 2025\nDiffusion large language models (dLLMs) enable any-order generation, but this \nflexibility enlarges the attack surface: harmful spans may appear at arbitrary \npositions, and template-based prefilling attacks such as DIJA bypass response-level", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.23286&hl=en&sa=X&d=16977104919503302935&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b-9ssXxkPlc54WBS9-P-Zx-&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Takedown: How It's Done in Modern Coding Agent Exploits", "first_label": [], "second_label": ["Agent", "Exploit"], "data": "E Lee, D Kim, W Kim, I Yun- arXiv preprint arXiv:2509.24240, 2025\nCoding agents, which are LLM-driven agents specialized in software development, \nhave become increasingly prevalent in modern programming environments. Unlike \ntraditional AI coding assistants, which offer simple code completion and suggestions\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.24240&hl=en&sa=X&d=9106409834220255345&ei=PGffaLauA_iu6rQPlOqwiQY&scisig=AAZF9b8XIiHlvS0_WuiZZbFkX08R&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Interpretable Vulnerability Detection in LLMs: A BERT-Based Approach with SHAP Explanations", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection"], "data": "N Ahmad, C Zhang - 2025\nSource code vulnerabilities present significant security threats, necessitating \neffective detection techniques. Rigid rule-sets and pattern matching are the \nfoundation of traditional static analysis tools, which drown developers in false \npositives and miss context-sensitive vulnerabilities. Large Language Models (LLMs) \nlike BERT, in particular, are examples of artificial intelligence (AI) that exhibit promise \nbut frequently lack transparency. In order to overcome the issues with model\nCites: Large language model for vulnerability detection and repair", "link": "https://scholar.google.com/scholar_url?url=https://cdn.techscience.press/files/cmc/2025/TSP_CMC-85-2/TSP_CMC_67044/TSP_CMC_67044.pdf&hl=en&sa=X&d=1252818813327302141&ei=O2ffaJ-eM52qieoPz8OP6Qo&scisig=AAZF9b-4FiCmmcGvTLnyvFdif2Cb&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=2&folt=cit", "author": ["Xin ZHOU"], "ref": ["4 new citations to articles by Xin ZHOU"]}
{"title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System", "first_label": ["LLM"], "second_label": ["Agent", "Exploit"], "data": "Y Liu, Y Xie, M Luo, Z Liu, Z Zhang, K Zhang, Z Li- arXiv preprint arXiv, 2025\nLLM-based agentic systems leverage large language models to handle user queries, \nmake decisions, and execute external tools for complex tasks across domains like \nchatbots, customer service, and software engineering. A critical component of these", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.05755%3F&hl=en&sa=X&d=7380962265010499197&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b_WNN5iEJzU4LQ3fLBGGpF6&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "first_label": ["LLM"], "second_label": [], "data": "Y Pang, W Meng, X Liao, T Wang- arXiv preprint arXiv:2509.07287, 2025\nWith the rapid development of large language models, the potential threat of their \nmalicious use, particularly in generating phishing content, is becoming increasingly \nprevalent. Leveraging the capabilities of LLMs, malicious users can synthesize", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.07287&hl=en&sa=X&d=841142860163656335&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9zUiVve_NVtg5_7UjR-gXJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Leveraging CVAE Encoding for Backdoor Attacks in Few-Shot Learning with Prototypical Networks", "first_label": [], "second_label": [], "data": "Q Yan, S Liang, A Ullah- IEEE Transactions on Dependable and Secure, 2025\nFew-shot learning (FSL) has demonstrated tremendous potential when challenged \nwith limited training data, but the assessment of its vulnerability to backdoor attacks is \nstill at an early stage. However, recent research revealed this deep learning", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11152502/&hl=en&sa=X&d=6687930075784700670&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9ptsbEWBr80FGSyzcZR2Er&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Prototype-Guided Robust Learning against Backdoor Attacks", "first_label": [], "second_label": [], "data": "W Guo, M Pintor, A Demontis, B Biggio- arXiv preprint arXiv:2509.08748, 2025\nBackdoor attacks poison the training data to embed a backdoor in the model, \ncausing it to behave normally on legitimate inputs but maliciously when specific \ntrigger signals appear. Training a benign model from a dataset poisoned by", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.08748&hl=en&sa=X&d=9079518275239981489&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b8nbcMJrz3EP9De5YM82FpF&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Oyster-I: Beyond Refusal--Constructive Safety Alignment for Responsible Language Models", "first_label": ["LLM"], "second_label": [], "data": "R Duan, J Liu, X Jia, S Zhao, R Cheng, F Wang, C Wei- arXiv preprint arXiv, 2025\nLarge language models (LLMs) typically deploy safety mechanisms to prevent \nharmful content generation. Most current approaches focus narrowly on risks posed \nby malicious actors, often framing risks as adversarial events and relying on", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.01909%3F&hl=en&sa=X&d=6590160868100905669&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b8p2pmNGwnqdehrNy547NkZ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Jailbreak Attack with Multimodal Virtual Scenario Hypnosis for Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "X Shi, S Chen, G Zhang, W Wei, Y Li, Z Fan, J Liu- Pattern Recognition, 2025\nDue to the inherent vulnerabilities of large Vision-Language Models (VLMs), security \ngovernance has emerged as a critical concern, particularly given the risks posed by \nnoisy and biased training data as well as adversarial attacks, including data", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0031320325010520&hl=en&sa=X&d=18404876866865125967&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9xJx3YOX0I9CBN24pCw_YG&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Why language models hallucinate", "first_label": ["LLM"], "second_label": [], "data": "AT Kalai, O Nachum, SS Vempala, E Zhang- arXiv preprint arXiv:2509.04664, 2025\nLike students facing hard exam questions, large language models sometimes guess \nwhen uncertain, producing plausible yet incorrect statements instead of admitting \nuncertainty. Such\" hallucinations\" persist even in state-of-the-art systems and", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.04664&hl=en&sa=X&d=17470905045322269269&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9-xvJdrsXYLi4Kkc78fX8P&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees", "first_label": ["LLM"], "second_label": [], "data": "S Zeighami, S Shankar, A Parameswaran- arXiv preprint arXiv:2509.02896, 2025\nLarge Language Models (LLMs) are being increasingly used as a building block in \ndata systems to process large text datasets. To do so, LLM model providers offer \nmultiple LLMs with different sizes, spanning various cost-quality trade-offs when", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.02896%3F&hl=en&sa=X&d=17574372354989562785&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b-DvnVUyd5iBucpFadx2SoL&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System", "first_label": ["LLM"], "second_label": ["Exploit"], "data": "P Reddy, AS Gujral- arXiv preprint arXiv:2509.10540, 2025\nLarge language model (LLM) assistants are increasingly integrated into enterprise \nworkflows, raising new security concerns as they bridge internal and external data \nsources. This paper presents an in-depth case study of EchoLeak (CVE-2025\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.10540%3F&hl=en&sa=X&d=3830339454672368349&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b-2-nfVtjM51-72VPmeR1Yc&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "first_label": ["LLM", "Bug", "Repository-Level"], "second_label": [], "data": "J Liu, Z Liu, Z Cheng, M He, X Shi, Y Guo, X Zhu, Y Guo- arXiv preprint arXiv, 2025\nLarge Language Models (LLMs) have exhibited significant proficiency in code \ndebugging, especially in automatic program repair, which may substantially reduce \nthe time consumption of developers and enhance their efficiency. Significant\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.04078%3F&hl=en&sa=X&d=12419661760319839544&ei=jNrdaL2QCJXP6rQP9ZK58Q0&scisig=AAZF9b8SHef7li0_lXitWcja8LI-&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "Generating Software Architectural Model from Source Code Using Module Clustering", "first_label": ["Code"], "second_label": [], "data": "B Arasteh, SS Sefati, H Kusetogullari, F Kiani- Symmetry, 2025\nSoftware maintenance is one of the most expensive phases in software \ndevelopment, especially when complex source code is the only available artifact. \nClustering software modules and generating a structured architectural model can\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2073-8994/17/9/1523&hl=en&sa=X&d=9037379356857496352&ei=jNrdaKWYLISb6rQPt9iYgAM&scisig=AAZF9b-K0RiHQ1pE7QZ3IUfLNE6M&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Aligning Requirement for Large Language Model's Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "Z Tian, J Chen- arXiv preprint arXiv:2509.01313, 2025\nCode generation refers to the automatic generation of source code based on a given \nprogramming specification, which has garnered significant attention particularly with \nthe advancement of large language models (LLMs). However, due to the inherent", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.01313&hl=en&sa=X&d=13370697676538068499&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b9B7L937IdcMo1J6Zsy7VAv&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "first_label": ["Fuzzing"], "second_label": ["Generation"], "data": "R Saravanan, S Paria, A Dasgupta, S Bhunia- arXiv preprint arXiv:2509.20808, 2025\nHardware Fuzzing emerged as one of the crucial techniques for finding security flaws \nin modern hardware designs by testing a wide range of input scenarios. One of the \nmain challenges is creating high-quality input seeds that maximize coverage and", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.20808&hl=en&sa=X&d=11635584275403043134&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b-EaDXCg2a3mA1rI2gL4oLU&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Performance and interpretability analysis of code generation large language models", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "VS Pendyala, NB Thakur- Neurocomputing, 2025\nAbstract As Large Language Models (LLMs) are increasingly getting integrated into \nsoftware development workflows, understanding their reliability, error patterns and \ninterpretability in real-world development scenarios is crucial for establishing their\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0925231225021332&hl=en&sa=X&d=12519052331836719178&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b_3S5NkeiHUCnc01CtGOqCD&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Moving Towards Robust and Reliable AI: Vulnerability Detection Framework Through Red Teaming", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "V Raman, A Desarkar, A Sen- Data Science and Communication Engineering, 2025\nEnsuring the reliability of AI-based systems is a crucial challenge in today's AI-driven \nenvironment. However, robustness is a key component of reli-able AI, as the failure \nof these systems could have severe consequences in the critical domains such as in \nhealthcare, transportation and finance. Eventually, the systems in these domains are \nlargely dependent on various language models. Hence, measuring the robustness of \nthose language models ultimately determines the end success though no such\nCites: Large language model for vulnerability detection and repair\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dli-KEQAAQBAJ%26oi%3Dfnd%26pg%3DPA209%26ots%3DWIkWfTeWxS%26sig%3DBsjJf3VFapSd3RlrgCNs85_xYMU&hl=en&sa=X&d=6448497152658089733&ei=jtrdaIOUC46IieoP7dK5sA0&scisig=AAZF9b8sv3RIed1EIJJXX70RAtS0&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["1 new citation to articles by Xin ZHOU"]}
{"title": "Fuzzing as editor feedback", "first_label": ["Fuzzing"], "second_label": [], "data": "M Garus, J Lincke, R Hirschfeld- Companion Proceedings of the 9th International, 2025\nLive programming requires concrete examples, but coming up with examples takes \neffort. However, there are ways to execute code without specifying examples, such \nas fuzzing. Fuzzing is a technique that synthesizes program inputs to find bugs in", "link": "https://scholar.google.com/scholar_url?url=https://drops.dagstuhl.de/storage/01oasics/oasics-vol134-programming2025/OASIcs.Programming.2025.8/OASIcs.Programming.2025.8.pdf&hl=en&sa=X&d=4846729257551256103&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b_a936tJ2euestamYofC9hC&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "PromeFuzz: A Knowledge-Driven Approach to Fuzzing Harness Generation with Large Language Models", "first_label": ["LLM", "Fuzzing"], "second_label": ["Generation"], "data": "Y Liu, J Deng, X Jia, Y Wang, M Wang, L Huang, T Wei\nFuzzing has long been recognized as an effective technique for uncovering security \nvulnerabilities by automatically generating and executing a diverse set of inputs [2, 3, \n6, 8, 14, 15, 18, 20, 24, 26, 30, 32, 34, 46, 52, 53, 55, 58]. Traditional fuzzing tools", "link": "https://scholar.google.com/scholar_url?url=https://pvz122.github.io/pdf/25-promefuzz.pdf&hl=en&sa=X&d=12396611312661296556&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b-wSitwHBsEc3RIko2Rlc5c&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Fuzzing JavaScript Engines by Fusing JavaScript and WebAssembly", "first_label": ["Fuzzing"], "second_label": [], "data": "J Lin, C Luo, M Zhang, L Lin, P Li, C Qian - 2026\nJavaScript engines are a fundamental part of modern browsers, and many efforts \nhave been invested in testing them to enhance their security. However, the \nincorporation of WebAssembly into JavaScript engines introduces new attack\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://peng-hui.github.io/data/paper/icse26:mad-eye.pdf&hl=en&sa=X&d=7025301240690243176&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b87H2l6opoFTjGi99T54n3u&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}

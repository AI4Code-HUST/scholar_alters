{"title": "Structure-EnhancedPrompt Learning for Graph-Based Code Vulnerability Detection", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection", "Graph"], "data": "W Chang, C Ye, H Zhou\\xc2\\xa0- Applied Sciences, 2025\nRecent advances in prompt learning have opened new avenues for enhancing \nnatural language understanding in domain-specific tasks, including code \nvulnerability detection. Motivated by the limitations of conventional binary\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2076-3417/15/11/6128&hl=en&sa=X&d=14787634818984465351&ei=r4g-aJ7WNdOj6rQPzMDA4AM&scisig=AAZF9b8AbdZB3i79KkRKBO0madiQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "Xin ZHOU - new related research"]}
{"title": "Investigating Reproducibility Challenges in LLM Bugfixing on the HumanEvalFix Benchmark", "first_label": ["LLM", "Bug"], "second_label": [], "data": "B Szalontai, B M\\xc3\\xa1rton, B Pint\\xc3\\xa9r, T Gregorics - 2025\nBenchmark results for Large Language Models often show inconsistencies across \ndifferent studies. This paper investigates the challenges of reproducing these results \nin automatic bugfixing using LLMs, on the HumanEvalFix benchmark. To determine\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.preprints.org/frontend/manuscript/fb66f679111eaa15fc8e3c47e4f289e3/download_pub&hl=en&sa=X&d=5498677636109518696&ei=r4g-aJ7WNdOj6rQPzMDA4AM&scisig=AAZF9b_WTbrAan-CoxH3zpY3J3tj&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=1&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "1 new citation to articles by Hong Jin Kang", "Xin ZHOU - new related research"]}
{"title": "An empirical study on architectural smells through a pipeline for continuous technical debt assessment", "first_label": [], "second_label": [], "data": "M Bochicchio, D Sas, A Gilardi, FA Fontana\\xc2\\xa0- Information and Software Technology, 2025\nContext: Architectural smells, are a well-known indicator of architectural technical \ndebt, their presence could have a great impact on the maintainability and evolvability \nof a project. Hence, it is important to carefully study and monitor them. Objective: In\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584925001223&hl=en&sa=X&d=12840437567590045959&ei=r4g-aJ7WNdOj6rQPzMDA4AM&scisig=AAZF9b85amGJ2QkpRdzBxsKM0zDQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Quang-Cuong Bui - new related research"]}
{"title": "Redefining Software Testing: How AI-Driven Automation Is Transforming Test Case Prioritization and Defect Prediction", "first_label": ["Software Testing", "Software Defect"], "second_label": [], "data": "E Oye - 2025\nThe landscape of software testing is undergoing a profound transformation driven by \nthe integration of Artificial Intelligence (AI) and automation technologies. This paper \nexplores how AI-driven automation is redefining essential aspects of software testing\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Emma-Oye/publication/392011768_Redefining_Software_Testing_How_AI-Driven_Automation_Is_Transforming_Test_Case_Prioritization_and_Defect_Prediction/links/683058dcbe1b507dce8ef446/Redefining-Software-Testing-How-AI-Driven-Automation-Is-Transforming-Test-Case-Prioritization-and-Defect-Prediction.pdf&hl=en&sa=X&d=8158512365196228399&ei=r4g-aJ7WNdOj6rQPzMDA4AM&scisig=AAZF9b_cWpchtUpewpXb0w4Q5FYx&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Improving prompt tuning-based software vulnerability assessment by fusing source code and vulnerability description", "first_label": ["Vulnerabilities", "Code"], "second_label": [], "data": "J Wang, X Chen, W Pei, S Yang\\xc2\\xa0- Automated Software Engineering, 2025\nTo effectively allocate resources for vulnerability remediation, it is crucial to prioritize \nvulnerability fixes based on vulnerability severity. With the increasingnumber of \nvulnerabilities in recent years, there is an urgent need for automated methods for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00525-5&hl=en&sa=X&d=632305407775732564&ei=sIg-aNaSAuSN6rQPzIKygQU&scisig=AAZF9b9Mq4kj-GXYL15rG6YasTTq&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines", "first_label": [], "second_label": [], "data": "B Ates, F Dobrosavljevi\\xc4\\x87, T Theodoridis, Z Su\nCompilers are essential for the performance and correct execution of software and \nhold universal relevance across various scientific disciplines. Despite this, there is a \nnotable lack of tools for testing and evaluating them, especially within the adaptable\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://filipdob.ro/papers/MLIR_Smith_Report.pdf&hl=en&sa=X&d=4663515892976989619&ei=sIg-aNaSAuSN6rQPzIKygQU&scisig=AAZF9b9F8m6qUmt2eAn_nfyYKk4S&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "first_label": ["APR"], "second_label": ["Repair"], "data": "FV Ruiz, M Hort, L Moonen\\xc2\\xa0- arXiv preprint arXiv:2505.02931, 2025\nAutomatic program repair (APR) aims to reduce the manual efforts required to \nidentify and fix errors in source code. Before the rise of LLM-based agents, a \ncommon strategy was to increase the number of generated patches, sometimes to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.02931%3F&hl=en&sa=X&d=16837479357373517474&ei=r4g-aKvgOYCt6rQP9sPUwAg&scisig=AAZF9b_WjCVulOfH0bpaSqZwLopC&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "CORECRISIS: Threat-Guided and Context-Aware Iterative Learning and Fuzzing of 5G Core Networks", "first_label": ["Fuzzing"], "second_label": [], "data": "Y Dong, T Yang, A Al Ishtiaq, SMM Rashid, A Ranjbar\\xe2\\x80\\xa6\nWe develop CORECRISIS, a stateful black-box fuzz-testing framework for 5G core \nnetwork (5GC) implementations. Unlike previous stateful security analysis efforts of \ncellular networks which rely on manually-crafted, static test inputs and are limited to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-1292-dong-yilu.pdf&hl=en&sa=X&d=4291979592406815479&ei=r4g-aKvgOYCt6rQP9sPUwAg&scisig=AAZF9b8xMpoFy9xYsoZdVl1unvGx&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Fault localization of AI-enabled cyber-physical systems by exploiting temporal neuron activation", "first_label": ["Fault Localization"], "second_label": ["Exploit", "Localization"], "data": "D Lyu, Y Li, Z Zhang, P Arcaini, XY Zhang, F Ishikawa\\xe2\\x80\\xa6\\xc2\\xa0- Journal of Systems and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nModern cyber\\xe2\\x80\\x93physical systems (CPS) are evolving to integrate deep neural \nnetworks (DNNs) as controllers, leading to the emergence of AI-enabled CPSs. An \ninadequately trained DNN controller may produce incorrect control actions, exposing\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0164121225001438&hl=en&sa=X&d=3286846770327067741&ei=r4g-aKvgOYCt6rQP9sPUwAg&scisig=AAZF9b-jUE3fT364wIMjAbdvLNMM&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Detection and Repair of Defects in Register Transfer Level Code Using Static Analysis and Generative AI", "first_label": ["Code", "Static Analysis", "Software Defect"], "second_label": ["Detection", "Repair"], "data": "B Ahmad - 2025\nThe detection and repair of hardware bugs have become increasingly important over \nthe past decade. In particular, security-related issues arising from these bugs have \nbeen the focus of significant academic and industrial efforts. It is crucial to detect \ndefects in hardware as early as possible to reduce costs, efforts, and damage to \nreputation down the line. Existing solutions rely on design-specific information, \nexpertise, and techniques. These constraints limit the generalizability of solutions\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRefixar: Multi-version reasoning for automated repair of regression\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/5af68b8237bd46beda3e6c893d2c6788/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=en&sa=X&d=18433095172492645771&ei=r4g-aJ36M-SN6rQPzIKygQU&scisig=AAZF9b9nU-lZ___LvtULFZR6l21f&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["1 new citation to articles by Bach Le", "2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "J Kong, H Fang, X Yang, K Gao, B Chen, ST Xia\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSupervised fine-tuning (SFT) aligns large language models (LLMs) with human \nintent by training them on labeled task-specific data. Recent studies have shown that \nmalicious attackers can inject backdoors into these models by embedding triggers\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.17601%3F&hl=en&sa=X&d=14932585774719166007&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b-bRcbQ7r2VSTXuEVUTZE69&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Evaluatiing the efficacy of LLM Safety Solutions: The Palit Benchmark Dataset", "first_label": ["LLM"], "second_label": [], "data": "S Palit, D Woods\\xc2\\xa0- arXiv preprint arXiv:2505.13028, 2025\nLarge Language Models (LLMs) are increasingly integrated into critical systems in \nindustries like healthcare and finance. Users can often submit queries to LLM-\nenabled chatbots, some of which can enrich responses with information retrieved\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13028%3F&hl=en&sa=X&d=1273460049730580919&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b9ByON4eaZyjRTpM1-cA_zU&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "first_label": ["LLM"], "second_label": ["Agent"], "data": "L Wang, W Wang, S Wang, Z Li, Z Ji, Z Lyu, D Wu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid advancement of Large Language Models (LLMs) has led to the \nemergence of Multi-Agent Systems (MAS) to perform complex tasks through \ncollaboration. However, the intricate nature of MAS, including their architecture and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12442&hl=en&sa=X&d=8825331941370095164&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b_j6hKF2Zy8dNMOx-tobM6E&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents", "first_label": [], "second_label": ["Agent"], "data": "Z Wang, V Siu, Z Ye, T Shi, Y Nie, X Zhao, C Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe strong planning and reasoning capabilities of Large Language Models (LLMs) \nhave fostered the development of agent-based systems capable of leveraging \nexternal tools and interacting with increasingly complex environments. However\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.05849&hl=en&sa=X&d=13145039506086791936&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b_5aWzkLQOK0HBWClMmbpt1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks", "first_label": [], "second_label": [], "data": "R Zhang, Y Shen, H Li, W Jiang, H Chen, Y Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent research highlights concerns about the trustworthiness of third-party Pre-\nTrained Language Models (PTLMs) due to potential backdoor attacks. These \nbackdoored PTLMs, however, are effective only for specific pre-defined downstream\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.11586&hl=en&sa=X&d=15157304470732077928&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b997v4XRj8-KVQslhBMZvqE&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World", "first_label": [], "second_label": [], "data": "J Guo, L Zhou, Z Wang, J He, Q Song, A Chen, W Jiang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn recent years, deep learning-based Monocular Depth Estimation (MDE) models \nhave been widely applied in fields such as autonomous driving and robotics. \nHowever, their vulnerability to backdoor attacks remains unexplored. To fill the gap in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.16154%3F&hl=en&sa=X&d=5415910119131408953&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b_P0jMEgHvYQo3O-kEno08o&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "first_label": ["LLM"], "second_label": [], "data": "G Chen, F Song, Z Zhao, X Jia, Y Liu, Y Qiao, W Zhang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nJailbreak attacks to Large audio-language models (LALMs) are studied recently, but \nthey achieve suboptimal effectiveness, applicability, and practicability, particularly, \nassuming that the adversary can fully manipulate user prompts. In this work, we first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14103&hl=en&sa=X&d=12355582258955555442&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b84twuHlVXU1hPFciKO3AfT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "Z Wang, H Li, R Zhang, W Jiang, K Chen, T Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn this paper, we present a new form of backdoor attack against Large Language \nModels (LLMs): lingual-backdoor attacks. The key novelty of lingual-backdoor attacks \nis that the language itself serves as the trigger to hijack the infected LLMs to generate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.03501&hl=en&sa=X&d=15489349756851264058&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b_j_X5MMHAf2tokb2xerP6r&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "A4FL: Federated Adversarial Defense via Adversarial Training and Pruning Against Backdoor Attack", "first_label": [], "second_label": [], "data": "B Li, M Hamid, M Saleem, M Aman\\xc2\\xa0- IEEE Access, 2025\nBackdoor attacks threaten federated learning (FL) models, where malicious \nparticipants embed hidden triggers into local models during training. These triggers \ncan compromise crucial applications, such as autonomous systems, when they\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10992684.pdf&hl=en&sa=X&d=4691259454218143835&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b-NVXh_0LVfWkgmGPYIRTtu&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "Z Xu, U Sanghi, M Kankanhalli\\xc2\\xa0- arXiv preprint arXiv:2505.12692, 2025\nLarge Language Models (LLMs) are increasingly deployed in interactions where \nthey are prompted to adopt personas. This paper investigates whether such persona \nconditioning affects model safety under bullying, an adversarial manipulation that\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12692%3F&hl=en&sa=X&d=3130006781540917286&ei=r4g-aNucPMiE6rQP59uGiAk&scisig=AAZF9b-r43I7NUbwAxgd2MM-t4Q_&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AnnCoder: A mti-Agent-Based Code Generation and Optimization Model", "first_label": ["Code"], "second_label": ["Generation", "Agent"], "data": "Z Zhang, J Wang, Z Li, Y Wang, J Zheng - 2025\nThe rapid progress of LLMs has greatly improved natural language tasks like code \ngeneration, boosting developer productivity. However, challenges persist. Generated \ncode often appears\" pseudocorrect\"\\xe2\\x80\\x94passing functional tests but plagued by \ninefficiency or redundant structures. Many models rely on outdated methods like \ngreedy selection, which trap them in local optima, limiting their ability to explore \nbetter solutions. We propose AnnCoder, a multi-agent framework that mimics the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Repair of Programs from Large Language Models\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.preprints.org/frontend/manuscript/59d3304520465f088e1d464fde951515/download_pub&hl=en&sa=X&d=6216303975224490934&ei=r4g-aJ2KN6uM6rQPsd_4QA&scisig=AAZF9b_CFpDRiIYOH7r3kCGBQ-OW&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Learning the fine-grained code representation for log-level prediction", "first_label": ["Code"], "second_label": [], "data": "Z Zhao, G Fan, J Li, M Zhu, H Zhang, H Su\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0of King Saud University Computer and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLog levels are crucial to distinguish the severity of logs and directly reflecting the \nurgency of transactions in software systems. Automatically and efficiently determining \nlog levels is a crucial and challenging task in log management. Current log-level\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s44443-025-00064-9&hl=en&sa=X&d=2987968475870767202&ei=sIg-aKxgj5TLBNeTjtgD&scisig=AAZF9b-oMqC_G5jyOLyAbJstOPno&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "When Feelings Meet Code: How Generative AI Affects the Emotions of Developers", "first_label": ["Code"], "second_label": [], "data": "PH Jacquemin, M Gr\\xc3\\xa4f, K Bauch, A Kaur, M Mehler - 2025\nAbstract Generative Artificial Intelligence (GenAI) is transforming professional \nworkflows, particularly in programming, where tools like ChatGPT assist with code \ngeneration, debugging, and explanations. While GenAI enhances performance, \nconcerns about the implications for well-being and emotions while working with \nGenAI persist. Especially emotions in terms of positive and negative feelings play a \ncrucial role, influencing how effectively GenAI is utilized in professional settings. Our\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRefining chatgpt-generated code: Characterizing and mitigating\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://aisel.aisnet.org/amcis2025/sig_cnow/sig_cnow/4/&hl=en&sa=X&d=7801860713258796150&ei=QRo9aKG-B-W16rQPsqVy&scisig=AAZF9b81za4Ew4tCXAKwNS9XMbiN&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["2 new citations to articles by Bach Le", "2 new citations to articles by Thanh Le-Cong"]}
{"title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization", "first_label": ["Code"], "second_label": [], "data": "M Du, LT Tuan, Y Liu, Y Qing, D Huang, X He, Q Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) generate functionally correct solutions but often fall \nshort in code efficiency, a critical bottleneck for real-world deployment. In this paper, \nwe introduce a novel test-time iterative optimization framework to address this, \nemploying a closed-loop system where LLMs iteratively refine code based on \nempirical performance feedback from an execution sandbox. We explore three \ntraining strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaThanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23387&hl=en&sa=X&d=13812914505441854655&ei=QRo9aKG-B-W16rQPsqVy&scisig=AAZF9b9g8QfU6R_Ix12KNvgAxXM_&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=1&folt=cit", "author": ["Bach Le"], "ref": ["2 new citations to articles by Bach Le", "Hong Jin Kang - new related research", "Bach Le - new related research", "2 new citations to articles by Thanh Le-Cong", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution", "first_label": ["GitHub Issue"], "second_label": [], "data": "L Guo, W Tao, R Jiang, Y Wang, J Chen, X Liu, Y Ma\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe GitHub issue resolution task aims to resolve issues reported in repositories \nautomatically. With advances in large language models (LLMs), this task has gained \nincreasing attention, and several benchmarks are proposed to evaluate the issue\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.04606&hl=en&sa=X&d=17721785651242552030&ei=QBo9aOjqPKuM6rQPsd_4QA&scisig=AAZF9b9vdcHDALBkXHSBcEUw295J&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "first_label": ["LLM", "Code"], "second_label": [], "data": "F Yao, Z Wang, L Liu, J Cui, L Zhong, X Fu, H Mai\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation with large language models (LLMs), often termed vibe coding, is \nincreasingly adopted in production but fails to ensure code quality, particularly in \nsecurity (eg, SQL injection vulnerabilities) and maintainability (eg, missing type\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.22704&hl=en&sa=X&d=13720297852295427429&ei=QBo9aOjqPKuM6rQPsd_4QA&scisig=AAZF9b9qU1hJff-ona5ynwmi0w3O&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Hong Jin Kang - new related research", "Bach Le - new related research", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "LLM Performance for Code Generation on Noisy Tasks", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "R Sendyka, C Cabrera, A Paleyes, D Robinson\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThis paper investigates the ability of large language models (LLMs) to recognise and \nsolve tasks which have been obfuscated beyond recognition. Focusing on \ncompetitive programming and benchmark tasks (LeetCode and MATH), we compare\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23598&hl=en&sa=X&d=9429091642612761899&ei=QBo9aOjqPKuM6rQPsd_4QA&scisig=AAZF9b8Uzy7LaG1aYtKQ7svdsdTk&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical Systems", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "K Etemadi, M Sirjani, MH Moghadam, P Strandberg\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCyber-physical systems (CPSs) are complex systems that integrate physical, \ncomputational, and communication subsystems. The heterogeneous nature of these \nsystems makes their safety assurance challenging. In this paper, we propose a novel\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23549&hl=en&sa=X&d=15514814573872381684&ei=QRo9aM21IObGieoP1sTUqQ8&scisig=AAZF9b9PvEQ6SOyVFPOVv07Iu9o3&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "Model Immunization from a Condition Number Perspective", "first_label": [], "second_label": [], "data": "AY Zheng, CS Bai, B Bullins, RA Yeh\\xc2\\xa0- arXiv preprint arXiv:2505.23760, 2025\nModel immunization aims to pre-train models that are difficult to fine-tune on harmful \ntasks while retaining their utility on other non-harmful tasks. Though prior work has \nshown empirical evidence for immunizing text-to-image models, the key \nunderstanding of when immunization is possible and a precise definition of an \nimmunized model remain unclear. In this work, we propose a framework, based on \nthe condition number of a Hessian matrix, to analyze model immunization for linear\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23760&hl=en&sa=X&d=12827498787661563471&ei=QBo9aOX_OdOj6rQPzMDA4AM&scisig=AAZF9b92-QyEr3RAbR40r2G0KTF4&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["1 new citation to articles by Richard Fang"]}
{"title": "A Survey on the Impact of Pre-Trained Language Models in Sentiment Classification Task", "first_label": ["LLM"], "second_label": [], "data": "H Gautam, A Gaur, DK Yadav\\xc2\\xa0- International Journal of Data Science and Analytics, 2025\nThe evolution of pre-trained language models (PLMs) has significantly transformed \nthe landscape of sentiment analysis, particularly in handling complex, noisy, \ninformal, and short-text commonly found on social media. While numerous surveys \nhave explored PLMs and sentiment analysis separately, few provide a focused \nevaluation of large language models (LLMs) in the context of sentiment classification \nacross diverse, real-world datasets. This survey addresses that gap by systematically\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBiasfinder: Metamorphic test generation to uncover bias for\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s41060-025-00805-z&hl=en&sa=X&d=7299075348891807052&ei=QRo9aJWLBoqIieoPj_7kqQE&scisig=AAZF9b8raeFdei0FHjjeOkWKQ4PT&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=0&folt=cit", "author": ["Hong Jin Kang"], "ref": ["2 new citations to articles by Hong Jin Kang"]}
{"title": "Advancements in Brain Tumor Segmentation: A Comprehensive Review of the BraTS Challenges", "first_label": [], "second_label": [], "data": "RR Kumar, S Gupta, KR Chythanya, R Singh\\xe2\\x80\\xa6\\xc2\\xa0- 2025 3rd International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nBrain tumor segmentation is a critical task in medical imaging, essential for accurate \ndiagnosis, treatment planning, and monitoring of gliomas, including high-grade and \nlower-grade gliomas. The Brain Tumor Segmentation (BraTS) challenges, held \nannually from 2017 to 2022, have significantly advanced the field by providing \nstandardized datasets, evaluation frameworks, and fostering innovation in \nsegmentation methodologies. In review examines the progression of BraTS\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAndroEvolve: Automated Android API update with data flow\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11011433/&hl=en&sa=X&d=256630132479435327&ei=QRo9aJWLBoqIieoPj_7kqQE&scisig=AAZF9b-e0zYAc-ap3L4-7_LbYiqG&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=1&folt=cit", "author": ["Hong Jin Kang"], "ref": ["2 new citations to articles by Hong Jin Kang"]}
{"title": "From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents", "first_label": [], "second_label": ["Agent"], "data": "T Lindenbauer, G Groh, H Sch\\xc3\\xbctze\\xc2\\xa0- arXiv preprint arXiv:2505.23422, 2025\nWe introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on top of \nAutoCodeRover (Zhang et al., 2024) that extends agentic reasoning frameworks with \nan episodic memory, more specifically, a general and repository-level Cross-Task-\nInstance Memory (CTIM). While existing open-source SE agents mostly rely on ReAct \n(Yao et al., 2023b), Reflexion (Shinn et al., 2023), or Code-Act (Wang et al., 2024), all \nof these reasoning and planning frameworks inefficiently discard their long-term\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutoCodeRover: Autonomous program improvement\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23422&hl=en&sa=X&d=798671286471025813&ei=QRo9aM-PD4OuieoP-ZTn2Aw&scisig=AAZF9b9I4avC7qYzBaHdUFuv9o3o&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Analisis Kinerja Pengujian Black Box Terhadap Website Nestify Setelah Implementasi Equivalence Partitioning dan Boundary Value Analysis", "first_label": [], "second_label": [], "data": "AR Wicaksono\\xc2\\xa0- Jurnal Informatika dan Teknologi Pendidikan, 2025\nAplikasi Nestify merupakan aplikasi berbasis website yang dirancang untuk \nmemfasilitasi pengelolaan properti indekos, baik bagi pemilik maupun penyewa. \nUntuk memastikan aplikasi ini berjalan sesuai dengan kebutuhan fungsional yang \ndigambarkan pada use case diagram, maka dilakukan pengujian menggunakan \nmetode Black Box dengan teknik Equivalence Partitioning (EP) dan Boundary Value \nAnalysis (BVA). Kasus uji didesain dengan pendekatan berbasis use case untuk\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaTest-equivalence analysis for automatic patch generation\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://jurnalitp.web.id/index.php/jitp/article/download/131/55&hl=en&sa=X&d=3769664691200650921&ei=QRo9aM-PD4OuieoP-ZTn2Aw&scisig=AAZF9b9j5Ymbdvd1EtdF5Zw192qs&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation", "first_label": ["Fuzzing", "Code"], "second_label": [], "data": "D Paa\\xc3\\x9fen, JR Giesen, L Davi\\xc2\\xa0- arXiv preprint arXiv:2505.02464, 2025\nRust is a promising programming language that focuses on concurrency, usability, \nand security. It is used in production code by major industry players and got \nrecommended by government bodies. Rust provides strong security guarantees\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.02464&hl=en&sa=X&d=17464682910892089266&ei=QRo9aNfcELeC6rQP9sj2mQ8&scisig=AAZF9b-bvKBMc2Po_9BYZAm8NsXQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "VERINA: Benchmarking Verifiable Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "Z Ye, Z Yan, J He, T Kasriel, K Yang, D Song\\xc2\\xa0- arXiv preprint arXiv:2505.23135, 2025\nLarge language models (LLMs) are increasingly integrated in software development, \nbut ensuring correctness in LLM-generated code remains challenging and often \nrequires costly manual review. Verifiable code generation--jointly generating code\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23135&hl=en&sa=X&d=17629286340878649896&ei=QRo9aNfcELeC6rQP9sj2mQ8&scisig=AAZF9b886dx7UbzS0zTtCozpRFS_&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research", "Xin ZHOU - new related research"]}
{"title": "SWE-bench Goes Live!", "first_label": [], "second_label": [], "data": "L Zhang, S He, C Zhang, Y Kang, B Li, C Xie, J Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe issue-resolving task, where a model generates patches to fix real-world bugs, \nhas emerged as a critical benchmark for evaluating the capabilities of large \nlanguage models (LLMs). While SWE-bench and its variants have become standard\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23419&hl=en&sa=X&d=1575278786032398107&ei=QRo9aNfcELeC6rQP9sj2mQ8&scisig=AAZF9b8tq3CrWVTGRUf3cVhoSLzm&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Comprehensive Predictive Analytics for Collaborators' Answers, Code Quality, and Dropout: Stack Overflow Case Study\\xe2\\x80\\x93Replication Package", "first_label": ["Code"], "second_label": [], "data": "E Zolduoarrati, S Licorish, N Stanger - 2025\nPrevious studies that used data from Stack Overflow to develop predictive models \noften employed limited benchmarks of 3-5 models or adopted arbitrary selection \nmethods. Despite being insightful, such approaches may not provide optimal results\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ourarchive.otago.ac.nz/esploro/outputs/dataset/Comprehensive-Predictive-Analytics-for-Collaborators-Answers/9926743737901891&hl=en&sa=X&d=4622743116783783239&ei=QRo9aK2LCYCt6rQP9sPUwAg&scisig=AAZF9b9T5D8iF5mUZMDOLFoxHQ1B&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "Z Yuan, Y Xu, J Shi, P Zhou, L Sun\\xc2\\xa0- arXiv preprint arXiv:2505.23561, 2025\nModel merging for Large Language Models (LLMs) directly fuses the parameters of \ndifferent models finetuned on various tasks, creating a unified model for multi-domain \ntasks. However, due to potential vulnerabilities in models available on open-source\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23561&hl=en&sa=X&d=6617189016454156514&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b-hKaNf3ql6GqPXtJeaxsCC&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "R-bench: Graduate-level multi-disciplinary benchmarks for llm & mllm complex reasoning evaluation", "first_label": ["LLM"], "second_label": ["Reasoning"], "data": "MH Guo, J Xu, Y Zhang, J Song, H Peng, YX Deng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nReasoning stands as a cornerstone of intelligence, enabling the synthesis of existing \nknowledge to solve complex problems. Despite remarkable progress, existing \nreasoning benchmarks often fail to rigorously evaluate the nuanced reasoning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.02018%3F&hl=en&sa=X&d=8843826795691330105&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b8oZTpHhJulcZKPuCXTowVa&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Superplatforms Have to Attack AI Agents", "first_label": [], "second_label": ["Agent"], "data": "J Lin, J Zhu, Z Zhou, Y Xi, W Liu, Y Yu, W Zhang\\xc2\\xa0- arXiv preprint arXiv:2505.17861, 2025\nOver the past decades, superplatforms, digital companies that integrate a vast range \nof third-party services and applications into a single, unified ecosystem, have built \ntheir fortunes on monopolizing user attention through targeted advertising and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.17861%3F&hl=en&sa=X&d=3066941296948922789&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b8JpE5325HDRkA5ocvOdQDq&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "first_label": ["LLM"], "second_label": [], "data": "S Cappelletti, T Poppi, S Poppi, ZX Yong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) are increasingly evaluated on multiple-choice \nquestion answering (MCQA) tasks using* first-token probability*(FTP), which selects \nthe answer option whose initial token has the highest likelihood. While efficient, FTP\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15323%3F&hl=en&sa=X&d=5513891085179250670&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b8ts6vJnVnPKgZRZk2J59vT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based Retrieval-Augmented Recommender Systems", "first_label": ["LLM"], "second_label": [], "data": "F Nazary, Y Deldjoo, T Di Noia, E Di Sciascio\\xc2\\xa0- arXiv preprint arXiv:2505.05196, 2025\nWe present a systematic study of provider-side data poisoning in retrieval-\naugmented recommender systems (RAG-based). By modifying only a small fraction \nof tokens within item descriptions--for instance, adding emotional keywords or\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.05196&hl=en&sa=X&d=7153996738322631678&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b9XYvMXz8xtJ6dU7EDwB2B7&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "PS Pandey, S Simko, K Pelrine, Z Jin\\xc2\\xa0- arXiv preprint arXiv:2505.16789, 2025\nAs large language models gain popularity, their vulnerability to adversarial attacks \nremains a primary concern. While fine-tuning models on domain-specific datasets is \noften employed to improve model performance, it can introduce vulnerabilities within\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.16789%3F&hl=en&sa=X&d=9827187220639785834&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b-hRa3CIybDfceUhaRlecWD&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution", "first_label": [], "second_label": [], "data": "J Guo, X Wen, W Jiang, C Huang, J Li, H Li\\xc2\\xa0- arXiv preprint arXiv:2505.15308, 2025\nWith the widespread application of super-resolution (SR) in various fields, \nresearchers have begun to investigate its security. Previous studies have \ndemonstrated that SR models can also be subjected to backdoor attacks through\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15308&hl=en&sa=X&d=13335437674086498390&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b8WmdvUXiuzvPutlRNCBmNP&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "K Zhu, J Zhang, Z Qi, N Shang, Z Liu, P Han, Y Su\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advancements in large language model (LLM) agents have significantly \naccelerated scientific discovery automation, yet concurrently raised critical ethical \nand safety concerns. To systematically address these challenges, we introduce\\\\textbf\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23559&hl=en&sa=X&d=11195872190670696763&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b-GHbm178P42FhdzzUwF79Z&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions", "first_label": ["LLM"], "second_label": [], "data": "X Wu, X Mao, F Li, X Zhang, X Zhang, J Zhou, Y Peng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) frequently refuse to respond to pseudo-malicious \ninstructions: semantically harmless input queries triggering unnecessary LLM \nrefusals due to conservative safety alignment, significantly impairing user\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23473&hl=en&sa=X&d=12108138930043984920&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b-1Z--NymRfljISu6T1JjVD&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Transferable adversarial attacks on black-box vision-language models", "first_label": ["LLM"], "second_label": [], "data": "K Hu, W Yu, L Zhang, A Robey, A Zou, C Xu, H Hu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nVision Large Language Models (VLLMs) are increasingly deployed to offer \nadvanced capabilities on inputs comprising both text and images. While prior \nresearch has shown that adversarial attacks can transfer from open-source to\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.01050&hl=en&sa=X&d=34750902821152867&ei=QRo9aLeVGcmx6rQP-rCh4AE&scisig=AAZF9b9lHgWBmRQgOUsWiSdzwfqQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors", "first_label": ["LLM"], "second_label": [], "data": "HT Madabushi, M Torgbi, C Bonial\\xc2\\xa0- arXiv preprint arXiv:2505.23323, 2025\nIn this position paper we raise critical awareness of a realistic view of LLM \ncapabilities that eschews extreme alternative views that LLMs are either\" stochastic \nparrots\" or in possession of\" emergent\" advanced reasoning capabilities, which, due \nto their unpredictable emergence, constitute an existential threat. Our middle-ground \nview is that LLMs extrapolate from priors from their training data, and that a \nmechanism akin to in-context learning enables the targeting of the appropriate\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLessLeak-Bench: A First Investigation of Data Leakage in LLMs\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23323&hl=en&sa=X&d=12118225113071950285&ei=QRo9aPGrF8y8ieoPz-jf4A8&scisig=AAZF9b8PFXYScS5n4hctPiMxQ3wV&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["2 new citations to articles by Xin ZHOU"]}
{"title": "Software Vulnerability Detection: Trends, Gaps, and Future Directions in IS Research", "first_label": ["Vulnerabilities"], "second_label": ["Detection", "Search"], "data": "R Karri, P Kumar, N Islam - 2025\nWith the rise of cyber threats, Software Vulnerability Detection (SVD) plays a vital role \nin ensuring software security and reliability. The rapid growth of research in this \ndomain necessitates a comprehensive understanding of the overall research \nlandscape, trends, gaps and future directions. Existing literature studies remained \nfragmented across various subdomains. And traditional methods of analyzing vast \nbody of research manually are challenging and time consuming. To address these\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model for vulnerability detection and repair\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://aisel.aisnet.org/amcis2025/intelfuture/intelfuture/42/&hl=en&sa=X&d=14638189496755740306&ei=QRo9aPGrF8y8ieoPz-jf4A8&scisig=AAZF9b_BIqgK8kFFL_pGq5_iT-il&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=1&folt=cit", "author": ["Xin ZHOU"], "ref": ["2 new citations to articles by Xin ZHOU"]}

{"title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Agent", "Reasoning"], "data": "Y Nie, H Li, C Guo, R Jiang, Z Wang, B Li, D Song- arXiv preprint arXiv, 2025\nWe propose VulnLLM-R, the~\\emph {first specialized reasoning LLM} for \nvulnerability detection. Our key insight is that LLMs can reason about program states \nand analyze the potential vulnerabilities, rather than simple pattern matching. This\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.07533&hl=en&sa=X&d=14822866035105946059&ei=iUZVadm5O-6TieoPutPYsQg&scisig=ALhkC2SrItwEGbUy2HNxI0DJG3ud&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:ALhkC2QkVwCdvNzUylYiTyCmheSL&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "A Context-Aware Lightweight Framework for Source Code Vulnerability Detection", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection"], "data": "Y Sanjalawe, B Allehyani, S Al-E'mari- Future Internet, 2025\nAs software systems grow increasingly complex and interconnected, detecting \nvulnerabilities in source code has become a critical and challenging task. Traditional \nstatic analysis methods often fall short in capturing deep, context-dependent\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/1999-5903/17/12/557&hl=en&sa=X&d=12580835281555547371&ei=ikZVaZmwHe-GieoPo9DluQE&scisig=ALhkC2RkBKy4uqBLI_VWYmJI5TaR&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:ALhkC2THo_3s8tBI3qmyNrpmj0cv&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "What Makes Code Generation Ethically Sourced?", "first_label": ["Code"], "second_label": ["Generation"], "data": "Z Xu, C Li, Q Li, SH Tan - 2026\nSeveral code generation models have been proposed to help reduce time and effort \nin solving software-related tasks. To ensure responsible AI, there are growing \ninterests over various ethical issues (eg, unclear licensing, privacy, fairness, and \nenvironment impact). These studies have the overarching goal of ensuring ethically \nsourced generation, which has gained growing attention in speech synthesis and \nimage generation. In this paper, we introduce the novel notion of Ethically Sourced\nCites: Automated Program Repair", "link": "https://scholar.google.com/scholar_url?url=https://www.shinhwei.com/ICSE26Paper.pdf&hl=en&sa=X&d=13119151750221247818&ei=cGhTadSFFrK16rQPn86DwQ8&scisig=ALhkC2R7ixMrv3JAoWPWbXS9lJbs&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:ALhkC2SLNtxsIYV7y8T6Ni2J4ruF&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Explorao de vulnerabilidade do tipo RCE", "first_label": [], "second_label": [], "data": "JLM Almeida - 2025\nINFORMTICA Page 1 Explorao de vulnerabilidade do tipo RCE Jos Lus \n\r\nAlmeida Dissertao para obteno do Grau de Mestre em INFORMTICA Jri \n\r\nPresidente: Professor Doutor Paulo Andr Reis Duarte Branco Arguente: Professora \n\r\nDoutora Carla Sofia Rocha da Silva Orientador: Professor Doutor Pedro Ramos dos \n\r\nSantos Brando Outubro, 2025 Page 2 Page 3 ISTEC Instituto Superior de \n\r\nTecnologias Avanadas Campus Acadmico do Lumiar, Lisboa Dissertao\nCites: Coverage-based greybox fuzzing as markov chain\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://comum.rcaap.pt/bitstreams/f951f046-1fd6-4618-b129-0a2c887271da/download&hl=en&sa=X&d=13226687935301171751&ei=cGhTadSFFrK16rQPn86DwQ8&scisig=ALhkC2RLEDkrU8x9p0IOKbaTjFBa&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:ALhkC2SLNtxsIYV7y8T6Ni2J4ruF&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Avoiding Label Leakage in AI Risk ModelsA Shared Responsibility for a Pervasive Problem", "first_label": [], "second_label": [], "data": "ME Matheny, SE Davis- JAMA Network Open, 2025\nRamadan et al1 provide a compelling narrative about the prevalence of label \nleakage in artificial intelligence (AI) and machine learning (ML) prediction modeling \ndelivered in 2 complementary analyses. First, they reveal that 40.2%(37 of 92 \narticles) of AI and ML modeling studies using Medical Information Mart for Intensive \nCare (MIMIC) data to predict events during the same hospitalization relied on \nInternational Classification of Diseases (ICD) with clinical modification diagnostic\nCites: LessLeak-Bench: A First Investigation of Data Leakage in LLMs\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2843183&hl=en&sa=X&d=4377683669285248679&ei=cGhTaYDZNZ6u6rQPotyMwAQ&scisig=ALhkC2RN6gsXKfsAE7JjFhmCyDST&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:ALhkC2S1ZxbvKZLR2YX67Bvdxjyf&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["1 new citation to articles by Xin ZHOU"]}
{"title": "Software Refactoring Research with Large Language Models: A Systematic Literature Review", "first_label": ["LLM"], "second_label": ["Search"], "data": "S Martinez, L Xu, M Elnaggar, EA Alomar- Journal of Systems and Software, 2025\nBackground: Code refactoring is the improvement of code internally without \nchanging the external functionalities of the program. Due to its exhaustive nature, \ndevelopers often avoid manually refactoring code. Researchers have since looked \ninto utilizing Large Language Models (LLMs) to automate the task of refactoring. Aim \nand Method: Despite the promising results, there is a lack of clear understanding of \nLLMs' effectiveness in automated refactoring. In order to address this issue, we\nCites: Refining chatgpt-generated code: Characterizing and mitigating\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0164121225004315&hl=en&sa=X&d=12557036843975764566&ei=bmhTaZPfNdrJieoPiYyysAk&scisig=ALhkC2SBjbS6YxnyRP-hweQINzrf&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:ALhkC2SLna-N2qwXFiQXiEQOdUM_&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["1 new citation to articles by Thanh Le-Cong", "1 new citation to articles by Bach Le"]}
{"title": "What You Trust Is Insecure: Demystifying How Developers (Mis) Use Trusted Execution Environments in Practice", "first_label": [], "second_label": [], "data": "Y Niu, J Shi, R Han, Y Liu, C Ma, Y Lyu, D Lo- arXiv preprint arXiv:2512.17363, 2025\nTrusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, \nprovide isolated regions of CPU and memory for secure computation and are \nincreasingly used to protect sensitive data and code across diverse application", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.17363&hl=en&sa=X&d=5012790250967918809&ei=cWhTaeTNB9_OieoPg6WE2QY&scisig=ALhkC2S7b8VBJNYtqWvLkTWa12R7&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:ALhkC2T269WwPtyd5qvti3WNZV40&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework", "first_label": ["LLM"], "second_label": [], "data": "A Parziale, G Voria, V Pontillo, G Catolino, A De Lucia- arXiv preprint arXiv, 2025\nNowadays, Large Language Models (LLMs) are foundational components of modern \nsoftware systems. As their influence grows, concerns about fairness have become \nincreasingly pressing. Prior work has proposed metamorphic testing to detect\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.16816&hl=en&sa=X&d=13945611357990131720&ei=cWhTaeTNB9_OieoPg6WE2QY&scisig=ALhkC2SJ3_WKXbKgYOI4O253XC6G&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:ALhkC2T269WwPtyd5qvti3WNZV40&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review", "first_label": ["LLM", "Code Review", "Code"], "second_label": [], "data": "K Wang, B Mao, S Jia, Y Ding, D Han, T Ma, B Cao- arXiv preprint arXiv:2512.17540, 2025\nAutomating code review with Large Language Models (LLMs) shows immense \npromise, yet practical adoption is hampered by their lack of reliability, context-\nawareness, and control. To address this, we propose Specification-Grounded Code\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.17540&hl=en&sa=X&d=16382589271726736633&ei=cGhTacSwBeqsieoPjYOSgAM&scisig=ALhkC2Rx_2I5F67w4_nJO52_b37Z&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:ALhkC2THo_3s8tBI3qmyNrpmj0cv&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Tuning LLM in secure code generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "DS Shaikhelislamov, MS Varetsa, AS Syomkin-  , 2025\nThe popularity of using LLM for code generation makes it mandatory to \ncomprehensively verify the security and reliability of the generated code. To verify the \ngenerated code, it is suggested to use the static analyzer Svace, which checks the\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.mathnet.ru/rus/tisp1045&hl=en&sa=X&d=5765086413756448946&ei=b2hTac6GH56u6rQPotyMwAQ&scisig=ALhkC2RNVNU28flijldY4h73JER0&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:ALhkC2RZmRqgZGCFQXCJyP9vthGu&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "first_label": ["LLM", "Fault Localization"], "second_label": ["Localization"], "data": "H Xu, H Liu, Y Wu, X Kang, X Chen, Y Liu- Journal of Systems and Software, 2025\nNovice programmers often face challenges in fault localization due to their limited \nexperience and understanding of programming syntax and logic. Traditional \nmethods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.03421&hl=en&sa=X&d=14474168845101817028&ei=b2hTaZ-UDO-GieoPo9DluQE&scisig=ALhkC2Sh55MR8Jvam3drU0L05GmR&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:ALhkC2QkVwCdvNzUylYiTyCmheSL&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}

{"title": "We've Got You Covered: Type-Guided Repair of Incomplete Input Generators", "first_label": [], "second_label": ["Repair"], "data": "P LaFontaine, Z Zhou, A Mishra, S Jagannathan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nProperty-based testing is a popular technique for automatically testing semantic \nproperties of a program, specified as a pair of pre-and post-conditions. The efficacy \nof this approach depends on being able to quickly generate inputs that meet the \nprecondition, in order to maximize the set of program behaviors that are probed. For \nsemantically rich preconditions, purely random generation is unlikely to produce \nmany valid inputs; when this occurs, users are forced to manually write their own\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaS3: Syntax- and Semantic-Guided Repair Synthesis via\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.06421&hl=en&sa=X&d=4537435868289132628&ei=3tz6Z9nVBpWz6rQPveLUmAo&scisig=AFWwaeZY_M8Dee75x_6YAOvVVoRP&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AFWwaebZb4G2z_XAHxtUtGUOv8go&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Bach Le"]}
{"title": "Using ML filters to help automated vulnerability repairs: when it helps and when it doesn't", "first_label": ["Vulnerabilities"], "second_label": ["Repair"], "data": "M Camporese, F Massacci\\xc2\\xa0- arXiv preprint arXiv:2504.07027, 2025\n[Context:] The acceptance of candidate patches in automated program repair has \nbeen typically based on testing oracles. Testing requires typically a costly process of \nbuilding the application while ML models can be used to quickly classify patches, \nthus allowing more candidate patches to be generated in a positive feedback \nloop.[Problem:] If the model predictions are unreliable (as in vulnerability detection) \nthey can hardly replace the more reliable oracles based on testing.[New Idea:] We\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaOverfitting in Semantics-based Automated Program Repair\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.07027&hl=en&sa=X&d=8016374535122276286&ei=3tz6Z9nVBpWz6rQPveLUmAo&scisig=AFWwaeaaxwymeGrLM10a1ld5ymOL&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AFWwaebZb4G2z_XAHxtUtGUOv8go&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Bach Le", "Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns", "first_label": [], "second_label": ["Localization"], "data": "D Kim, M Jeon, D Hwang, H Oh\\xc2\\xa0- Proceedings of the ACM on Programming\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe present PAFL, a new technique for enhancing existing fault localization methods \nby leveraging project-specific fault patterns. We observed that each software project \nhas its own challenges and suffers from recurring fault patterns associated with those\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3720526&hl=en&sa=X&d=18081862642413033156&ei=3tz6Z-7oA5yV6rQP8q-dkA0&scisig=AFWwaebn1piFwIZdyjc2aafeRxly&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=0&folt=rel", "ref": ["Thanh Le-Cong - new related research", "Bach Le - new related research", "3 new citations to articles by Hong Jin Kang"]}
{"title": "RustEvo^ 2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "L Liang, J Gong, M Liu, C Wang, G Ou, Y Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have become pivotal tools for automating code \ngeneration in software development. However, these models face significant \nchallenges in producing version-aware code for rapidly evolving languages like\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16922%3F&hl=en&sa=X&d=16675224894932160996&ei=3tz6Z-7oA5yV6rQP8q-dkA0&scisig=AFWwaea4g_0zH5bNDjn1WSqwrFGh&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "LLM Test Generation via Iterative Hybrid Program Analysis", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "S Gu, N Nashid, A Mesbah\\xc2\\xa0- arXiv preprint arXiv:2503.13580, 2025\nAutomating unit test generation remains a significant challenge, particularly for \ncomplex methods in real-world projects. While Large Language Models (LLMs) have \nmade strides in code generation, they struggle to achieve high branch coverage due\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.13580&hl=en&sa=X&d=207857029493572923&ei=3tz6Z-7oA5yV6rQP8q-dkA0&scisig=AFWwaeYzdRQn3PjIc4QGlsuh6_K4&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=3&folt=rel", "ref": ["Thanh Le-Cong - new related research", "Bach Le - new related research", "Hong Jin Kang - new related research", "Richard Fang - new related research"]}
{"title": "Smart Contract Vulnerability Detection Using Large Language Models and Graph Structural Analysis", "first_label": ["Vulnerabilities", "Smart Contracts", "LLM"], "second_label": ["Detection"], "data": "RY Choi, Y Song, M Jang, T Kim, J Ahn, DH Im\\xc2\\xa0- Computers, Materials and Continua, 2025\nSmart contracts are self-executing programs on blockchains that manage complex \nbusiness logic with transparency and integrity. However, their immutability after \ndeployment makes programming errors particularly critical, as such errors can be\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/org/science/article/pii/S1546221825002504&hl=en&sa=X&d=4504820058459498391&ei=3tz6Z-7oA5yV6rQP8q-dkA0&scisig=AFWwaeZMeaniylWwEGOyOdrF9Ulz&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=4&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "first_label": ["LLM", "Code"], "second_label": ["Agent", "Localization"], "data": "Z Chen, X Tang, G Deng, F Wu, J Wu, Z Jiang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode localization--identifying precisely where in a codebase changes need to be \nmade--is a fundamental yet challenging task in software maintenance. Existing \napproaches struggle to efficiently navigate complex codebases when identifying\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.09089%3F&hl=en&sa=X&d=276668227003209980&ei=3tz6Z-7oA5yV6rQP8q-dkA0&scisig=AFWwaebrlhR4TOxAKNYgDQ0UAK82&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=5&folt=rel", "ref": ["Thanh Le-Cong - new related research", "David Lo - new related research"]}
{"title": "Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights", "first_label": ["LLM"], "second_label": [], "data": "T Khan, S Motie, SA Kocak, S Raza\\xc2\\xa0- arXiv preprint arXiv:2504.06307, 2025\nThe rapid adoption of large language models (LLMs) has led to significant energy \nconsumption and carbon emissions, posing a critical challenge to the sustainability \nof generative AI technologies. This paper explores the integration of energy-efficient \noptimization techniques in the deployment of LLMs to address these environmental \nconcerns. We present a case study and framework that demonstrate how strategic \nquantization and local inference techniques can substantially lower the carbon\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaGreening large language models of code\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.06307&hl=en&sa=X&d=6387174928915548778&ei=3tz6Z_agBY-j6rQPo73a4AM&scisig=AFWwaeZsppe_Gg9AId0scfgh7vgW&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AFWwaeZ0cPfysy_B7V1I3HcGE9Io&html=&pos=0&folt=cit", "ref": ["3 new citations to articles by Hong Jin Kang"]}
{"title": "DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction", "first_label": ["Code"], "second_label": ["Detection"], "data": "Y Xiao, Y Chen, S Ma, H Huang, C Fang, Y Chen\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWatermarking is a technique to help identify the source of data points, which can be \nused to help prevent the misuse of protected datasets. Existing methods on code \nwatermarking, leveraging the idea from the backdoor research, embed stealthy \ntriggers as watermarks. Despite their high resilience against dilution attacks and \nbackdoor detections, the robustness has not been fully evaluated. To fill this gap, we \npropose DeCoMa, a dual-channel approach to Detect and purify Code dataset\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaStealthy backdoor attack for code models\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.07002&hl=en&sa=X&d=15919972095243974215&ei=3tz6Z_agBY-j6rQPo73a4AM&scisig=AFWwaeYiPCmm16yztgJgTf31LLj0&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AFWwaeZ0cPfysy_B7V1I3HcGE9Io&html=&pos=1&folt=cit", "ref": ["3 new citations to articles by Hong Jin Kang"]}
{"title": "VDMAF: Cross-language source code vulnerability detection using multi-head attention fusion", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection"], "data": "Y Li, Q Luo, P Wu, H Zheng\\xc2\\xa0- Information and Software Technology, 2025\nContext: Detecting potential vulnerabilities is critical for ensuring the stability and \nreliability of software systems. Traditional static detection methods fall short in \naccuracy and efficiency. Furthermore, existing deep learning-based vulnerability\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584925000783&hl=en&sa=X&d=1379216563488707466&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaeb0mECFrsofCEyewySgPyJO&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research", "David Lo - new related research"]}
{"title": "Enhancing llm code generation with ensembles: A similarity-based selection approach", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "T Mahmud, B Duan, C Pasareanu, G Yang\\xc2\\xa0- arXiv preprint arXiv:2503.15838, 2025\nEnsemble learning has been widely used in machine learning to improve model \nrobustness, accuracy, and generalization, but has not yet been applied to code \ngeneration tasks with large language models (LLMs). We propose an ensemble\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15838&hl=en&sa=X&d=6315180162277152648&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaeYw_s8WAmNdUs0l_UHZtKix&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "A Comprehensive Study of LLM Secure Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "SC Dai, J Xu, G Tao\\xc2\\xa0- arXiv preprint arXiv:2503.15554, 2025\nLLMs are widely used in software development. However, the code generated by \nLLMs often contains vulnerabilities. Several secure code generation methods have \nbeen proposed to address this issue, but their current evaluation schemes leave\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15554%3F&hl=en&sa=X&d=1363137439941333778&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaea70lzbx-2dnx-SlddbyIDc&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Automated Harmfulness Testing for Code Large Language Models", "first_label": ["LLM", "Code", "Software Testing"], "second_label": [], "data": "H Tan, H Wang, D Pressato, Y Xu, SH Tan\\xc2\\xa0- arXiv preprint arXiv:2503.16740, 2025\nGenerative AI systems powered by Large Language Models (LLMs) usually use \ncontent moderation to prevent harmful content spread. To evaluate the robustness of \ncontent moderation, several metamorphic testing techniques have been proposed to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16740&hl=en&sa=X&d=9984898886618882508&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaeY9TKp5hroYtjAXp0K1dUfY&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks", "first_label": ["LLM", "Code"], "second_label": ["Repair"], "data": "D Dai, MW Liu, A Li, J Cao, Y Wang, C Wang, X Peng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode repair is a fundamental task in software development, facilitating efficient bug \nresolution and software maintenance. Although large language models (LLMs) have \ndemonstrated considerable potential in automated code repair, their ability to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.06939&hl=en&sa=X&d=3630442079653699433&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaeakmawt1RZy7sc-t0c4bk9f&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research", "Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "CKTyper: Enhancing Type Inference for Java Code Snippets by Leveraging Crowdsourcing Knowledge in Stack Overflow", "first_label": ["Code"], "second_label": [], "data": "A LI, N ZHANG, Y ZOU, Z CHEN, J WANG, Z ZHENG - 2025\nThere are typically two categories of approaches proposed for inferring the types of \nAPIs in CSs, including constraint-based [7, 23, 24] and statistically-based [11, 17, 21, \n26]. Constraint-based approaches pre-build a knowledge base (KB) that contains\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://seal-queensu.github.io/publications/pdf/FSE-Anji-25.pdf&hl=en&sa=X&d=7004309645188894300&ei=3tz6Z-HDDvCj6rQPwoSn6A4&scisig=AFWwaeYyrjqaFJaDUk1WxX0Qr9lX&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in Repository-level Code Completion", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "J Li, H Zhu, H Liu, X Shi, H Zong, Y Dong, K Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRepository-level code completion aims to complete code based on the long contexts \nof the repository. Existing studies extract long contexts from the repository as inputs \nand leverage Large Language Models (LLMs) to generate code. However, we reveal\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15301&hl=en&sa=X&d=15788931866516891147&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeYi_7K3zAnZWQ5kRBngFmjW&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Does context matter? contextualjudgebench for evaluating llm-based judges in contextual settings", "first_label": ["LLM"], "second_label": [], "data": "A Xu, S Bansal, Y Ming, S Yavuz, S Joty\\xc2\\xa0- arXiv preprint arXiv:2503.15620, 2025\nThe large language model (LLM)-as-judge paradigm has been used to meet the \ndemand for a cheap, reliable, and fast evaluation of model outputs during AI system \ndevelopment and post-deployment monitoring. While judge models--LLMs finetuned\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15620&hl=en&sa=X&d=4694277635380098830&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeZWWBCgHzhwdV7rV4U7IIiY&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique", "first_label": ["LLM"], "second_label": [], "data": "Y Li, J Xu, T Liang, X Chen, Z He, Q Liu, R Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nEnhancing the reasoning capabilities of large language models (LLMs), particularly \nfor complex tasks requiring multi-step logical deductions, remains a significant \nchallenge. Traditional inference time scaling methods utilize scalar reward signals\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17363%3F&hl=en&sa=X&d=11521510970677819720&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeZ1FoGj3fIwjgWCbOxO43ev&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "first_label": ["LLM"], "second_label": [], "data": "K Cheng, W Song, J Fan, Z Ma, Q Sun, F Xu, C Yan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nImage captioning has been a longstanding challenge in vision-language research. \nWith the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed \nand comprehensive image descriptions. However, benchmarking the quality of such\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12329%3F&hl=en&sa=X&d=407472380549964642&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeYceA6AtNeTroAmdGm1ugPz&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "first_label": ["LLM"], "second_label": [], "data": "O Shaikh, H Mozannar, G Bansal, A Fourney, E Horvitz\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLanguage models excel at following instructions but often struggle with the \ncollaborative aspects of conversation that humans naturally employ. This limitation in \ngrounding--the process by which conversation participants establish mutual\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.13975&hl=en&sa=X&d=6056628797355529511&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeYuzwNKVc6ZCL0NUy7wCU7P&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills", "first_label": [], "second_label": ["Agent"], "data": "B Zheng, MY Fatemi, X Jin, ZZ Wang, A Gandhi\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nTo survive and thrive in complex environments, humans have evolved sophisticated \nself-improvement mechanisms through environment exploration, hierarchical \nabstraction of experiences into reuseable skills, and collaborative construction of an\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.07079&hl=en&sa=X&d=13301755129364600749&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeY7dZcJ4SFWTXTfStwkKDuq&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Task-to-Instance Prompt Learning for Vision-Language Models at Test Time", "first_label": ["Software Testing"], "second_label": [], "data": "Z Lu, J Bai, X Li, Z Xiao, X Wang\\xc2\\xa0- IEEE Transactions on Image Processing, 2025\nPrompt learning has been recently introduced into the adaption of pre-trained vision-\nlanguage models (VLMs) by tuning a set of trainable tokens to replace hand-crafted \ntext templates. Despite the encouraging results achieved, existing methods largely\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10925517/&hl=en&sa=X&d=7061085335833209582&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeauALB5Xz0dtHolj6VGw3Nk&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Auditing language models for hidden objectives", "first_label": [], "second_label": [], "data": "S Marks, J Treutlein, T Bricken, J Lindsey, J Marcus\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe study the feasibility of conducting alignment audits: investigations into whether \nmodels have undesired objectives. As a testbed, we train a language model with a \nhidden objective. Our training pipeline first teaches the model about exploitable\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10965%3F&hl=en&sa=X&d=1501136133567193109&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeadoHBh04I6TvMcoaA7ksCs&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "first_label": [], "second_label": [], "data": "Y Ma, S Chern, X Shen, Y Zhong, P Liu\\xc2\\xa0- arXiv preprint arXiv:2504.02587, 2025\nReinforcement learning (RL) has recently shown strong potential in improving the \nreasoning capabilities of large language models and is now being actively extended \nto vision-language models (VLMs). However, existing RL applications in VLMs often\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.02587&hl=en&sa=X&d=3642027329340221236&ei=3tz6Z9W1C8uZieoPg4fsmAg&scisig=AFWwaeb-XYmW-h1bslhyY7wBzqSK&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "CCCI: Code Completion with Contextual Information for Complex Data Transfer Tasks Using Large Language Models", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "H Jin, M Hamdaqa\\xc2\\xa0- arXiv preprint arXiv:2503.23231, 2025\nUnlike code generation, which involves creating code from scratch, code completion \nfocuses on integrating new lines or blocks of code into an existing codebase. This \nprocess requires a deep understanding of the surrounding context, such as variable\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.23231&hl=en&sa=X&d=4201012503536136494&ei=3tz6Z-ySDZuw6rQPjdfcmQQ&scisig=AFWwaeaTwcJgPIAurLq4aX4MyRvJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AFWwaeZADCuvrSiGaZ1pge7b9bMB&html=&pos=2&folt=rel", "ref": ["Xin ZHOU - new related research"]}
{"title": "Benchmarking Failures in Tool-Augmented Language Models", "first_label": [], "second_label": [], "data": "E Trevi\\xc3\\xb1o, H Contant, J Ngai, G Neubig, ZZ Wang\\xc2\\xa0- arXiv preprint arXiv:2503.14227, 2025\nThe integration of tools has extended the capabilities of language models (LMs) \nbeyond vanilla text generation to versatile scenarios. However, tool-augmented \nlanguage models (TaLMs) often assume'perfect'information access and tool\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14227&hl=en&sa=X&d=12864745453246544967&ei=3tz6Z7yNCNSyieoP2ane-QQ&scisig=AFWwaeZOi6uJnH7_v5ou5yMqdUFy&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=3&folt=rel", "ref": ["David Lo - new related research"]}
{"title": "How Effectively Do LLMs Extract Feature-Sentiment Pairs from App Reviews?", "first_label": ["LLM"], "second_label": [], "data": "FA Shah, A Sabir, R Sharma, D Pfahl\\xc2\\xa0- International Working Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAbstract [Motivation] Automatic analysis of user reviews to understand user \nsentiments toward app functionality (ie. app features) helps align development efforts \nwith user expectations and needs. Recent advances in Large Language Models\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-88531-0_9&hl=en&sa=X&d=16503861718654765605&ei=3tz6Z7yNCNSyieoP2ane-QQ&scisig=AFWwaeYT1ecgB91lHBOvPYNcQzpS&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=5&folt=rel", "ref": ["David Lo - new related research"]}

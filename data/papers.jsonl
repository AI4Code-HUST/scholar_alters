{"title": "Exploring the Effects of Memory Vulnerabilities Across the Computer Architecture Stack", "first_label": ["Vulnerabilities"], "second_label": [], "data": "Y Tobah - 2025\nThe trend in computer architecture has long been to push toward developing more \npowerand area-efficient systems while neglecting potential security risks, spawning a \nplethora of microarchitectural and side-channel vulnerabilities. For example, \narchitectural features such as caching and speculative execution have brought \nindispensable performance boosts, but can leak micro-architectural information by \nfollowing execution paths dependent on \\xe2\\x80\\x9csecret\\xe2\\x80\\x9d input data. These unintended side\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaoo7: Low-overhead defense against spectre attacks via program\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://rtcl.eecs.umich.edu/rtclweb/assets/dissertations/2025/ytobah_thesis.pdf&hl=en&sa=X&d=3352724748047395013&ei=LnfeZ_HxOJm7ieoP6cWmgQE&scisig=AFWwaeZPlnJ5npcSe3exDAcMsZ4N&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "SPI: Similar Patch Identifier for Automated Program Repair", "first_label": ["Automated Program Repair"], "second_label": ["Repair"], "data": "\\xec\\x9e\\xa5\\xec\\x84\\xb8\\xec\\xb0\\xbd\\xef\\xbc\\x8c \\xea\\xb9\\x80\\xec\\x84\\xb1\\xeb\\xb9\\x88\\xef\\xbc\\x8c \\xec\\xb5\\x9c\\xec\\xa4\\x80\\xed\\x98\\x81\\xef\\xbc\\x8c \\xea\\xb9\\x80\\xec\\xa7\\x84\\xeb\\x8c\\x80\\xef\\xbc\\x8c \\xeb\\x82\\xa8\\xec\\x9e\\xac\\xec\\xb0\\xbd\\xc2\\xa0- Journal of KIISE, 2025\n\\xed\\x9c\\xb4\\xeb\\xa6\\xac\\xec\\x8a\\xa4\\xed\\x8b\\xb1 \\xea\\xb8\\xb0\\xeb\\xb0\\x98 \\xec\\x9e\\x90\\xeb\\x8f\\x99 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\xa8 \\xec\\x88\\x98\\xec\\xa0\\x95 (Automated Program Repair, APR) \\xea\\xb8\\xb0\\xec\\x88\\xa0\\xec\\x9d\\x98 \\xec\\xa3\\xbc\\xec\\x9a\\x94 \n\\xea\\xb4\\x80\\xec\\x8b\\xac\\xec\\x82\\xac\\xeb\\x8a\\x94 \\xed\\x83\\x90\\xec\\x83\\x89 \\xea\\xb3\\xb5\\xea\\xb0\\x84\\xec\\x9d\\x98 \\xed\\x81\\xac\\xea\\xb8\\xb0 \\xeb\\xac\\xb8\\xec\\xa0\\x9c\\xec\\x9d\\xb4\\xeb\\x8b\\xa4. \\xeb\\xb3\\xb8 \\xec\\x97\\xb0\\xea\\xb5\\xac\\xec\\x97\\x90\\xec\\x84\\x9c\\xeb\\x8a\\x94 \\xeb\\xb2\\x84\\xea\\xb7\\xb8\\xeb\\xa5\\xbc \\xec\\x83\\x9d\\xec\\x84\\xb1\\xed\\x95\\x9c \\xec\\x88\\x98\\xec\\xa0\\x95 (Bug \nIntroducing Change) \\xec\\x9d\\x98 \\xec\\x9c\\xa0\\xec\\x82\\xac\\xec\\x84\\xb1\\xec\\x9d\\x84 \\xed\\x99\\x9c\\xec\\x9a\\xa9\\xed\\x95\\x98\\xec\\x97\\xac \\xed\\x83\\x90\\xec\\x83\\x89 \\xea\\xb3\\xb5\\xea\\xb0\\x84\\xec\\x9d\\x84 \\xec\\xa4\\x84\\xec\\x9d\\xb4\\xea\\xb3\\xa0, \\xec\\xa0\\x81\\xec\\xa0\\x88\\xed\\x95\\x9c \\xec\\x88\\x98\\xec\\xa0\\x95 \\xec\\x97\\xb0\\xec\\x82\\xb0\\xec\\x9e\\x90\\xeb\\xa5\\xbc \n\\xec\\xa0\\x9c\\xec\\x95\\x88\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x83\\x88\\xeb\\xa1\\x9c\\xec\\x9a\\xb4 \\xec\\xa0\\x91\\xea\\xb7\\xbc \\xeb\\xb0\\xa9\\xec\\x8b\\x9d\\xec\\x9d\\xb8 Similar Patch Identifier (SPI) \\xeb\\xa5\\xbc \\xec\\xa0\\x9c\\xec\\x95\\x88\\xed\\x95\\x9c\\xeb\\x8b\\xa4. \\xec\\x9d\\xb4 \\xec\\xa0\\x91\\xea\\xb7\\xbc\\xeb\\xb2\\x95\\xec\\x9d\\x84 \n\\xed\\x8f\\x89\\xea\\xb0\\x80\\xed\\x95\\x98\\xea\\xb8\\xb0 \\xec\\x9c\\x84\\xed\\x95\\xb4, \\xea\\xb8\\xb0\\xec\\xa1\\xb4\\xec\\x9d\\x98 \\xeb\\xac\\xb8\\xeb\\xa7\\xa5 \\xea\\xb8\\xb0\\xeb\\xb0\\x98 APR \\xeb\\x8f\\x84\\xea\\xb5\\xac\\xec\\x9d\\xb8 ConFix \\xec\\x99\\x80 \\xec\\x9e\\x90\\xeb\\xb0\\x94 \\xea\\xb2\\xb0\\xed\\x95\\xa8 \\xeb\\xb2\\xa4\\xec\\xb9\\x98 \\xeb\\xa7\\x88\\xed\\x81\\xac\\xec\\x9d\\xb8 \nDefects4J \\xeb\\xa5\\xbc \\xed\\x99\\x9c\\xec\\x9a\\xa9\\xed\\x96\\x88\\xeb\\x8b\\xa4. \\xec\\x8b\\xa4\\xed\\x97\\x98\\xec\\x9d\\x84 \\xed\\x86\\xb5\\xed\\x95\\xb4 SPI \\xea\\xb0\\x80 \\xed\\x83\\x90\\xec\\x83\\x89 \\xea\\xb3\\xb5\\xea\\xb0\\x84\\xec\\x9d\\x84 \\xea\\xb0\\x81 \\xea\\xb2\\xb0\\xed\\x95\\xa8\\xec\\x97\\x90 \\xec\\xa0\\x81\\xed\\x95\\xa9\\xed\\x95\\x9c 10 \\xea\\xb0\\x9c\\xec\\x9d\\x98 \\xeb\\xb2\\x84\\xea\\xb7\\xb8\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Program Repair\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.dbpia.co.kr/Journal/articleDetail%3FnodeId%3DNODE12086914&hl=en&sa=X&d=9310000476149752472&ei=LnfeZ_HxOJm7ieoP6cWmgQE&scisig=AFWwaeZzLDFvXmonKz0xU87q5L4I&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants", "first_label": [], "second_label": [], "data": "A \\xc5\\xa0torek, M Gupta, N Bhatt, A Gupta, J Kim\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAI coding assistants are widely used for tasks like code generation, bug detection, \nand comprehension. These tools now require large and complex contexts, \nautomatically sourced from various origins $\\\\unicode {x2014} $ across files, projects\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14281&hl=vi&sa=X&d=4453262851610230891&ei=L3feZ8yQBJuoieoPy6GmoQM&scisig=AFWwaebqSIPeUVuDbMNr_CDJdkCX&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=0&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi", "Bach Le - new related research", "2 new citations to articles by Hong Jin Kang", "Thanh Le-Cong - new related research"]}
{"title": "OASIS: Order-Augmented Strategy for Improved Code Search", "first_label": ["Code"], "second_label": [], "data": "Z Gao, Z Zhan, X Li, E Yu, H Zhang, Y Zhang, J Li\\xc2\\xa0- arXiv preprint arXiv:2503.08161, 2025\nCode embeddings capture the semantic representations of code and are crucial for \nvarious code-related large language model (LLM) applications, such as code search. \nPrevious training primarily relies on optimizing the InfoNCE loss by comparing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08161&hl=vi&sa=X&d=15392589071867118103&ei=L3feZ8yQBJuoieoPy6GmoQM&scisig=AFWwaebVZs65Dm92Gs-2tyOziyOp&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=1&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "SOSecure: Safer Code Generation with RAG and StackOverflow Discussions", "first_label": ["Code"], "second_label": ["Generation"], "data": "M Mukherjee, VJ Hellendoorn\\xc2\\xa0- arXiv preprint arXiv:2503.13654, 2025\nLarge Language Models (LLMs) are widely used for automated code generation. \nTheir reliance on infrequently updated pretraining data leaves them unaware of \nnewly discovered vulnerabilities and evolving security standards, making them prone\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.13654&hl=vi&sa=X&d=9905663223559758922&ei=L3feZ8yQBJuoieoPy6GmoQM&scisig=AFWwaebLP9fuCdqh8SzaPWMjjcJz&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=2&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi", "Triet H. M. Le - new related research"]}
{"title": "LLM Test Generation via Iterative Hybrid Program Analysis", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "S Gu, N Nashid, A Mesbah\\xc2\\xa0- arXiv preprint arXiv:2503.13580, 2025\nAutomating unit test generation remains a significant challenge, particularly for \ncomplex methods in real-world projects. While Large Language Models (LLMs) have \nmade strides in code generation, they struggle to achieve high branch coverage due\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://export-test.arxiv.org/abs/2503.13580%3Fcontext%3Dcs.AI&hl=vi&sa=X&d=207857029493572923&ei=L3feZ8yQBJuoieoPy6GmoQM&scisig=AFWwaeYjkUqboOm8PzXTh6gTYp2o&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=3&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi", "Hong Jin Kang - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A Systematic Literature Review on Mining LTL Specifications", "first_label": [], "second_label": [], "data": "S Germiniani, D Nicoletti, G Pravadelli\\xc2\\xa0- IEEE Access, 2025\nLinear Temporal Logic (LTL) specifications play a crucial role in the verification \nprocess of cyber-physical systems, increasing the guarantees of their correctness. \nThese specifications are vital for ensuring that both hardware and software \ncomponents behave as expected, especially in complex real-world scenarios. In the \nlast decades, researchers have developed several methodologies and tools to \nautomatically generate LTL specifications, creating an urgent need to organize and\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSynergizing specification miners through model fissions and\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10926195.pdf&hl=en&sa=X&d=3745737933325408654&ei=L3feZ_6xAdmlieoPu-rnmA4&scisig=AFWwaeYavtPfdmVjr0NHi1U8y4ea&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Bach Le", "2 new citations to articles by Hong Jin Kang"]}
{"title": "Exploring representation-level augmentation and RAG-based vulnerability augmentation with LLMs for vulnerability detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "SS Daneshvar - 2025\nUsing deep learning (DL) for detecting software vulnerabilities has become \ncommonplace. However, data shortage remains a significant challenge due to the \nscarce nature of vulnerabilities. A few papers have attempted to address the data \nscarcity issue through oversampling, creating specific types of vulnerabilities, or \ngenerating code with single-statement vulnerabilities. In this thesis, we aim to find a \ngeneral-purpose methodology that covers various types of vulnerabilities and\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLinevul: A transformer-based line-level vulnerability prediction\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://mspace.lib.umanitoba.ca/bitstreams/b0baac42-5bff-46ee-8e9f-5575dc281905/download&hl=en&sa=X&d=9257436628680239801&ei=LnfeZ7DuNZuoieoPy6GmoQM&scisig=AFWwaeZ-bcyp22EkZzjW-eL12lK6&oi=scholaralrt&hist=apJ4fD8AAAAJ:4465730527138788254:AFWwaebhnVuF-27TSh32-dm_KGTR&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Michael Fu", "1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU", "1 new citation to articles by Richard Fang"]}
{"title": "SoK: AI-Powered Security Analysis of Smart Contract", "first_label": ["Smart Contracts"], "second_label": [], "data": "S Yang, J Niu, Y Zhang\nWith the soaring popularity of decentralized applications (DApps), smart contract \nsecurity has become increasingly important. Recently, numerous studies have \nleveraged artificial intelligence (AI) techniques to enhance efficiency and functional \ndiversity of smart contract security analysis. However, a comprehensive survey of \nthese studies to guide future development is still missing. To fill this gap, we present \nan innovative and systematic review. First, we establish filtering criteria and define\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaa{SmarTest}: Effectively hunting vulnerable transaction sequences\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://yinqian.org/papers/icmc24.pdf&hl=vi&sa=X&d=1594489748035319472&ei=L3feZ977CbutieoP9Om12Qc&scisig=AFWwaeYi9a9AKtdX5hOapUyjfNMq&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit", "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Lserve: Efficient long-sequence llm serving with unified sparse attention", "first_label": ["Large Language Models"], "second_label": [], "data": "S Yang, J Guo, H Tang, Q Hu, G Xiao, J Tang, Y Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown remarkable potential in processing long \nsequences, yet efficiently serving these long-context models remains challenging \ndue to the quadratic computational complexity of attention in the prefilling stage and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14866%3F&hl=en&sa=X&d=15664006594860475276&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeYz5XhtIZXaHxxrx6sNKSYk&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "C Cummins, V Seeker, D Grubisic, B Roziere\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 34th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across \na variety of software engineering and coding tasks. However, their application in the \ndomain of code and compiler optimization remains underexplored. Training LLMs is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712691&hl=en&sa=X&d=15248179687375146706&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeaQLmVGmhuYPirTSRdIm2C5&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research", "Michael Fu - new related research"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research", "Abhik Roychoudhury - new related research", "Hong Jin Kang - new related research"]}
{"title": "Backdoor Defense in Transportation Cyber-Physical Systems Using Frequency Domain Hybrid Distillation", "first_label": [], "second_label": [], "data": "B Hu, K Guo, Z Wu, X Wen, X Zhou\\xc2\\xa0- IEEE Transactions on Intelligent Transportation\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn the context of transportation cyber-physical systems (T-CPS), backdoor attacks \nleveraging traffic images have emerged as a significant security threat. As T-CPS \nincreasingly relies on visual information, such as real-time images captured by traffic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10897325/&hl=en&sa=X&d=10553411889537910232&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaebnL1xelHnGz0rcCJLkqGSd&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "IRIS: LLM-assisted static analysis for detecting security vulnerabilities", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "Z Li, S Dutta, M Naik\\xc2\\xa0- The Thirteenth International Conference on Learning\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSoftware is prone to security vulnerabilities. Program analysis tools to detect them \nhave limited effectiveness in practice due to their reliance on human labeled \nspecifications. Large language models (or LLMs) have shown impressive code\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.cis.upenn.edu/~mhnaik/papers/iclr25a.pdf&hl=en&sa=X&d=4911301143608099566&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeaKrD6cmuwVsXrzRQ7Os7qg&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research", "Hong Jin Kang - new related research", "Triet H. M. Le - new related research"]}
{"title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting", "first_label": [], "second_label": [], "data": "R Pu, C Li, R Ha, L Zhang, L Qiu, X Zhang\\xc2\\xa0- arXiv preprint arXiv:2503.12931, 2025\nDefending large language models (LLMs) against jailbreak attacks is crucial for \nensuring their safe deployment. Existing defense strategies generally rely on \npredefined static criteria to differentiate between harmful and benign prompts\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12931&hl=en&sa=X&d=10427174958506060579&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeaYZiecpY4MieXkcmzWoYSl&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger", "first_label": [], "second_label": [], "data": "M Zhu, Y Li, J Guo, T Wei, ST Xia, Z Qin\\xc2\\xa0- IEEE Transactions on Dependable and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCurrently, sample-specific backdoor attacks (SSBAs) are the most advanced and \nmalicious methods since they can easily circumvent most of the current backdoor \ndefenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due to their\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10930650/&hl=en&sa=X&d=4596311667492613613&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaebuUZy_q9wVjl1Ieoit-Y0Q&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning", "first_label": ["Large Language Models"], "second_label": [], "data": "T Xie, Z Gao, Q Ren, H Luo, Y Hong, B Dai, J Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInspired by the success of DeepSeek-R1, we explore the potential of rule-based \nreinforcement learning (RL) in large reasoning models. To analyze reasoning \ndynamics, we use synthetic logic puzzles as training data due to their controllable\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14768&hl=en&sa=X&d=11806421076571041592&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeZNJUBzVDx4LJM6wPl4G0un&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Wei, O Duchenne, J Copet, Q Carbonneaux, L Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe recent DeepSeek-R1 release has demonstrated the immense potential of \nreinforcement learning (RL) in enhancing the general reasoning capabilities of large \nlanguage models (LLMs). While DeepSeek-R1 and other follow-up work primarily\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18449&hl=en&sa=X&d=1711375529222221971&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaeavh1KD65WcmHgHtnD4-4_C&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Theoremexplainagent: Towards multimodal explanations for llm theorem understanding", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "M Ku, T Chong, J Leung, K Shah, A Yu, W Chen\\xc2\\xa0- arXiv preprint arXiv:2502.19400, 2025\nUnderstanding domain-specific theorems often requires more than just text-based \nreasoning; effective communication through structured visual explanations is crucial \nfor deeper comprehension. While large language models (LLMs) demonstrate strong\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19400%3F&hl=en&sa=X&d=15511409549046124762&ei=LnfeZ6CmN8mpieoPrOWNiQo&scisig=AFWwaebM2V7UIQ3C0vGSjDgE2gGP&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Moneta: Ex-Vivo GPU Driver Fuzzing by Recalling In-Vivo Execution States", "first_label": ["Fuzzing"], "second_label": [], "data": "J Jung, J Jang, Y Jo, J Vinck, A Voulimeneas\\xe2\\x80\\xa6\nGraphics Processing Units (GPUs) have become an indispensable part of modern \ncomputing infrastructure. They can execute massively parallel tasks on large data \nsets and have rich user space-accessible APIs for 3D rendering and generalpurpose\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-218-paper.pdf&hl=en&sa=X&d=4536160784874836119&ei=L3feZ_fhAsuZieoPiNWegAU&scisig=AFWwaeZSpFxKVjEvBs8doewhVJqc&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "first_label": [], "second_label": [], "data": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira\\xc2\\xa0- arXiv preprint arXiv:2502.16173, 2025\nTo compare autoregressive language models at scale, we propose using log-\nlikelihood vectors computed on a predefined text set as model features. This \napproach has a solid theoretical basis: when treated as model coordinates, their\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16173&hl=en&sa=X&d=1172765274563837678&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeZTv7upoMviITq65L9yzqfB&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Reinforced Diffuser for Red Teaming Large Vision-Language Models", "first_label": [], "second_label": [], "data": "R Wang, X Zheng, X Wang, C Wang, X Ma\\xc2\\xa0- arXiv preprint arXiv:2503.06223, 2025\nThe rapid advancement of large Vision-Language Models (VLMs) has raised \nsignificant safety concerns, particularly regarding their vulnerability to jailbreak \nattacks. While existing research primarily focuses on VLMs' susceptibility to harmful\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06223&hl=en&sa=X&d=10511149449596864280&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeYy58SFxfq_z39Cm9leB-EZ&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "first_label": [], "second_label": [], "data": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang\\xc2\\xa0- arXiv preprint arXiv:2503.07065, 2025\nWhile state-of-the-art vision-language models (VLMs) have demonstrated \nremarkable capabilities in complex visual-text tasks, their success heavily relies on \nmassive model scaling, limiting their practical deployment. Small-scale VLMs offer a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07065&hl=en&sa=X&d=2020924830875015883&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaebioJORon_ZKuPc7H2A6OVa&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "first_label": [], "second_label": [], "data": "A Narayan, D Biderman, S Eyuboglu, A May\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe investigate an emerging setup in which a small, on-device language model (LM) \nwith access to local data communicates with a frontier, cloud-hosted LM to solve real-\nworld tasks involving financial, medical, and scientific reasoning over long\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15964&hl=en&sa=X&d=9683035044090951544&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeaw3_fDH9NhBejdO8K5wsfH&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Process-based self-rewarding language models", "first_label": [], "second_label": [], "data": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models have demonstrated outstanding performance across \nvarious downstream tasks and have been widely applied in multiple scenarios. \nHuman-annotated preference data is used for training to further improve LLMs'\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.03746&hl=en&sa=X&d=9535134590722729967&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeYmBjvjsP_YEsr1ofmAH2_n&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision", "first_label": [], "second_label": [], "data": "D Zhu, X Wei, G Zhao, W Wu, H Zou, J Ran, X Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advances in Large Language Models (LLMs) have highlighted the challenge \nof handling long-context tasks, where models need to reason over extensive input \ncontexts to aggregate target information. While Chain-of-Thought (CoT) prompting\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20790&hl=en&sa=X&d=15555072442700286297&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaebsBDLwA9frPKCINS5aODkL&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?", "first_label": [], "second_label": [], "data": "Y Zhang, L Wang, M Fang, Y Du, C Huang, J Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDistilling large language models (LLMs) typically involves transferring the teacher \nmodel's responses through supervised fine-tuning (SFT). However, this approach \nneglects the potential to distill both data (output content) and reward signals (quality\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19557&hl=en&sa=X&d=12438440425156467205&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeZ-M5iqzVfklEqAxg19PnsO&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "In-Context Defense in Computer Agents: An Empirical Study", "first_label": [], "second_label": ["Agent"], "data": "P Yang, H Ci, MZ Shou\\xc2\\xa0- arXiv preprint arXiv:2503.09241, 2025\nComputer agents powered by vision-language models (VLMs) have significantly \nadvanced human-computer interaction, enabling users to perform complex tasks \nthrough natural language instructions. However, these agents are vulnerable to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.09241&hl=en&sa=X&d=4993462092907919996&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeaxFetVPMwkGumyBh5wz7Uj&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "first_label": [], "second_label": [], "data": "J Tae, H Ivison, S Kumar, A Cohan\\xc2\\xa0- arXiv preprint arXiv:2502.13917, 2025\nWe introduce TESS 2, a general instruction-following diffusion language model that \noutperforms contemporary instruction-tuned diffusion models, as well as matches \nand sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13917%3F&hl=en&sa=X&d=1855260706280837543&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeZ-HiKumWKVlqKpcJBDV9eE&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Graph-augmented reasoning: Evolving step-by-step knowledge graph retrieval for llm reasoning", "first_label": ["Large Language Models"], "second_label": [], "data": "W Wu, Y Jing, Y Wang, W Hu, D Tao\\xc2\\xa0- arXiv preprint arXiv:2503.01642, 2025\nRecent large language model (LLM) reasoning, despite its success, suffers from \nlimited domain knowledge, susceptibility to hallucinations, and constrained \nreasoning depth, particularly in small-scale models deployed in resource\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01642&hl=en&sa=X&d=14568014839624178945&ei=LnfeZ_GKNMuZieoPiNWegAU&scisig=AFWwaeZXCM0_kavqMkjD18ZX17c0&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Small sample smart contract vulnerability detection method based on multi-layer feature fusion", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection"], "data": "J Fan, Y He, H Wu\\xc2\\xa0- Complex & Intelligent Systems, 2025\nThe identification of vulnerabilities in smart contracts is necessary for ensuring their \nsecurity. As a pre-trained language model, BERT has been employed in the \ndetection of smart contract vulnerabilities, exhibiting high accuracy in tasks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s40747-025-01782-3&hl=en&sa=X&d=9070163655454212135&ei=LnfeZ5y1OoC96rQPjOH0iAk&scisig=AFWwaebHz4RVDViMhZrB1X8ahfTv&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research", "Hong Jin Kang - new related research"]}
{"title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models", "first_label": [], "second_label": ["Generation"], "data": "Q Zhang, C Fang, Y Zheng, Y Zhang, Y Zhao, R Huang\\xe2\\x80\\xa6\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUnit testing validates the correctness of the units of the software system under test \nand serves as the cornerstone in improving software quality and reliability. To reduce \nmanual efforts in writing unit tests, some techniques have been proposed to generate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3721128&hl=en&sa=X&d=7994025115950886329&ei=LnfeZ5y1OoC96rQPjOH0iAk&scisig=AFWwaebGza7bXsWUQ2XMouPDx_vl&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Augmenting Software Bills of Materials with Software Vulnerability Description: A Preliminary Study on GitHub", "first_label": ["Vulnerabilities"], "second_label": [], "data": "D Fucci, M Di Penta, S Romano, G Scannielllo\\xc2\\xa0- arXiv preprint arXiv:2503.13998, 2025\nSoftware Bills of Material (SBOMs) are becoming a consolidated, often enforced by \ngovernmental regulations, way to describe software composition. However, based on \nrecent studies, SBOMs suffer from limited support for their consumption and lack\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2503.13998&hl=en&sa=X&d=16046805564640624447&ei=LnfeZ5y1OoC96rQPjOH0iAk&scisig=AFWwaeZ8-zvVBWY16y9_baS-Rqex&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=3&folt=rel", "ref": ["Michael Fu - new related research", "Triet H. M. Le - new related research"]}
{"title": "Poisoned source code detection in code models", "first_label": ["Code"], "second_label": ["Detection"], "data": "E Ghannoum, M Ghafari\\xc2\\xa0- Journal of Systems and Software, 2025\nDeep learning models have gained popularity for conducting various tasks involving \nsource code. However, their black-box nature raises concerns about potential risks. \nOne such risk is a poisoning attack, where an attacker intentionally contaminates the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13459&hl=en&sa=X&d=6875396652207410319&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaeZ_hcNzoEiXth4lEVB3Rdg4&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "REFLECTA: Reflection-based Scalable and Semantic Scripting Language Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "C Zhang, G Lee, Q Liu, M Payer - 2025\nScripting languages such as Python and JavaScript have revolutionized modern \nsoftware development thanks to their flexibility and rich functionalities. However, \nscripting languages provide a large attack surface, allowing adversaries to exploit\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://cyruscyliu.github.io/papers/reflecta-asiaccs25.pdf&hl=en&sa=X&d=4212490224602162326&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaeYLEPbMXDGCgF6fiu0HmJmF&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Fixing Large Language Models' Specification Misunderstanding for Better Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Tian, J Chen, X Zhang\\xc2\\xa0- 2025 IEEE/ACM 47th International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation is to automatically generate source code conforming to a given \nprogramming specification, which has received extensive attention especially with \nthe development of large language models (LLMs). Due to the inherent difficulty of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://drive.google.com/file/d/1eiTmCxYs6FgdxCZNOdS9egz6U1Jf4wOL/view&hl=en&sa=X&d=6513233471596826292&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaeZjNzZZqbOuWO8MZmi5NgiS&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "A Survey On Large Language Models For Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "N Huynh, B Lin\\xc2\\xa0- arXiv preprint arXiv:2503.01245, 2025\nLarge Language Models (LLMs) have demonstrated their remarkable capabilities in \nnumerous fields. This survey focuses on how LLMs empower users, regardless of \ntheir technical background, to use human languages to automatically generate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01245&hl=en&sa=X&d=8140973833599212139&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaeZNqmOVJkz7-dPox9lTnTPB&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Cao, S Apel, A Singla, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.15835, 2025\nLarge Language Models (LLMs) have demonstrated impressive potential in \ntranslating natural language (NL) instructions into program code. However, user \ninstructions often contain inherent ambiguities, making it challenging for LLMs to\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15835&hl=en&sa=X&d=6927995321596801340&ei=LnfeZ5HhO8CSieoPqLGJyAM&scisig=AFWwaebpCZ-l4eJjTQ3XaxQi2c08&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "A privacy-preserving standardized model for large-scale source code fingerprint extraction and clone detection", "first_label": ["Code"], "second_label": ["Detection"], "data": "M Yang, Y Tan, N Shi, Y Wang, Z Wang, Q Liang\\xc2\\xa0- Computer Standards & Interfaces, 2025\nWith the rapid advancement of software technology, developers often replicate or \nmodify existing code to achieve code cloning, thereby improving development \nefficiency. However, the widespread use of open-source code may lead to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0920548925000273&hl=vi&sa=X&d=15928109942599938692&ei=L3feZ9mTDdSyieoPwLeW8AY&scisig=AFWwaeZ2xG1S9qgSohf6PC57s1Rm&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Improving Graph Learning-Based Fault Localization with Tailored Semi-supervised Learning", "first_label": ["Fault Localization"], "second_label": [], "data": "C LI, HUI LI, Z LI, M PAN, X LI - 2025\nAuthors' Contact Information: Chun Li, Nanjing University, State Key Laboratory for \nNovel Software Technology, Nanjing, China, chunli@ smail. nju. edu. cn; Hui Li, \nSamsung Electronics (China) R&D Centre, Nanjing, China, hui. li@ samsung. com;\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://pppppkun.github.io/files/fse25.pdf&hl=vi&sa=X&d=3684507761350432065&ei=L3feZ9mTDdSyieoPwLeW8AY&scisig=AFWwaeaCU1iGZOL3wdyrA6_vPx0l&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=2&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Simple Fault Localization using Execution Traces", "first_label": ["Fault Localization"], "second_label": [], "data": "JA Prenner, R Robbes\\xc2\\xa0- arXiv preprint arXiv:2503.04301, 2025\nTraditional spectrum-based fault localization (SBFL) exploits differences in a \nprogram's coverage spectrum when run on passing and failing test cases. However, \nsuch runs can provide a wealth of additional information beyond mere coverage\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.04301&hl=vi&sa=X&d=2093297660534139222&ei=L3feZ9mTDdSyieoPwLeW8AY&scisig=AFWwaebKjdYjJYByY7wZO158t1Ec&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=3&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "When Crypto Fails: Demystifying Cryptographic Defects in Ethereum Smart Contracts", "first_label": ["Smart Contracts"], "second_label": [], "data": "J Zhang, J Chen, Y Shen, T Zhang, Y Wang, T Chen\\xe2\\x80\\xa6\\xc2\\xa0- IEEE Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nEthereum has officially provided a set of system-level cryptographic APIs to enhance \nsmart contracts with cryptographic capabilities. These APIs have been utilized in over \n13.8% of Ethereum transactions, motivating developers to implement various on\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10929622/&hl=vi&sa=X&d=18134148251769182035&ei=L3feZ9mTDdSyieoPwLeW8AY&scisig=AFWwaeZDoJ6zBImr2Qv32z1OdgAK&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=4&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Optimizing generative AI by backpropagating language model feedback", "first_label": [], "second_label": [], "data": "M Yuksekgonul, F Bianchi, J Boen, S Liu, P Lu\\xe2\\x80\\xa6\\xc2\\xa0- Nature, 2025\nRecent breakthroughs in artificial intelligence (AI) are increasingly driven by systems \norchestrating multiple large language models (LLMs) and other specialized tools, \nsuch as search engines and simulators. So far, these systems are primarily \nhandcrafted by domain experts and tweaked through heuristics rather than being \nautomatically optimized, presenting a substantial challenge to accelerating progress. \nThe development of artificial neural networks faced a similar challenge until\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.nature.com/articles/s41586-025-08661-4&hl=en&sa=X&d=10536062179912825199&ei=L3feZ-_JDp-_6rQP17bc2QY&scisig=AFWwaeY7bchvvFaNSwhv9am0nHrh&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Carlos E. Jimenez"]}
{"title": "An I/O Characterizing Study of Offloading LLM Models and KV Caches to NVMe SSD", "first_label": ["Large Language Models"], "second_label": [], "data": "Z Ren, K Doekemeijer, T De Matteis, C Pinto, R Stoica\\xe2\\x80\\xa6 - 2025\nWith the popularity of generative AI, LLM inference has become one of the most \npopular cloud workloads. Modern popular LLMs have hundreds of billions of \nparameters and support very large input/output prompt token sizes (100K\\xe2\\x80\\x931M). As a \nresult, their computational state during LLM inference can exceed the memory \navailable on GPUs. One solution to this GPU memory problem is to offload the model \nweights and KV cache to the host memory. As the size of the models and prompts\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://atlarge-research.com/pdfs/2025-cheops-llm.pdf&hl=en&sa=X&d=5038529650303285448&ei=L3feZ-_JDp-_6rQP17bc2QY&scisig=AFWwaeaZWhrSEyp8Yb2HG9gZhwbH&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Carlos E. Jimenez"]}
{"title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "M Du, AT Luu, B Ji, X Wu, D Huang, TY Zhuo, Q Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have reshaped code generation by synergizing \ntheir exceptional comprehension of natural language and programming syntax, \nthereby substantially boosting developer productivity. These advancements have\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01295&hl=en&sa=X&d=9148487091916340178&ei=L3feZ9eTB8mpieoPrOWNiQo&scisig=AFWwaeYpS2I0prku3mc7CBfO4ED0&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, R Qian, P Lin, Y Liu, C Ying, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.18793, 2025\nLarge language models (LLMs) have transformed code generation. However, most \nexisting approaches focus on mainstream languages such as Python and Java, \nneglecting the Solidity language, the predominant programming language for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18793&hl=en&sa=X&d=18108558928985012184&ei=L3feZ9eTB8mpieoPrOWNiQo&scisig=AFWwaeYB8M09Myp3zooUoYE61pwf&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Guessing as a service: large language models are not yet ready for vulnerability detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "F Panebianco, A Isgro, S Longari, S Zanero\\xe2\\x80\\xa6\\xc2\\xa0- Guessing As A Service\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe growing number of reported software vulnerabilities underscores the need for \nefficient detection methods, especially for resource-limited organizations. While \ntraditional techniques like fuzzing and symbolic execution are effective, they require\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://re.public.polimi.it/bitstream/11311/1284205/1/_ITASEC__Survey_LLMs_for_Vulnerability_Detection.pdf&hl=en&sa=X&d=9488022294044051922&ei=L3feZ9eTB8mpieoPrOWNiQo&scisig=AFWwaeZfyMM1sIcP_FxrcS4WQQmp&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=3&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}

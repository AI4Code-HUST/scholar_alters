{"title": "GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer", "first_label": ["Code"], "second_label": [], "data": "Y Ye, P Pang, T Zhang, H Huang\\xc2\\xa0- arXiv preprint arXiv:2502.15202, 2025\nCode retrieval is a crucial component in modern software development, particularly \nin large-scale projects. However, existing approaches relying on sequence-based \nmodels often fail to fully exploit the structural dependencies inherent in code, leading\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15202&hl=en&sa=X&d=15251363462411189671&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaebhk-iDdcWh25bfnPBb2pGI&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research", "Hong Jin Kang - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Identifying software vulnerabilities via code representation learning", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection"], "data": "B Wu - 2025\nIn the era of big code, the proliferation of software applications has led to a \ncorresponding increase in unidentified vulnerabilities, necessitating early detection \nduring development to avoid potential disruptions and security risks post\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dr.ntu.edu.sg/bitstream/10356/182636/2/thesis_wbz.pdf&hl=en&sa=X&d=9518447506841292613&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaeZFN8VED7Vhr_v-27_UY-sX&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=2&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "J Lin, D Mohaisen\nLarge Language Models (LLMs) have demonstrated strong potential in tasks such as \ncode understanding and generation. This study evaluates several advanced LLMs\\xe2\\x80\\x94\nsuch as LLaMA-2, CodeLLaMA, LLaMA-3, Mistral, Mixtral, Gemma, CodeGemma\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1491-paper.pdf&hl=en&sa=X&d=4792361613594223308&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaeY2mHkuZqyAPpG8grIXpgws&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=3&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=4&folt=rel", "ref": ["Michael Fu - new related research", "Bach Le - new related research", "Richard Fang - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Bugfix: a standard language, database schema and repository for research on bugs and automatic program repair", "first_label": ["Automated Program Repair", "Bug"], "second_label": ["Repair"], "data": "V Kananchuk, I Mustafin, B Meyer\\xc2\\xa0- arXiv preprint arXiv:2502.15599, 2025\nAutomatic Program Repair (APR) is a brilliant idea: when detecting a bug, also \nprovide suggestions for correcting the program. Progress towards that goal is \nhindered by the absence of a common frame of reference for the multiplicity of APR\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15599&hl=en&sa=X&d=10642094292575353027&ei=mcjTZ9K5D5uoieoPksjy2Ag&scisig=AFWwaebYZkmLIso9Pt-1zoBNSzF7&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=5&folt=rel", "ref": ["Michael Fu - new related research", "Bach Le - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Knowledge-Enhanced Program Repair for Data Science Code", "first_label": ["Automated Program Repair", "Code"], "second_label": ["Repair"], "data": "S Ouyang, JM Zhang, Z Sun, AM Penuela\\xc2\\xa0- arXiv preprint arXiv:2502.09771, 2025\nThis paper introduces DSrepair, a knowledge-enhanced program repair method \ndesigned to repair the buggy code generated by LLMs in the data science domain. \nDSrepair uses knowledge graph based RAG for API knowledge retrieval as well as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09771&hl=en&sa=X&d=3654048262265967&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaea3b7K5alddz2-3Pw0svJ0R&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "ICSQuartz: Scan Cycle-Aware and Vendor-Agnostic Fuzzing for Industrial Control Systems", "first_label": ["Fuzzing"], "second_label": [], "data": "C Villa, C Doumanidis, H Lamri, PHN Rajput\\xe2\\x80\\xa6\nIndustrial Control Systems (ICS) ensure the automation and safe operation of critical \nindustry, energy, and commerce processes. Despite its importance, ICS code often \ncannot be evaluated as rigorously as software on traditional computing platforms, as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-795-paper.pdf&hl=en&sa=X&d=10524862650214833634&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeZZ_2ARI6V22CenXEygxr-a&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=1&folt=rel", "ref": ["Abhik Roychoudhury - new related research", "Hong Jin Kang - new related research"]}
{"title": "ROSA: Finding Backdoors with Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "D Kokkonis, M Marcozzi, E Decoux, S Zacchiroli\nA code-level backdoor is a hidden access, programmed and concealed within the \ncode of a program. For instance, hard-coded credentials planted in the code of a file \nserver application would enable maliciously logging into all deployed instances of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://binsec.github.io/assets/publications/papers/2025-icse.pdf&hl=en&sa=X&d=13531564943831013756&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeaGFFpUlOhEdz0gAvuIoA9h&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=2&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "XMutant: XAI-based Fuzzing for Deep Learning Systems", "first_label": ["Fuzzing"], "second_label": [], "data": "X Chen, M Biagiola, V Riccio, M d'Amorim, A Stocco\\xc2\\xa0- arXiv preprint arXiv:2503.07222, 2025\nSemantic-based test generators are widely used to produce failure-inducing inputs \nfor Deep Learning (DL) systems. They typically generate challenging test inputs by \napplying random perturbations to input semantic concepts until a failure is found or a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07222&hl=en&sa=X&d=13793734475605200419&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeYnfys4dbn96t74wYrCxCKf&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=3&folt=rel", "ref": ["Abhik Roychoudhury - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "C Cummins, V Seeker, D Grubisic, B Roziere\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 34th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across \na variety of software engineering and coding tasks. However, their application in the \ndomain of code and compiler optimization remains underexplored. Training LLMs is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712691&hl=en&sa=X&d=15248179687375146706&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeaQLmVGmhuYPirTSRdIm2C5&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=4&folt=rel", "ref": ["Abhik Roychoudhury - new related research", "Richard Fang - new related research"]}
{"title": "Snowplow: Effective Kernel Fuzzing with a Learned White-box Test Mutator", "first_label": ["Fuzzing"], "second_label": [], "data": "S Gong, R Wang, D Alt\\xc4\\xb1nb\\xc3\\xbcken, P Fonseca, P Maniatis - 2025\nKernel fuzzers rely heavily on program mutation to automatically generate new test \nprograms based on existing ones. In particular, program mutation can alter the test's \ncontrol and data flow inside the kernel by inserting new system calls, changing the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://sishuaigong.github.io/pdf/asplos25-snowplow.pdf&hl=en&sa=X&d=1525548876762051891&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeYhlLgOV_U4w2jfk052O3uX&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=5&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Ratte: Fuzzing for Miscompilations in Multi-Level Compilers Using Composable Semantics", "first_label": ["Fuzzing"], "second_label": [], "data": "P Yu, N Wu, AF Donaldson - 2025\nMulti-level intermediate representation (MLIR) is a rapidly growing compiler \nframework, with its defining feature being an ecosystem of modular language \nfragments called dialects. Specifying dialect semantics and validating dialect\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.doc.ic.ac.uk/~afd/papers/2025/ASPLOS-Ratte.pdf&hl=en&sa=X&d=2019974718744086328&ei=mcjTZ8ysFJGu6rQPgrKC0AY&scisig=AFWwaeaxX8iJzmc1p4D9ISFbKJuw&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=6&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Static analysis to make the most of CHERI C/C++ for existing code: improving memory safety at scale", "first_label": ["Code"], "second_label": [], "data": "I Dudina, I Stark\\xc2\\xa0- International Journal on Software Tools for Technology\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe describe and evaluate custom static analyses to support transitioning existing \nC/C++ codebases to CHERI hardware. CHERI is a novel architectural extension, \nimplemented for RISC-V and AArch64, that uses capabilities to provide fine-grained\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10009-025-00781-6&hl=vi&sa=X&d=14053332402067480520&ei=mcjTZ-3DH9mlieoPir2teA&scisig=AFWwaeZmsm_SQ2rC_EIGO_p9Mm7v&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Code Clone Detection Techniques Based on Large Language Models", "first_label": ["Large Language Models", "Code"], "second_label": ["Detection"], "data": "AA Almatrafi, FA Eassa, SA Sharaf\\xc2\\xa0- IEEE Access, 2025\nCode duplication, commonly known as code cloning, is a persistent challenge in \nsoftware development. While reusing code fragments boosts productivity, excessive \ncloning poses challenges to maintenance and elevates the risk of bugs. Therefore\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10918947.pdf&hl=vi&sa=X&d=10938180619195651115&ei=mcjTZ-3DH9mlieoPir2teA&scisig=AFWwaea5cu1UZa40TeDwr8iLOSGG&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=1&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi", "Hong Jin Kang - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection", "Repair", "Agent"], "data": "R Karanjai, S Blackshear, L Xu, W Shi\\xc2\\xa0- arXiv preprint arXiv:2502.18515, 2025\nThe rapid growth of the blockchain ecosystem and the increasing value locked in \nsmart contracts necessitate robust security measures. While languages like Solidity \nand Move aim to improve smart contract security, vulnerabilities persist. This paper\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18515&hl=vi&sa=X&d=1730857676253704795&ei=mcjTZ-3DH9mlieoPir2teA&scisig=AFWwaeYgvwUigV0hoLL-dsRz63X5&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=2&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "R+ R: Security Vulnerability Dataset Quality Is Critical", "first_label": ["Vulnerabilities"], "second_label": [], "data": "AS Yadav, JN Wilson\\xc2\\xa0- arXiv preprint arXiv:2503.06387, 2025\nLarge Language Models (LLMs) are of great interest in vulnerability detection and \nrepair. The effectiveness of these models hinges on the quality of the datasets used \nfor both training and evaluation. Our investigation reveals that a number of studies\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06387&hl=en&sa=X&d=7430364552601577064&ei=mcjTZ9KHHpyV6rQPpZCUsQI&scisig=AFWwaebW57wHB3rhH5wQB1sz504R&oi=scholaralrt&hist=apJ4fD8AAAAJ:15725322226479601129:AFWwaeYp-8wbw5OHTjoCHLP43E0V&html=&pos=0&folt=rel", "ref": ["Triet H. M. Le - new related research", "Hong Jin Kang - new related research", "1 new citation to articles by Michael Fu", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Unveiling Inefficiencies in LLM-Generated Code: Toward a Comprehensive Taxonomy", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "AA Abbassi, L Da Silva, A Nikanjam, F Khomh\\xc2\\xa0- arXiv preprint arXiv:2503.06327, 2025\nLarge Language Models (LLMs) are widely adopted for automated code generation \nwith promising results. Although prior research has assessed LLM-generated code \nand identified various quality issues--such as redundancy, poor maintainability, and\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nTriet H. M. Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06327&hl=en&sa=X&d=6733572883991899470&ei=mcjTZ9KHHpyV6rQPpZCUsQI&scisig=AFWwaeaeAfHK3r5zaQtIFej6UdVT&oi=scholaralrt&hist=apJ4fD8AAAAJ:15725322226479601129:AFWwaeYp-8wbw5OHTjoCHLP43E0V&html=&pos=1&folt=rel", "ref": ["Triet H. M. Le - new related research", "2 new citations to articles by Abhik Roychoudhury", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "A Giagnorio, A Mastropaolo, S Afrin, M Di Penta\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have shown an impressive capability in code \ngeneration and, specifically, to automatically implement requirements described in \nnatural language. The LLM effectiveness generally increases with its size: The\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07103&hl=en&sa=X&d=1611881913120984149&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaeYzl6k73zDgmveCBEmS-PKK&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Effectively Detecting Software Vulnerabilities via Leveraging Features on Program Slices", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "X Zhang, H Guo, Z Zhang, G Tang, J Sun, Y Shen, J Ma\\xc2\\xa0- IEEE Internet of Things\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDetecting software vulnerabilities has become increasingly challenging with the \ngrowing size and complexity of modern software. Traditional static and dynamic \nanalysis methods often suffer from poor accuracy and reliance on expert knowledge\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10884696/&hl=en&sa=X&d=12677073240774667567&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaea6FcI0bPWKCTkqRqPcGxwD&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Manual Prompt Engineering is Not Dead: A Case Study on Large Language Models for Code Vulnerability Detection with DSPy", "first_label": ["Vulnerabilities", "Large Language Models", "Code"], "second_label": ["Detection"], "data": "F Trad, A Chehab\\xc2\\xa0- 2025 8th International Conference on Data Science\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAutomated prompt engineering tools have recently emerged as a promising solution \nto simplify the traditional man-ual task of crafting prompts for large language models \n(LLMs). This study investigates whether such tools can fully replace manual prompt\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10908746/&hl=en&sa=X&d=5315185820890395558&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaeb1X7i5BNQkTn_MCK7RZ9Ck&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "LLM-enhanced evolutionary test generation for untyped languages", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "R Yang, X Xu, R Wang\\xc2\\xa0- Automated Software Engineering, 2025\nDynamic programming languages, such as Python, are widely used for their flexibility \nand support for rapid development. However, the absence of explicit parameter type \ndeclarations poses significant challenges in generating automated test cases. This\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00496-7&hl=en&sa=X&d=5209764566405071124&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaeZZcNNRu-1nh6WZ8bsL9w5L&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "An Empirical Study on the Relationship Between Defects and Source Code's Unnaturalness", "first_label": ["Code"], "second_label": [], "data": "Y Jiang, H Liu, J Liu, Y Zhang, W Ji, H Zhong, L Zhang\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nNatural languages are \\xe2\\x80\\x9cnatural\\xe2\\x80\\x9d in that texts in natural languages are repetitive and \npredictable. Recent research indicates that programming languages share similar \ncharacteristics (naturalness), with source code displaying patterns of repetition and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3718083&hl=en&sa=X&d=7306116860520413653&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaeYhQXymsslwMKE9PXdq4Ud6&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Design pattern recognition: a study of large language models", "first_label": ["Large Language Models"], "second_label": [], "data": "SK Pandey, S Chand, J Horkoff, M Staron, M Ochodek\\xe2\\x80\\xa6\\xc2\\xa0- Empirical Software\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAbstract Context As Software Engineering (SE) practices evolve due to extensive \nincreases in software size and complexity, the importance of tools to analyze and \nunderstand source code grows significantly. Objective This study aims to evaluate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10625-1&hl=en&sa=X&d=8546412004218051293&ei=mcjTZ5yCEYC96rQP2s_YiAM&scisig=AFWwaealng_j1iX3a2iIiURF0bTp&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Reeq: Testing and Mitigating Ethically Inconsistent Suggestions of Large Language Models with Reflective Equilibrium", "first_label": ["Large Language Models"], "second_label": [], "data": "P Ma, Z Wang, Z Li, Z Ji, A Sun, J Rahmel, S Wang\\xc2\\xa0- ACM Transactions on Software\\xc2\\xa0\\xe2\\x80\\xa6\nLLMs increasingly serve as general-purpose AI assistants in daily life, and their \nsubtly unethical suggestions become a serious and real concern. It is demanding to \ntest and mitigate such unethical suggestions from LLMs. Despite existing efforts to \ndetect violations of \\xe2\\x80\\x9ctestable\\xe2\\x80\\x9d facets of ethics (eg, fairness testing), it is challenging to \nencode the full scope of ethics (eg, justice, deontology) into a test oracle without \nhuman annotations or intervention. In this paper, we take inspiration from reflective\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBiasfinder: Metamorphic test generation to uncover bias for\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3722554&hl=en&sa=X&d=14585413160540204549&ei=mcjTZ8qaF7utieoP8ref4Qo&scisig=AFWwaeaMuhz3cDU1fj9ILRjx0z_v&oi=scholaralrt&hist=apJ4fD8AAAAJ:11486195984023826531:AFWwaebYo-fw1j0PJswL-CdomZqY&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Hong Jin Kang"]}
{"title": "High-accuracy, privacy-compliant multilingual sentiment categorization on consumer-grade hardware: A monte carlo evaluation of locally deployed large language\\xc2\\xa0\\xe2\\x80\\xa6", "first_label": [], "second_label": [], "data": "M Carlo, O Takeuchi\\xc2\\xa0- Digital Applied Linguistics, 2025\nThis study presents a comprehensive evaluation of multilingual sentiment \ncategorization performance using locally deployed large language models (LLMs) \non consumer-grade hardware, focusing on GDPR-compliant implementation \nscenarios. Through extensive Monte Carlo validation involving 947,700 \nclassifications over 702 iterations, we demonstrate significant performance \ncapabilities across English, Italian, and Japanese languages while operating within\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBiasfinder: Metamorphic test generation to uncover bias for\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.castledown.com/journals/dal/article/download/dal.v3.102585/894&hl=en&sa=X&d=15354660043741349817&ei=mcjTZ8qaF7utieoP8ref4Qo&scisig=AFWwaeZr3lPZjeWlq73Vlnxcppay&oi=scholaralrt&hist=apJ4fD8AAAAJ:11486195984023826531:AFWwaebYo-fw1j0PJswL-CdomZqY&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Hong Jin Kang"]}
{"title": "Will AI replace Software Engineers? Hold your Breath", "first_label": [], "second_label": [], "data": "A Roychoudhury, A Zeller\\xc2\\xa0- arXiv preprint arXiv:2502.20429, 2025\nArtificial Intelligence (AI) technology such as Large Language Models (LLMs) have \nbecome extremely popular in creating code. This has led to the conjecture that future \nsoftware jobs will be exclusively conducted by LLMs, and the software industry will\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20429&hl=en&sa=X&d=1904718572146504993&ei=mcjTZ528IpuoieoPksjy2Ag&scisig=AFWwaebNK4i6_E4Z7RUb5bPOXY1t&oi=scholaralrt&hist=apJ4fD8AAAAJ:16488056128958629805:AFWwaeZVy5biUXZBZUZeh3-Oz0_I&html=&pos=0&folt=rel", "ref": ["Bach Le - new related research"]}
{"title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning", "first_label": ["Code"], "second_label": ["Agent"], "data": "J Yang, W Zhang, J Yang, Y Miao, S Quan, Z Wu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advancement in code understanding and generation demonstrates that code \nLLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities \nto address wide-ranging code-related tasks. However, most previous existing\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.07487&hl=en&sa=X&d=8565771609752539455&ei=mcjTZ528IpuoieoPksjy2Ag&scisig=AFWwaebjTbGZreWIIbwyARTALp3U&oi=scholaralrt&hist=apJ4fD8AAAAJ:16488056128958629805:AFWwaeZVy5biUXZBZUZeh3-Oz0_I&html=&pos=3&folt=rel", "ref": ["Bach Le - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "N Christakis, D Drikakis\\xc2\\xa0- arXiv preprint arXiv:2503.05852, 2025\nThis study introduces a new methodology for an Inference Index (InI), called \nINFerence INdex In Testing model Effectiveness methodology (INFINITE), aiming to \nevaluate the performance of Large Language Models (LLMs) in code generation \ntasks. The InI index provides a comprehensive assessment focusing on three key \ncomponents: efficiency, consistency, and accuracy. This approach encapsulates time-\nbased efficiency, response quality, and the stability of model outputs, offering a\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaThanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.05852&hl=en&sa=X&d=14140978932483800844&ei=mcjTZ-PpEsmpieoP4aW8iQ8&scisig=AFWwaeZnEh81GGEYenY0Yh4Q6IVq&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Bach Le", "1 new citation to articles by Thanh Le-Cong"]}
{"title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol", "first_label": [], "second_label": [], "data": "R Koohestani, P de Bekker, M Izadi\\xc2\\xa0- arXiv preprint arXiv:2503.05860, 2025\nBenchmarks are essential for consistent evaluation and reproducibility. The \nintegration of Artificial Intelligence into Software Engineering (AI4SE) has given rise \nto numerous benchmarks for tasks such as code generation and bug fixing. \nHowever, this surge presents challenges:(1) scattered benchmark knowledge across \ntasks,(2) difficulty in selecting relevant benchmarks,(3) the absence of a uniform \nstandard for benchmark development, and (4) limitations of existing benchmarks. In\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaCodeultrafeedback: An llm-as-a-judge dataset for aligning large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.05860&hl=vi&sa=X&d=16801918549662477580&ei=mcjTZ7WDG5-_6rQPzI2_uQ0&scisig=AFWwaeYL3gFB9cG4vAmw1QLXc4ba&oi=scholaralrt&hist=apJ4fD8AAAAJ:11724652424841979500:AFWwaeb06hHZ-3j7Bb1sOMTsP9ed&html=&pos=0&folt=cit", "ref": ["3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU", "4 new citations to articles by Carlos E. Jimenez"]}
{"title": "Explainable Android Malware Detection and Malicious Code Localization Using Graph Attention", "first_label": ["Code"], "second_label": ["Detection"], "data": "MC Ipek, S Sen\\xc2\\xa0- arXiv preprint arXiv:2503.07109, 2025\nWith the escalating threat of malware, particularly on mobile devices, the demand for \neffective analysis methods has never been higher. While existing security solutions, \nincluding AI-based approaches, offer promise, their lack of transparency constraints \nthe understanding of detected threats. Manual analysis remains time-consuming and \nreliant on scarce expertise. To address these challenges, we propose a novel \napproach called XAIDroid that leverages graph neural networks (GNNs) and graph\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaDexBERT: Effective, Task-Agnostic and Fine-grained\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07109&hl=vi&sa=X&d=6805359935762496609&ei=mcjTZ7WDG5-_6rQPzI2_uQ0&scisig=AFWwaeZc7tae2QbaUJA3BSMn6nqQ&oi=scholaralrt&hist=apJ4fD8AAAAJ:11724652424841979500:AFWwaeb06hHZ-3j7Bb1sOMTsP9ed&html=&pos=2&folt=cit", "ref": ["3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Identifying the most efficient vulnerability detection methods: a multi-criteria decision-making approach", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "P Gupta, D Aggrawal, A Anand, J Singh\\xc2\\xa0- Operational Perspective of Modeling\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn today's software-driven era, extensive research has been conducted to analyze \nthe growth patterns of software reliability. Numerous factors were found to be \nimpacting its qualitative indices, out of which security concerns regarding a software \nsystem always hold importance in the eyes of its customers. Accurately assessing \nand evaluating these security concerns, referred to as \\xe2\\x80\\x9cvulnerabilities,\\xe2\\x80\\x9d is crucial for a \ncomprehensive understanding of the reliability of software. Researchers have\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaFitness guided vulnerability detection with greybox fuzzing\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Djl1NEQAAQBAJ%26oi%3Dfnd%26pg%3DPA295%26ots%3DDsPVQGPJ34%26sig%3DrfLmaRaoMsUkN2jNOoZXdTdtFK0&hl=en&sa=X&d=3974042219840003002&ei=mcjTZ_yEDsuZieoP9PnpkQc&scisig=AFWwaebs6KUECKZDoXs2hhQnDZlr&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "\\xea\\xb3\\xb5\\xea\\xb8\\x89\\xeb\\xa7\\x9d \\xeb\\xb3\\xb4\\xec\\x95\\x88\\xec\\x9d\\x84 \\xec\\x9c\\x84\\xed\\x95\\x9c \\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4 \\xeb\\xaa\\x85\\xec\\x84\\xb8\\xec\\x84\\x9c (SBOM) \\xea\\xb0\\x9c\\xec\\x84\\xa0 \\xec\\x97\\xb0\\xea\\xb5\\xac", "first_label": [], "second_label": [], "data": "\\xec\\xb5\\x9c\\xec\\x98\\x81\\xec\\x9e\\xac\\xef\\xbc\\x8c \\xec\\x96\\x91\\xed\\x9d\\xac\\xeb\\x8f\\x99\\xef\\xbc\\x8c \\xec\\x9a\\xb0\\xec\\x8a\\xb9\\xed\\x9b\\x88\\xc2\\xa0- \\xec\\xa0\\x95\\xeb\\xb3\\xb4\\xeb\\xb3\\xb4\\xed\\x98\\xb8\\xed\\x95\\x99\\xed\\x9a\\x8c\\xec\\xa7\\x80, 2025\n\\xec\\x95\\x88\\xec\\xa0\\x84\\xed\\x95\\x9c \\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4 \\xea\\xb3\\xb5\\xea\\xb8\\x89\\xeb\\xa7\\x9d\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xea\\xb8\\xb0 \\xec\\x9c\\x84\\xed\\x95\\xb4, \\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4 \\xec\\xa0\\x9c\\xed\\x92\\x88\\xec\\x97\\x90 \\xed\\x8f\\xac\\xed\\x95\\xa8\\xeb\\x90\\x9c \\xea\\xb5\\xac\\xec\\x84\\xb1\\xec\\x9a\\x94\\xec\\x86\\x8c, \n\\xeb\\x9d\\xbc\\xec\\x9d\\xb4\\xeb\\xb8\\x8c\\xeb\\x9f\\xac\\xeb\\xa6\\xac, \\xeb\\xaa\\xa8\\xeb\\x93\\x88 \\xeb\\xb0\\x8f \\xea\\xb7\\xb8 \\xeb\\xb2\\x84\\xec\\xa0\\x84 \\xec\\xa0\\x95\\xeb\\xb3\\xb4\\xeb\\xa5\\xbc \\xec\\xb2\\xb4\\xea\\xb3\\x84\\xec\\xa0\\x81\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xeb\\xaa\\x85\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4 \\xeb\\xaa\\x85\\xec\\x84\\xb8\\xec\\x84\\x9c (Software \nBill of Materials; SBOM) \\xec\\x9d\\x98 \\xed\\x99\\x9c\\xec\\x9a\\xa9\\xec\\x9d\\xb4 \\xec\\xa3\\xbc\\xeb\\xaa\\xa9\\xeb\\xb0\\x9b\\xea\\xb3\\xa0 \\xec\\x9e\\x88\\xeb\\x8b\\xa4. SBOM \\xec\\x9d\\x98 \\xed\\x88\\xac\\xeb\\xaa\\x85\\xec\\x84\\xb1 \\xeb\\xb0\\x8f \\xea\\xb3\\xb5\\xea\\xb8\\x89\\xeb\\xa7\\x9d \n\\xeb\\xb3\\xb4\\xec\\x95\\x88\\xec\\x97\\x90\\xec\\x84\\x9c\\xec\\x9d\\x98 \\xed\\x9a\\xa8\\xec\\x9c\\xa8\\xec\\x84\\xb1\\xec\\x97\\x90\\xeb\\x8f\\x84 \\xeb\\xb6\\x88\\xea\\xb5\\xac\\xed\\x95\\x98\\xea\\xb3\\xa0, \\xec\\x97\\xac\\xec\\xa0\\x84\\xed\\x9e\\x88 \\xec\\x97\\xac\\xeb\\x9f\\xac \\xed\\x95\\x9c\\xea\\xb3\\x84\\xec\\xa0\\x90\\xea\\xb3\\xbc \\xea\\xb0\\x9c\\xec\\x84\\xa0\\xec\\x9d\\x98 \\xec\\x97\\xac\\xec\\xa7\\x80\\xea\\xb0\\x80 \\xec\\xa1\\xb4\\xec\\x9e\\xac\\xed\\x95\\x9c\\xeb\\x8b\\xa4. \\xeb\\xb3\\xb8 \n\\xea\\xb3\\xa0\\xec\\x97\\x90\\xec\\x84\\x9c\\xeb\\x8a\\x94 \\xed\\x98\\x84\\xec\\x9e\\xac SBOM \\xec\\x9d\\x98 \\xed\\x95\\x9c\\xea\\xb3\\x84\\xec\\xa0\\x90\\xec\\x9d\\x84 \\xeb\\xb6\\x84\\xec\\x84\\x9d\\xed\\x95\\x98\\xea\\xb3\\xa0, \\xec\\x8b\\xa4\\xec\\xa7\\x88\\xec\\xa0\\x81 \\xed\\x95\\xb4\\xea\\xb2\\xb0 \\xeb\\xb0\\xa9\\xec\\x95\\x88\\xea\\xb3\\xbc \\xec\\x97\\xb0\\xea\\xb5\\xac \\xeb\\x8f\\x99\\xed\\x96\\xa5\\xec\\x9d\\x84 \\xec\\xa0\\x9c\\xec\\x8b\\x9c\\xed\\x95\\x9c\\xeb\\x8b\\xa4.\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaa{V1SCAN}: Discovering 1-day Vulnerabilities in Reused {C/C++\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.dbpia.co.kr/Journal/articleDetail%3FnodeId%3DNODE12088070&hl=vi&sa=X&d=2134902882929036244&ei=mcjTZ469HNSyieoPy4i_oQo&scisig=AFWwaeZ0GiBwCDJXOvc26EIv4Kb6&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit", "ref": ["2 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Universal Scalability in Declarative Program Analysis (with Choice-Based Combination Pruning)", "first_label": [], "second_label": [], "data": "A Antoniadis, I Tsatiris, N Grech, Y Smaragdakis\\xc2\\xa0- arXiv preprint arXiv:2503.05945, 2025\nIn this work, we present a simple, uniform, and elegant solution to the problem, with \nstunning practical effectiveness and application to virtually any Datalog-based \nanalysis. The approach consists of leveraging the choice construct, supported \nnatively in modern Datalog engines like Souffl\\\\'e. The choice construct allows the \ndefinition of functional dependencies in a relation and has been used in the past for \nexpressing worklist algorithms. We show a near-universal construction that allows\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaReturn of CFA: call-site sensitivity can be superior to object\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.05945&hl=vi&sa=X&d=17688206857132704588&ei=mcjTZ469HNSyieoPy4i_oQo&scisig=AFWwaeYYEcQaexanoQkh7p8v7o6-&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=1&folt=cit", "ref": ["2 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts", "first_label": [], "second_label": [], "data": "T Lin, J Xie, S Yuan, D Yang\\xc2\\xa0- arXiv preprint arXiv:2503.07604, 2025\nTest-time compute is emerging as a new paradigm for enhancing language models' \ncomplex multi-step reasoning capabilities, as demonstrated by the success of \nOpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in \ntest-time compute, implicit reasoning is more inference-efficient, requiring fewer \ngenerated tokens. However, why does the advanced reasoning capability fail to \nemerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07604&hl=en&sa=X&d=2003940781293571982&ei=mcjTZ4z9IMuZieoP9PnpkQc&scisig=AFWwaeYYLlzZtl3b9W6vjAjE0lQH&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit", "ref": ["4 new citations to articles by Carlos E. Jimenez"]}
{"title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing", "first_label": [], "second_label": [], "data": "Y Xie, A Xie, D Sheth, P Liu, D Fried, C Rose\\xc2\\xa0- arXiv preprint arXiv:2503.07358, 2025\nWe present RepoST, a scalable method to construct environments that provide \nexecution feedback for repository-level code generation for both training and \nevaluation. Unlike existing works that aim to build entire repositories for execution, \nwhich is challenging for both human and LLMs, we provide execution feedback with \nsandbox testing, which isolates a given target function and its dependencies to a \nseparate script for testing. Sandbox testing reduces the complexity of external\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07358&hl=en&sa=X&d=6882274016387550647&ei=mcjTZ4z9IMuZieoP9PnpkQc&scisig=AFWwaeYrsn2XEmd-388alv7J5Ajv&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=2&folt=cit", "ref": ["4 new citations to articles by Carlos E. Jimenez"]}
{"title": "ML-DEV-BENCH: COMPARATIVE ANALYSIS OF AI", "first_label": [], "second_label": [], "data": "AON ML\nIn this report, we present ML-Dev-Bench, a benchmark aimed at testing agentic \ncapabilities on applied Machine Learning development tasks. While existing \nbenchmarks focus on isolated coding tasks or Kaggle-style competitions, ML-Dev-\nBench tests agents' ability to handle the full complexity of ML development \nworkflows. The benchmark assesses performance across critical aspects including \ndataset handling, model training, improving existing models, debugging, and API\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DUlwyv3mrQ2&hl=en&sa=X&d=4792585805052094982&ei=mcjTZ4z9IMuZieoP9PnpkQc&scisig=AFWwaebdkNyGWh_Cg_wptmIfguiq&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=3&folt=cit", "ref": ["4 new citations to articles by Carlos E. Jimenez"]}
{"title": "Unveiling Privacy Risks in LLM Agent Memory", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "B Wang, W He, P He, S Zeng, Z Xiang, Y Xing, J Tang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Model (LLM) agents have become increasingly prevalent across \nvarious real-world applications. They enhance decision-making by storing private \nuser-agent interactions in the memory module for demonstrations, introducing new\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13172&hl=en&sa=X&d=1205784360742732283&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaebxlZzBMwyVDWpxNWr57ygF&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation", "first_label": [], "second_label": [], "data": "W Zhang, H Xu, Z Wang, Z He, Z Zhu, K Ren\\xc2\\xa0- arXiv preprint arXiv:2503.06519, 2025\nSmall language models (SLMs) have emerged as promising alternatives to large \nlanguage models (LLMs) due to their low computational demands, enhanced privacy \nguarantees and comparable performance in specific domains through light-weight\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06519&hl=en&sa=X&d=2672498427852424334&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeZmKanyZmfNpRYUmhF27wWz&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research", "Carlos E. Jimenez - new related research"]}
{"title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "X Liu, S Liang, M Han, Y Luo, A Liu, X Cai, Z He, D Tao\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nGenerative large language models are crucial in natural language processing, but \nthey are vulnerable to backdoor attacks, where subtle triggers compromise their \nbehavior. Although backdoor attacks against LLMs are constantly emerging, existing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18511&hl=en&sa=X&d=2326898847250844616&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeYsxLaPZ35g7ZwCcXLowDSz&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM Red Teaming", "first_label": ["Large Language Models"], "second_label": [], "data": "S Schoepf, MZ Hameed, A Rawat, K Fraser, G Zizzo\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith LLM usage rapidly increasing, their vulnerability to jailbreaks that create harmful \noutputs are a major security risk. As new jailbreaking strategies emerge and models \nare changed by fine-tuning, continuous testing for security vulnerabilities is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06253&hl=en&sa=X&d=3588023232993252325&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeYHbS5o0ji4-iobFrCpIVw8&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Pang, B Yang, H Tu, Y Cao, Z Zhang\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa02025-2025 IEEE International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAlthough Large Language Models (LLMs) excel in reasoning and generation for \nlanguage tasks, they are not specifically designed for multimodal challenges. \nTraining Multimodal Large Language Models (MLLMs), however, is resource\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11751&hl=en&sa=X&d=11065919914523651369&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeafJ9hHibn9FVw6fdjz4t5T&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models", "first_label": [], "second_label": [], "data": "X Liu, X Jia, Y Xun, H Zhang, X Cao\\xc2\\xa0- arXiv preprint arXiv:2502.16167, 2025\nDiffusion models (DMs) have revolutionized data generation, particularly in text-to-\nimage (T2I) synthesis. However, the widespread use of personalized generative \nmodels raises significant concerns regarding privacy violations and copyright\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2502.16167&hl=en&sa=X&d=4329161943110226749&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeafrJsvZfvDQ-TQGz6g7XVq&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection", "first_label": [], "second_label": ["Detection", "Agent"], "data": "W Luo, S Dai, X Liu, S Banerjee, H Sun, M Chen\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid advancements in Large Language Models (LLMs) have enabled their \ndeployment as autonomous agents for handling complex tasks in dynamic \nenvironments. These LLMs demonstrate strong problem-solving capabilities and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11448&hl=en&sa=X&d=2582607623046763543&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaeaMaFOos2AUAeOF2ngl9gyP&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM Pretraining with Continuous Concepts", "first_label": ["Large Language Models"], "second_label": [], "data": "J Tack, J Lanchantin, J Yu, A Cohen, I Kulikov, J Lan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nNext token prediction has been the standard training objective used in large \nlanguage model pretraining. Representations are learned as a result of optimizing \nfor token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.08524&hl=en&sa=X&d=12504769844288214763&ei=mcjTZ7_HDNmlieoPir2teA&scisig=AFWwaebpYuCvNC11VxPVRWmt6yly&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models", "first_label": [], "second_label": [], "data": "J Huang, J Qin, J Zhang, Y Yuan, W Wang, J Zhao\\xc2\\xa0- arXiv preprint arXiv:2503.07575, 2025\nThis research investigates both explicit and implicit social biases exhibited by Vision-\nLanguage Models (VLMs). The key distinction between these bias types lies in the \nlevel of awareness: explicit bias refers to conscious, intentional biases, while implicit\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07575&hl=en&sa=X&d=4674178038254960174&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeZypCF53HcST5a-C2B3oJ_r&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap", "first_label": [], "second_label": [], "data": "Q Liu, F Wang, C Xiao, M Chen\\xc2\\xa0- arXiv preprint arXiv:2502.10486, 2025\nThe emergence of vision language models (VLMs) comes with increased safety \nconcerns, as the incorporation of multiple modalities heightens vulnerability to \nattacks. Although VLMs can be built upon LLMs that have textual safety alignment, it\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.10486&hl=en&sa=X&d=11854639627530484779&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaea98Rw6lmSGwcIUp-ijLR2F&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning", "first_label": [], "second_label": [], "data": "H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang\\xc2\\xa0- arXiv preprint arXiv:2503.07065, 2025\nWhile state-of-the-art vision-language models (VLMs) have demonstrated \nremarkable capabilities in complex visual-text tasks, their success heavily relies on \nmassive model scaling, limiting their practical deployment. Small-scale VLMs offer a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07065&hl=en&sa=X&d=2020924830875015883&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaebioJORon_ZKuPc7H2A6OVa&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities", "first_label": [], "second_label": [], "data": "F Jiang, Z Xu, Y Li, L Niu, Z Xiang, B Li, BY Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nEmerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage \nlong chain-of-thought (CoT) reasoning to generate structured intermediate steps, \nenhancing their reasoning capabilities. However, long CoT does not inherently\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12025%3F&hl=en&sa=X&d=11794055058332912936&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeZIkep8RWr0oJ-gto7S9nZK&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models", "first_label": [], "second_label": [], "data": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu\\xc2\\xa0- arXiv preprint arXiv:2502.14272, 2025\nAligning small language models (SLMs) with human values typically involves \ndistilling preference knowledge from large language models (LLMs). However, \nexisting distillation methods model preference knowledge in teacher LLMs by\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14272&hl=en&sa=X&d=3283888565285721117&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeacCk5hYRYI-fAwtLN-1pFN&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training", "first_label": [], "second_label": [], "data": "F Weng, J Lou, J Feng, M Huang, W Wang\\xc2\\xa0- arXiv preprint arXiv:2502.11455, 2025\nSafety alignment is critical in pre-training large language models (LLMs) to generate \nresponses aligned with human values and refuse harmful queries. Unlike LLM, the \ncurrent safety alignment of VLMs is often achieved with post-hoc safety fine-tuning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11455&hl=en&sa=X&d=14199852315052579694&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeaQdgo38ob0HLeDPQrlflMZ&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment", "first_label": ["Large Language Models"], "second_label": [], "data": "J Deng, Z Jiang, L Pang, L Chen, K Xu, Z Wei, H Shen\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nA new trend uses LLMs as dense text encoders via contrastive learning. However, \nsince LLM embeddings predict the probability distribution of the next token, they are \ninherently generative and distributive, conflicting with contrastive learning, which\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11401&hl=en&sa=X&d=5106071575990121675&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeZbBDaqVX9ZcIUYqZxcu6rk&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training", "first_label": [], "second_label": [], "data": "J Wang, M Wang, Z Zhou, J Yan, L Wu\\xc2\\xa0- arXiv preprint arXiv:2502.19002, 2025\nTransformers consist of diverse building blocks, such as embedding layers, \nnormalization layers, self-attention mechanisms, and point-wise feedforward \nnetworks. Thus, understanding the differences and interactions among these blocks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19002&hl=en&sa=X&d=2677356598046008203&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeYGlogmLr9txJCzfFUo4830&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling", "first_label": [], "second_label": [], "data": "W Zhao, T Pan, X Han, Y Zhang, A Sun, Y Huang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSpeculative sampling has emerged as an important technique for accelerating the \nauto-regressive generation process of large language models (LLMs) by utilizing a \ndraft-then-verify mechanism to produce multiple tokens per forward pass. While state\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14856&hl=en&sa=X&d=4250945624494244927&ei=mcjTZ5W1CdSyieoPy4i_oQo&scisig=AFWwaeYRHIy0HLbCZ0oP5bnAdlsn&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on Code Naturalness", "first_label": ["Code"], "second_label": [], "data": "W Sun, Y Chen, M Yuan, C Fang, Z Chen, C Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nNeural code models (NCMs) have demonstrated extraordinary capabilities in code \nintelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems has \ngarnered increasing attention. In particular, NCMs are often trained on large-scale\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15830&hl=en&sa=X&d=8780101222380067393&ei=mcjTZ8HwGOehieoPw9e4wQE&scisig=AFWwaebHDymjqAhpyWTraugmtN9X&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Less is More: On the Importance of Data Quality for Unit Test Generation", "first_label": ["Unit Test"], "second_label": ["Generation"], "data": "J Zhang, X Hu, S Gao, X Xia, D Lo, S Li\\xc2\\xa0- arXiv preprint arXiv:2502.14212, 2025\nUnit testing is crucial for software development and maintenance. Effective unit \ntesting ensures and improves software quality, but writing unit tests is time-\nconsuming and labor-intensive. Recent studies have proposed deep learning (DL)\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14212&hl=en&sa=X&d=16543327815838598951&ei=mcjTZ8HwGOehieoPw9e4wQE&scisig=AFWwaebqw4pVoJz5WcqCzMNxWA6K&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Cao, S Apel, A Singla, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.15835, 2025\nLarge Language Models (LLMs) have demonstrated impressive potential in \ntranslating natural language (NL) instructions into program code. However, user \ninstructions often contain inherent ambiguities, making it challenging for LLMs to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15835&hl=en&sa=X&d=6927995321596801340&ei=mcjTZ8HwGOehieoPw9e4wQE&scisig=AFWwaebpCZ-l4eJjTQ3XaxQi2c08&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=5&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Q Zhao, L Zhang, F Liu, X Lian, Q Meng, Z Jiao, Z Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation is a latency-sensitive task that demands high timeliness, but the \nautoregressive decoding mechanism of Large Language Models (LLMs) leads to \npoor inference efficiency. Existing LLM inference acceleration methods mainly focus\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.17139&hl=en&sa=X&d=12309021167084800557&ei=mcjTZ8HwGOehieoPw9e4wQE&scisig=AFWwaebM1YnN4NurSD2BsoIiHS26&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=6&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Cloud Software Code Generation via Knowledge Graphs and Multi-Modal Learning", "first_label": ["Code"], "second_label": ["Generation"], "data": "F Zhang, H Chen, Q Chen, J Liu - 2025\nAs cloud computing continues to experience rapid growth, the demand for cloud-\nnative applications is escalating, leading to more complex and diverse \ndevelopmentrequirements. In this context, automated code generation plays a pivotal\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchsquare.com/article/rs-6153598/latest&hl=vi&sa=X&d=3129591140480420110&ei=mcjTZ7rjFZm7ieoP1NrBiQU&scisig=AFWwaeZgbWySvOHSlVeAaK5uQ3mY&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=5&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging", "first_label": ["Code", "Bug"], "second_label": ["Generation"], "data": "J Wu, H Li, X Zhang, J Luo, Y Huang, R Chu, Y Yang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nPreference learning enhances Code LLMs beyond supervised fine-tuning by \nleveraging relative quality comparisons. Existing methods construct preference pairs \nfrom candidates based on test case success, treating the higher pass rate sample as\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.02783&hl=vi&sa=X&d=6763756223659251534&ei=mcjTZ7rjFZm7ieoP1NrBiQU&scisig=AFWwaeafb3JFnlavrHAmECLB0wC6&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=6&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}

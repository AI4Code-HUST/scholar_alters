{"title": "Bogus Bugs, Duplicates, and Revealing Comments: Data Quality Issues in NPR", "first_label": ["Bug"], "second_label": [], "data": "JA Prenner, R Robbes\\xc2\\xa0- arXiv preprint arXiv:2503.08532, 2025\nThe performance of a machine learning system is not only determined by the model \nbut also, to a substantial degree, by the data it is trained on. With the increasing use \nof machine learning, issues related to data quality have become a concern also in \nautomated program repair research. In this position paper, we report some of the \ndata-related issues we have come across when working with several large APR \ndatasets and benchmarks, including, for instance, duplicates or\" bogus bugs\". We\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaProgram Repair Competition 2024\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08532&hl=en&sa=X&d=12067643889807296478&ei=chHYZ9fUNIWlieoP3KvqcQ&scisig=AFWwaeZF-iE2RsvcnaFeoEzPqhkJ&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["4 new citations to articles by Abhik Roychoudhury"]}
{"title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Maintenance", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "X Peng, C Wang, M Liu, Y Lou, Y Wu\\xc2\\xa0- arXiv preprint arXiv:2503.07967, 2025\nWhile large language models (LLMs) have demonstrated promise in software \nengineering tasks like code completion and generation, their support for the \nmaintenance of complex software systems remains limited. These models often \nstruggle with understanding the tacit knowledge embedded in systems, such as \nresponsibility allocation and collaboration across different modules. To address this \ngap, we introduce the concept and framework of\\\\textbf {Code Digital Twin}, a\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutoCodeRover: Autonomous program improvement\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07967&hl=en&sa=X&d=16378833991906536071&ei=chHYZ9fUNIWlieoP3KvqcQ&scisig=AFWwaeZyXBnJeyLHzQ-xOWwR44ZM&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["4 new citations to articles by Abhik Roychoudhury", "Hong Jin Kang - new related research", "10 new citations to articles by Carlos E. Jimenez", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Towards Diverse Program Transformations for Program Simplification", "first_label": [], "second_label": [], "data": "H Wang, Z Xing, C Sun, Z Wang, SH Tan\\xc2\\xa0- Proceedings of the ACM on Software\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAuthors' Contact Information: Haibo Wang, Concordia University, Montreal, Canada, \nhaibo. wang@ mail. concordia. ca; Zezhong Xing, Southern University of Science \nand Technology, Shenzhen, China, 12232384@ mail. sustech. edu. cn; Chengnian \nSun, University of Waterloo, Waterloo, Canada, cnsun@ uwaterloo. ca; Zheng Wang, \nUniversity of Leeds, Leeds, United Kingdom, z. wang5@ leeds. ac. uk; Shin Hwei \nTan, Concordia University, Montreal, Canada, shinhwei. tan@ concordia. ca.\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Repair of Programs from Large Language Models\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://eprints.whiterose.ac.uk/224243/1/fse25.pdf&hl=en&sa=X&d=10478209242717899004&ei=chHYZ9fUNIWlieoP3KvqcQ&scisig=AFWwaea002BDgKUoeWHmmazd6so-&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=2&folt=cit", "ref": ["4 new citations to articles by Abhik Roychoudhury"]}
{"title": "and AST-Based Template-Driven Automated", "first_label": [], "second_label": [], "data": "T Guan, YYQ Su, R Qiu\\xc2\\xa0- Advances in Guidance, Navigation and Control\\xc2\\xa0\\xe2\\x80\\xa6\nThis paper explores optimizing static code defect detection across multiple standards \n(eg, GJB8114, CWE, MISRA2012) and AST-based template repair strategies to \nimprove software quality and efficiency. It introduces a uni-versal mechanism for \ndetecting code defects and an AST-based manual template method for precise \nautomated repair. Through experiments on real-world code and benchmarks, the \nproposed methods show superior accuracy and repair preci-sion over popular neural\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSemFix: Program Repair via Semantic Analysis\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DabVNEQAAQBAJ%26oi%3Dfnd%26pg%3DPA169%26ots%3D_pJG1QcP2Z%26sig%3DuzG2MmH8DOkh8Znk2xZrowantzE&hl=en&sa=X&d=3119394781514157885&ei=chHYZ9fUNIWlieoP3KvqcQ&scisig=AFWwaeaVnhW01AQrjvZgXfgIgZKk&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=3&folt=cit", "ref": ["4 new citations to articles by Abhik Roychoudhury"]}
{"title": "Investigating Execution-Aware Language Models for Code Optimization", "first_label": ["Code"], "second_label": [], "data": "F Di Menna, L Traini, G Bavota, V Cortellessa\\xc2\\xa0- arXiv preprint arXiv:2503.08228, 2025\nCode optimization is the process of enhancing code efficiency, while preserving its \nintended functionality. This process often requires a deep understanding of the code \nexecution behavior at run-time to identify and address inefficiencies effectively\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08228&hl=en&sa=X&d=11464541261744981902&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeZOrw28MJs1HKINGUJCROdQ&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research", "Bach Le - new related research", "Triet H. M. Le - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "Poisoned source code detection in code models", "first_label": ["Code"], "second_label": ["Detection"], "data": "E Ghannoum, M Ghafari\\xc2\\xa0- Journal of Systems and Software, 2025\nDeep learning models have gained popularity for conducting various tasks involving \nsource code. However, their black-box nature raises concerns about potential risks. \nOne such risk is a poisoning attack, where an attacker intentionally contaminates the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13459&hl=en&sa=X&d=6875396652207410319&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeZ_hcNzoEiXth4lEVB3Rdg4&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "REFLECTA: Reflection-based Scalable and Semantic Scripting Language Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "C Zhang, G Lee, Q Liu, M Payer - 2025\nScripting languages such as Python and JavaScript have revolutionized modern \nsoftware development thanks to their flexibility and rich functionalities. However, \nscripting languages provide a large attack surface, allowing adversaries to exploit\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://nebelwelt.net/publications/files/25AsiaCCS.pdf&hl=en&sa=X&d=4212490224602162326&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeYwLqCscLO_LxY8j6LmOli-&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, R Qian, P Lin, Y Liu, C Ying, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.18793, 2025\nLarge language models (LLMs) have transformed code generation. However, most \nexisting approaches focus on mainstream languages such as Python and Java, \nneglecting the Solidity language, the predominant programming language for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18793&hl=en&sa=X&d=18108558928985012184&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeYB8M09Myp3zooUoYE61pwf&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Cao, S Apel, A Singla, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.15835, 2025\nLarge Language Models (LLMs) have demonstrated impressive potential in \ntranslating natural language (NL) instructions into program code. However, user \ninstructions often contain inherent ambiguities, making it challenging for LLMs to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15835&hl=en&sa=X&d=6927995321596801340&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaebpCZ-l4eJjTQ3XaxQi2c08&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=7&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "MTVHunter: Smart Contracts Vulnerability Detection Based on Multi-Teacher Knowledge Translation", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection", "Generation"], "data": "G Sun, Y Zhuang, S Zhang, X Feng, Z Liu, L Zhang\\xc2\\xa0- arXiv preprint arXiv:2502.16955, 2025\nSmart contracts, closely intertwined with cryptocurrency transactions, have sparked \nwidespread concerns about considerable financial losses of security issues. To \ncounteract this, a variety of tools have been developed to identify vulnerability in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16955&hl=en&sa=X&d=12290673098951683274&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeY1AF73jrMTFiILJHNbvTE2&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Fully Autonomous Programming using Iterative Multi-Agent Debugging with Large Language Models", "first_label": ["Large Language Models", "Bug"], "second_label": ["Agent"], "data": "A Grishina, V Liventsev, A H\\xc3\\xa4rm\\xc3\\xa4, L Moonen\\xc2\\xa0- ACM Transactions on Evolutionary\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nProgram synthesis with Large Language Models (LLMs) suffers from a \\xe2\\x80\\x9cnear-miss \nsyndrome\\xe2\\x80\\x9d: the generated code closely resembles a correct solution but fails unit \ntests due to minor errors. We address this with a multi-agent framework called\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3719351&hl=en&sa=X&d=14886956962007206250&ei=chHYZ97wN6OD6rQPt-Ce6A4&scisig=AFWwaeY7OYiGi7vaZtjP_oFCIOEe&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention", "first_label": ["Large Language Models"], "second_label": [], "data": "S Yang, J Guo, H Tang, Q Hu, G Xiao, J Tang, Y Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown remarkable potential in processing long \nsequences, yet efficiently serving these long-context models remains challenging \ndue to the quadratic computational complexity of attention in the prefilling stage and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14866%3F&hl=en&sa=X&d=15664006594860475276&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeYz5XhtIZXaHxxrx6sNKSYk&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Imperceptible Backdoor Attacks on Text-Guided 3D Scene Grounding", "first_label": [], "second_label": [], "data": "D Liu, W Hu\\xc2\\xa0- IEEE Transactions on Multimedia, 2025\nWith the maturity of depth sensors, the vulnerability of 3D point cloud models has \nreceived increasing attention in various applications such as autonomous driving \nand robot navigation. Previous 3D adversarial attackers mainly focus on attacking\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10891607/&hl=en&sa=X&d=15631879208678090546&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeb2vVVPMT4s6XsyqzH_994B&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Mitigating backdoor attacks in Federated Learning based intrusion detection systems through Neuron Synaptic Weight Adjustment", "first_label": [], "second_label": ["Detection"], "data": "U Zukaib, X Cui\\xc2\\xa0- Knowledge-Based Systems, 2025\nFederated Learning has emerged as a transformative paradigm that enables \ncollaborative model training across distributed clients while preserving data privacy. \nHowever, Federated Learning systems are vulnerable to backdoor attacks, where\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S095070512500214X&hl=en&sa=X&d=3344237733639979278&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeZiRQp24mPLFQoWAjl8T3ii&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence", "first_label": [], "second_label": ["Agent"], "data": "Z Li, S Huang, J Wang, N Zhang, A Antoniades, W Hua\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs language agents progressively automate critical tasks across domains, their \nability to operate within operational constraints and safety protocols becomes \nessential. While extensive research has demonstrated these agents' effectiveness in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08669&hl=en&sa=X&d=17080806982111925858&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeaS4ulH9oRQCL95tBfPDlv5&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research", "10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Covert and Potent: A Weather-Camouflaged Backdoor Attacks on Self-Supervised Learning", "first_label": [], "second_label": [], "data": "Y Wei, Y Yang, B Liu, B Xiao\\xc2\\xa0- ICASSP 2025-2025 IEEE International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSelf-supervised learning is widely applied across various domains due to its \nadvantage of learning data representations without the need for labels. However, \nrecent research shows that backdoor attacks on self-supervised learning are\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/10887540/10887541/10888213.pdf&hl=en&sa=X&d=9082644966487276858&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeYdnNCSJNQ5KzQ6j2mbiQOj&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Improving LLM Safety Alignment with Dual-Objective Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "X Zhao, W Cai, T Shi, D Huang, L Lin, S Mei, D Song\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nExisting training-time safety alignment techniques for large language models (LLMs) \nremain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely \ndeployed alignment method, exhibits limitations in both experimental and theoretical\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.03710%3F&hl=en&sa=X&d=8827875917875040781&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeYUuSC1Sn9TE8AM0vm1rztU&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "ASIDE: Architectural Separation of Instructions and Data in Language Models", "first_label": [], "second_label": [], "data": "E Zverev, E Kortukov, A Panfilov, S Tabesh\\xe2\\x80\\xa6\\xc2\\xa0- ICLR 2025 Workshop on Building\\xc2\\xa0\\xe2\\x80\\xa6\nDespite their remarkable performance, large language models lack elementary \nsafety features, and this makes them susceptible to numerous malicious attacks. In \nparticular, previous work has identified the absence of an intrinsic separation\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DGlmqRQsCaI&hl=en&sa=X&d=6953317997255117272&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeal4CsBNajdA3Xjm6Ep4Sii&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Safety Guardrails for LLM-Enabled Robots", "first_label": ["Large Language Models"], "second_label": [], "data": "Z Ravichandran, A Robey, V Kumar, GJ Pappas\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAlthough the integration of large language models (LLMs) into robotics has unlocked \ntransformative capabilities, it has also introduced significant safety concerns, ranging \nfrom average-case LLM errors (eg, hallucinations) to adversarial jailbreaking attacks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07885&hl=en&sa=X&d=7690067763279260274&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeatxC4bHbIblFnE1eANwhUB&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Backdoor Defense in Transportation Cyber-Physical Systems Using Frequency Domain Hybrid Distillation", "first_label": [], "second_label": [], "data": "B Hu, K Guo, Z Wu, X Wen, X Zhou\\xc2\\xa0- IEEE Transactions on Intelligent Transportation\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn the context of transportation cyber-physical systems (T-CPS), backdoor attacks \nleveraging traffic images have emerged as a significant security threat. As T-CPS \nincreasingly relies on visual information, such as real-time images captured by traffic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10897325/&hl=en&sa=X&d=10553411889537910232&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaebnL1xelHnGz0rcCJLkqGSd&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "BDPFL: Backdoor Defense for Personalized Federated Learning via Explainable Distillation", "first_label": [], "second_label": [], "data": "C Zhu, J Zhang, D Wu, G Long\\xc2\\xa0- arXiv preprint arXiv:2503.06554, 2025\nFederated learning is a distributed learning paradigm that facilitates the collaborative \ntraining of a global model across multiple clients while preserving the privacy of local \ndatasets. To address inherent challenges related to data heterogeneity and satisfy\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06554&hl=en&sa=X&d=879279576510460296&ei=chHYZ4T9Mo-j6rQPlMHcuAk&scisig=AFWwaeYyDfyhAXUzF-ydXVr2T575&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models", "first_label": [], "second_label": ["Generation"], "data": "Q Zhang, C Fang, Y Zheng, Y Zhang, Y Zhao, R Huang\\xe2\\x80\\xa6\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUnit testing validates the correctness of the units of the software system under test \nand serves as the cornerstone in improving software quality and reliability. To reduce \nmanual efforts in writing unit tests, some techniques have been proposed to generate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3721128&hl=en&sa=X&d=7994025115950886329&ei=chHYZ7uMNsCSieoPoa_pwQc&scisig=AFWwaebGza7bXsWUQ2XMouPDx_vl&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Evaluating LLaMA 3.2 for Software Vulnerability Detection", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "J Gon\\xc3\\xa7alves, M Silva, B Cabral, T Dias, E Maia, I Pra\\xc3\\xa7a\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDeep Learning (DL) has emerged as a powerful tool for vulnerability detection, often \noutperforming traditional solutions. However, developing effective DL models \nrequires large amounts of real-world data, which can be difficult to obtain in sufficient\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07770&hl=en&sa=X&d=5655905225238348384&ei=chHYZ7uMNsCSieoPoa_pwQc&scisig=AFWwaeazeQ3hEaA2CJvyUGPkSzZu&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research", "Triet H. M. Le - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "C Cummins, V Seeker, D Grubisic, B Roziere\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 34th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across \na variety of software engineering and coding tasks. However, their application in the \ndomain of code and compiler optimization remains underexplored. Training LLMs is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712691&hl=en&sa=X&d=15248179687375146706&ei=chHYZ7uMNsCSieoPoa_pwQc&scisig=AFWwaeaQLmVGmhuYPirTSRdIm2C5&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=2&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "VulKiller: Java Web Vulnerability Detection with Code Property Graph and Large Language Models", "first_label": ["Vulnerabilities", "Large Language Models", "Code"], "second_label": ["Detection"], "data": "X Chen, B Wang, M Zhang, Y Cao, Q Liu\\xc2\\xa0- ICASSP 2025-2025 IEEE International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn recent years, web application development has become more efficient, yet \nvulnerabilities still pose significant risks. Traditional static and dynamic detection \ntechniques are prone to false positives and negatives, making it challenging for small\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10890652/&hl=en&sa=X&d=16132470342424149214&ei=chHYZ7uMNsCSieoPoa_pwQc&scisig=AFWwaebtiqQOyVJt5SyK9mTsITpC&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=3&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "first_label": [], "second_label": [], "data": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira\\xc2\\xa0- arXiv preprint arXiv:2502.16173, 2025\nTo compare autoregressive language models at scale, we propose using log-\nlikelihood vectors computed on a predefined text set as model features. This \napproach has a solid theoretical basis: when treated as model coordinates, their\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16173&hl=en&sa=X&d=1172765274563837678&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZTv7upoMviITq65L9yzqfB&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "first_label": [], "second_label": [], "data": "A Narayan, D Biderman, S Eyuboglu, A May\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe investigate an emerging setup in which a small, on-device language model (LM) \nwith access to local data communicates with a frontier, cloud-hosted LM to solve real-\nworld tasks involving financial, medical, and scientific reasoning over long\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15964&hl=en&sa=X&d=9683035044090951544&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeaw3_fDH9NhBejdO8K5wsfH&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding", "first_label": [], "second_label": [], "data": "KH Huang, C Qin, H Qiu, P Laban, S Joty, C Xiong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nVision Language Models (VLMs) have achieved remarkable progress in multimodal \ntasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like \nobject counting or length comparison, which are essential for relevant complex tasks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11492&hl=en&sa=X&d=10820422815461711349&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZl5d9VroV8qK3-PwXRx0PP&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Self-Steering Language Models", "first_label": [], "second_label": [], "data": "G Grand, JB Tenenbaum, V Mansinghka, AK Lew\\xe2\\x80\\xa6\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0: VerifAI: AI Verification in the Wild\nFor many reasoning tasks, augmenting language models with test-time compute can \nsignificantly boost performance. However, scaling inference is costly for complex \nproblems that require extensive search or sampling. Nevertheless, even when LMs\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dx7E2Qt7n0V&hl=en&sa=X&d=17836731747212682693&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeYnQPOEYSmZZ5fMjuMV3S05&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "first_label": [], "second_label": [], "data": "J Nielsen, P Schneider-Kamp, L Galke\\xc2\\xa0- arXiv preprint arXiv:2502.11895, 2025\nLarge language models (LLMs) require immense resources for training and \ninference. Quantization, a technique that reduces the precision of model parameters, \noffers a promising solution for improving LLM efficiency and sustainability. While post\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11895&hl=en&sa=X&d=5315475009734524900&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeac1W1T5CLpz9ocqASaiX6z&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Seeing Delta Parameters as JPEG Images: Data-Free Delta Compression with Discrete Cosine Transform", "first_label": [], "second_label": [], "data": "C Huang, P Ye, X Wang, S Zheng, B Qi, L Bai\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith transformer-based models and the pretrain-finetune paradigm becoming \nmainstream, the high storage and deployment costs of individual finetuned models \non multiple tasks pose critical challenges. Delta compression attempts to lower the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06676&hl=en&sa=X&d=995345491034050371&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZfZBS0V6XZUNj8Ge8yWa9K&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "first_label": [], "second_label": [], "data": "J Tae, H Ivison, S Kumar, A Cohan\\xc2\\xa0- arXiv preprint arXiv:2502.13917, 2025\nWe introduce TESS 2, a general instruction-following diffusion language model that \noutperforms contemporary instruction-tuned diffusion models, as well as matches \nand sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13917%3F&hl=en&sa=X&d=1855260706280837543&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZ-HiKumWKVlqKpcJBDV9eE&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning", "first_label": ["Large Language Models"], "second_label": [], "data": "W Wu, Y Jing, Y Wang, W Hu, D Tao\\xc2\\xa0- arXiv preprint arXiv:2503.01642, 2025\nRecent large language model (LLM) reasoning, despite its success, suffers from \nlimited domain knowledge, susceptibility to hallucinations, and constrained \nreasoning depth, particularly in small-scale models deployed in resource\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01642&hl=en&sa=X&d=14568014839624178945&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZXCM0_kavqMkjD18ZX17c0&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision", "first_label": [], "second_label": [], "data": "D Zhu, X Wei, G Zhao, W Wu, H Zou, J Ran, X Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advances in Large Language Models (LLMs) have highlighted the challenge \nof handling long-context tasks, where models need to reason over extensive input \ncontexts to aggregate target information. While Chain-of-Thought (CoT) prompting\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20790&hl=en&sa=X&d=15555072442700286297&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaebsBDLwA9frPKCINS5aODkL&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models", "first_label": [], "second_label": [], "data": "S Yi, T Cong, X He, Q Li, J Song\\xc2\\xa0- arXiv preprint arXiv:2502.19883, 2025\nSmall language models (SLMs) have become increasingly prominent in the \ndeployment on edge devices due to their high efficiency and low computational cost. \nWhile researchers continue to advance the capabilities of SLMs through innovative\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19883&hl=en&sa=X&d=6808888617871597297&ei=chHYZ9DxMLutieoP8ref4Qo&scisig=AFWwaeZ02d25aB4BRMPfjFn7klzv&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Dynamic information utilization for securing Ethereum smart contracts: A literature review", "first_label": ["Smart Contracts"], "second_label": [], "data": "T Hu, B Li\\xc2\\xa0- Information and Software Technology, 2025\nSmart contracts, self-executing programs that govern digital assets on blockchain \nplatforms, have gained widespread adoption due to their automation and \ntransparency. However, vulnerabilities in smart contracts can lead to financial losses \nand reputational damage, making their security a critical concern. Static code \nauditing methods are prone to false positives and false negatives, as they fail to \naccount for real-time execution conditions. The integration of dynamic information\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaa{SmarTest}: Effectively hunting vulnerable transaction sequences\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584925000588&hl=vi&sa=X&d=9536013557238741880&ei=cxHYZ9LtA9mlieoPir2teA&scisig=AFWwaea_2L9eNRkAAyH8WcNh-eYL&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit", "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "LLM-enhanced evolutionary test generation for untyped languages", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "R Yang, X Xu, R Wang\\xc2\\xa0- Automated Software Engineering, 2025\nDynamic programming languages, such as Python, are widely used for their flexibility \nand support for rapid development. However, the absence of explicit parameter type \ndeclarations poses significant challenges in generating automated test cases. This\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00496-7&hl=en&sa=X&d=5209764566405071124&ei=cxHYZ8aGAtSyieoPy4i_oQo&scisig=AFWwaeZZcNNRu-1nh6WZ8bsL9w5L&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Code2JSON: Can a Zero-Shot LLM Agent Extract Code Features for Code RAG?", "first_label": ["Large Language Models", "Code"], "second_label": ["Agent"], "data": "A Singhal, R Ghosh, R Mundra, H Dadlani, D Dutta\\xc2\\xa0- ICLR 2025 Third Workshop on Deep\\xc2\\xa0\\xe2\\x80\\xa6\nA retrieval-augmented generation (RAG) framework that accepts natural language \n(NL) queries and returns contextual responses based on source code is crucial for \nenhancing developer productivity. However, building a code RAG system is\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nTriet H. M. Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DglBWrVLvKi&hl=en&sa=X&d=11144165502460504562&ei=cxHYZ-KuBZGu6rQPgrKC0AY&scisig=AFWwaeY8HJiVRFGiPbA1RbV7KmJo&oi=scholaralrt&hist=apJ4fD8AAAAJ:15725322226479601129:AFWwaeYp-8wbw5OHTjoCHLP43E0V&html=&pos=2&folt=rel", "ref": ["Triet H. M. Le - new related research"]}
{"title": "BEARCUBS: A benchmark for computer-using web agents", "first_label": [], "second_label": ["Agent"], "data": "Y Song, K Thai, CM Pham, Y Chang, M Nadaf, M Iyyer\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nModern web agents possess computer use abilities that allow them to interact with \nwebpages by sending commands to a virtual keyboard and mouse. While such \nagents have considerable potential to assist human users with complex tasks, \nevaluating their capabilities in real-world settings poses a major challenge. To this \nend, we introduce BEARCUBS, a\" small but mighty\" benchmark of 111 information-\nseeking questions designed to evaluate a web agent's ability to search, browse, and\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07919&hl=en&sa=X&d=4390211763772413235&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeamIalQyRk8RjPl9IfW_elW&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=2&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Evaluating and Mitigating the Safety Awareness-Execution Gaps of LM Agents", "first_label": [], "second_label": ["Agent"], "data": "Y Tang, T Li, E Li, CJ Maddison, H Dong, Y Ruan\\xc2\\xa0- ICLR 2025 Workshop on Building Trust in\\xc2\\xa0\\xe2\\x80\\xa6\nLanguage model (LM) agents have demonstrated significant potential for automating \nreal-world tasks, yet they pose a diverse array of potential, severe risks in safety-\ncritical scenarios. In this work, we identify a significant gap between LM agents' risk \nawareness and safety execution abilities: while they often answer\" Yes''to queries \nlike $\\\\texttt {\" Is executingsudo rm-rf/*'dangerous?\"} $, they will likely fail to identify \nsuch risks in instantiated trajectories or even directly perform these risky actions\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DlTYxZGbLwA&hl=en&sa=X&d=12665022109662357488&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeYeYalWgwRFZAXpgCk4Kn7H&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=3&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Harmful Helper: Perform malicious tasks? Web AI agents might help", "first_label": [], "second_label": ["Agent"], "data": "JYF Chiang, S Lee, JB Huang, F Huang, Y Chen\\xc2\\xa0- ICLR 2025 Workshop on Building Trust in\\xc2\\xa0\\xe2\\x80\\xa6\nRecent research has significantly advanced Web AI agents, introducing \ngroundbreaking architectures and benchmarks demonstrating major progress in \nautonomous web interaction and navigation. However, recent studies have shown \nthat many AI agents can execute malicious tasks and are more vulnerable than \nstandalone LLMs. Our work studies why Web AI agents, built on safety-aligned \nbackbone Large Language Models (LLMs), remain highly susceptible to following\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D4KoMbO2RJ9&hl=en&sa=X&d=5972951429756937768&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeZe422Jp1sh1qHY5mmZ3wIZ&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=4&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez", "2 new citations to articles by Richard Fang"]}
{"title": "OWLViz: An Open-World Benchmark for Visual Question Answering", "first_label": [], "second_label": [], "data": "T Nguyen, D Nguyen, H Nguyen, T Luong, LH Dang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe present a challenging benchmark for the Open WorLd VISual question \nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that \nrequire integrating multiple capabilities, including visual understanding, web \nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on \nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini \n2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07631&hl=en&sa=X&d=10230210277250183610&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeZCzfllcDkycdlIymJFx7Lh&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=5&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Automated Benchmark Generation for Repository-Level Coding Tasks", "first_label": [], "second_label": ["Generation"], "data": "K Vergopoulos, MN M\\xc3\\xbcller, M Vechev\\xc2\\xa0- arXiv preprint arXiv:2503.07701, 2025\nCode Agent development is an extremely active research area, where a reliable \nperformance metric is critical for tracking progress and guiding new developments. \nThis demand is underscored by the meteoric rise in popularity of SWE-Bench. This \nbenchmark challenges code agents to generate patches addressing GitHub issues \ngiven the full repository as context. The correctness of generated patches is then \nevaluated by executing a human-written test suite extracted from the repository after\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.07701&hl=en&sa=X&d=7005412014360862536&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeaW6_om8Fr-qRJw2hmx4TLh&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=6&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Why Do Multiagent Systems Fail?", "first_label": [], "second_label": ["Agent"], "data": "MZ Pan, M Cemri, LA Agrawal, S Yang, B Chopra\\xe2\\x80\\xa6\\xc2\\xa0- ICLR 2025 Workshop on Building\\xc2\\xa0\\xe2\\x80\\xa6\nDespite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM \nagents collaborate to accomplish tasks, their performance gains across popular \nbenchmarks remain minimal compared to single-agent frameworks. This gap \nhighlights the need to analyze the challenges hindering MAS effectiveness. In this \npaper we conduct the first comprehensive study of challenges of MAS across 5 \npopular Multi-Agent Systems over 150+ tasks. We conduct an investigation with four\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DwM521FqPvI&hl=en&sa=X&d=16901806311960849862&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaebeH6Ajrjn9HYE7HVFJEPvb&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=7&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Towards Robust and Scalable Evaluation for Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "WL Chiang - 2024\nThe rapid advancement of Large Language Models (LLMs), driven by scaling laws \nand substantial computational investments, has revolutionized artificial intelligence, \nenabling significant breakthroughs across domains such as conversational AI, \nprogramming, and complex reasoning. However, traditional benchmarks, being static \nand narrowly scoped, struggle to comprehensively evaluate LLM capabilities in real-\nworld scenarios. These limitations are further compounded by potential\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://escholarship.org/content/qt51r72944/qt51r72944.pdf&hl=en&sa=X&d=4592380092285880539&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeZM9OKNgk3kiC-SM8eSIR89&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=8&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Model Evaluations Need Rigorous and Transparent Human Baselines", "first_label": [], "second_label": [], "data": "K Wei, P Paskov, S Dev, MJ Byun, A Reuel\\xe2\\x80\\xa6\\xc2\\xa0- ICLR 2025 Workshop on Building\\xc2\\xa0\\xe2\\x80\\xa6\nThis position paper argues that human baselines in foundation model evaluations \nmust be more rigorous and more transparent to enable meaningful comparisons of \nhuman vs. AI performance.} Human performance baselines are vital for the machine \nlearning community, downstream users, and policymakers to interpret AI evaluations. \nModels are often claimed to achieve``super-human''performance, but existing \nbaselining methods are neither sufficiently rigorous nor sufficiently well-documented\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaCarets: A consistency and robustness evaluative test suite for vqa\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DVbG9sIsn4F&hl=en&sa=X&d=5198268575558107115&ei=cxHYZ5OzCOOO6rQPq9iPqQQ&scisig=AFWwaeYMEYFNoKHB7xr3OCWOHD8L&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=9&folt=cit", "ref": ["10 new citations to articles by Carlos E. Jimenez"]}
{"title": "Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging", "first_label": ["Large Language Models", "Bug"], "second_label": [], "data": "P Straubinger, M Kreis, S Lukasczyk, G Fraser\\xc2\\xa0- arXiv preprint arXiv:2503.08182, 2025\nLarge Language Models (LLMs) can generate plausible test code. Intuitively they \ngenerate this by imitating tests seen in their training data, rather than reasoning \nabout execution semantics. However, such reasoning is important when applying\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08182&hl=vi&sa=X&d=2332004138139485912&ei=cxHYZ_vnBp-_6rQPzI2_uQ0&scisig=AFWwaeYcNR65MJHLfE5YUPXL-MLn&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Exploring Depths of WebAudio: Advancing Greybox Fuzzing for Vulnerability Detection in Safari", "first_label": ["Vulnerabilities", "Fuzzing"], "second_label": ["Detection"], "data": "J Wang, J Wang, J Xie, Z Li, Y Chen, P Qian\nWebAudio is a widely used audio processing API in popular browsers, which \nprovides rich audio support for the exclusive browser Safari on macOS. Given its \nwidespread use, it is critical to thoroughly test WebAudio to ensure its reliability\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://users.cs.northwestern.edu/~ychen/Papers/WebAudio_APSEC.pdf&hl=vi&sa=X&d=14421413917870641412&ei=cxHYZ_vnBp-_6rQPzI2_uQ0&scisig=AFWwaeaAZBPJhY_hEZJJoC6kkjo6&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=1&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Fixer-level supervised contrastive learning for bug assignment", "first_label": ["Bug"], "second_label": [], "data": "R Wang, X Ji, Y Tian, S Xu, X Sun, S Jiang\\xc2\\xa0- Empirical Software Engineering, 2025\nDeep neural network (DNN)-based approaches have rapidly gained prominence as \na leading method in automating bug assignment, a task that is both time-intensive \nand critical for effective bug triage in software development. However, DNNs have\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10634-0&hl=vi&sa=X&d=2585297586798999943&ei=cxHYZ_vnBp-_6rQPzI2_uQ0&scisig=AFWwaeZW1YD-zmskx21214lhmIxM&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=2&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Symbolic Execution of Unmodified SystemC Peripherals", "first_label": [], "second_label": [], "data": "KA Rudkowski, S Ahmadi-Pour, R Drechsler\nIn the modern hardware design process, the ever-increasing complexity of designs \nposes a challenge. An important step in the workflow is the verification, which aims to \nidentify errors early on. To this end, executable models of the design, called Virtual\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://agra.informatik.uni-bremen.de/doc/work/mbmv2025_AR.pdf&hl=vi&sa=X&d=10595622509437163807&ei=cxHYZ_vnBp-_6rQPzI2_uQ0&scisig=AFWwaeZXUx-jhmbJGrPvGlHeCBMO&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=3&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging", "first_label": ["Large Language Models"], "second_label": [], "data": "A Djuhera, S Kadhe, F Ahmed, S Zawad, H Boche\\xc2\\xa0- ICLR 2025 Workshop on Building Trust\\xc2\\xa0\\xe2\\x80\\xa6\nFine-tuning large language models (LLMs) on downstream tasks can inadvertently \nerode their safety alignment, even for benign fine-tuning datasets. We address this \nchallenge by proposing SafeMERGE, a post\\xe2\\x80\\x93fine-tuning framework that preserves \nsafety while maintaining task utility. It achieves this by selectively merging fine-tuned \nand safety-aligned model layers only when those deviate from safe behavior, \nmeasured by a cosine similarity criterion. We evaluate SafeMERGE against other\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dd8LFGLGMRA&hl=en&sa=X&d=3163041960388358134&ei=chHYZ5LNOZuw6rQP27yHsAU&scisig=AFWwaeY0N8gVnpyRg0f_8i7ehAzp&oi=scholaralrt&hist=apJ4fD8AAAAJ:9077511576393718270:AFWwaeYjhZg9MUHEYuARvipEszZC&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Richard Fang"]}
{"title": "Moneta: Ex-Vivo GPU Driver Fuzzing by Recalling In-Vivo Execution States", "first_label": ["Fuzzing"], "second_label": [], "data": "J Jung, J Jang, Y Jo, J Vinck, A Voulimeneas\\xe2\\x80\\xa6\nGraphics Processing Units (GPUs) have become an indispensable part of modern \ncomputing infrastructure. They can execute massively parallel tasks on large data \nsets and have rich user space-accessible APIs for 3D rendering and generalpurpose\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-218-paper.pdf&hl=en&sa=X&d=4536160784874836119&ei=chHYZ8_gO4a56rQP1a-HqQw&scisig=AFWwaeZSpFxKVjEvBs8doewhVJqc&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}

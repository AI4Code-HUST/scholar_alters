{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention", "first_label": ["Large Language Models"], "second_label": [], "data": "S Yang, J Guo, H Tang, Q Hu, G Xiao, J Tang, Y Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown remarkable potential in processing long \nsequences, yet efficiently serving these long-context models remains challenging \ndue to the quadratic computational complexity of attention in the prefilling stage and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14866%3F&hl=en&sa=X&d=15664006594860475276&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeYz5XhtIZXaHxxrx6sNKSYk&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Imperceptible Backdoor Attacks on Text-Guided 3D Scene Grounding", "first_label": [], "second_label": [], "data": "D Liu, W Hu\\xc2\\xa0- IEEE Transactions on Multimedia, 2025\nWith the maturity of depth sensors, the vulnerability of 3D point cloud models has \nreceived increasing attention in various applications such as autonomous driving \nand robot navigation. Previous 3D adversarial attackers mainly focus on attacking\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10891607/&hl=en&sa=X&d=15631879208678090546&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeb2vVVPMT4s6XsyqzH_994B&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Safechain: Safety of language models with long chain-of-thought reasoning capabilities", "first_label": [], "second_label": [], "data": "F Jiang, Z Xu, Y Li, L Niu, Z Xiang, B Li, BY Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nEmerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage \nlong chain-of-thought (CoT) reasoning to generate structured intermediate steps, \nenhancing their reasoning capabilities. However, long CoT does not inherently\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12025%3F&hl=en&sa=X&d=11794055058332912936&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeZIkep8RWr0oJ-gto7S9nZK&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research", "Hong Jin Kang - new related research"]}
{"title": "Adversarial Sample Generation Based on Model Simulation Analysis in Intrusion Detection Systems", "first_label": [], "second_label": ["Detection", "Generation"], "data": "J Sun, S Yang\\xc2\\xa0- Electronics, 2025\nThe explosive development of artificial intelligence technology is profoundly affecting \nthe strategic landscape of cyberspace security, demonstrating enormous potential in \nthe field of intrusion detection. Recent research has found that machine learning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2079-9292/14/5/870&hl=en&sa=X&d=14677941750205145749&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeb2l7KoMtz9Rk6j0b_v_eqU&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Self-regularization with latent space explanations for controllable llm-based classification", "first_label": ["Large Language Models"], "second_label": [], "data": "X Wu, W Yu, X Zhai, N Liu\\xc2\\xa0- arXiv preprint arXiv:2502.14133, 2025\nModern text classification methods heavily rely on contextual embeddings from large \nlanguage models (LLMs). Compared to human-engineered features, these \nembeddings provide automatic and effective representations for classification model\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14133&hl=en&sa=X&d=4757495942301661489&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeY4_J4wOHv0aBKN7bjXACSi&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning", "first_label": ["Large Language Models"], "second_label": [], "data": "T Xie, Z Gao, Q Ren, H Luo, Y Hong, B Dai, J Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInspired by the success of DeepSeek-R1, we explore the potential of rule-based \nreinforcement learning (RL) in large reasoning models. To analyze reasoning \ndynamics, we use synthetic logic puzzles as training data due to their controllable\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14768&hl=en&sa=X&d=11806421076571041592&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeZNJUBzVDx4LJM6wPl4G0un&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Lume: Llm unlearning with multitask evaluations", "first_label": ["Large Language Models"], "second_label": [], "data": "A Ramakrishna, Y Wan, X Jin, KW Chang, Z Bu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUnlearning aims to remove copyrighted, sensitive, or private content from large \nlanguage models (LLMs) without a full retraining. In this work, we develop a multi-\ntask unlearning benchmark (LUME) which features three tasks:(1) unlearn\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15097&hl=en&sa=X&d=15431032367192817700&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeYrLEcIffndgl1M7o1B_0a5&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "P He, Y Lin, S Dong, H Xu, Y Xing, H Liu\\xc2\\xa0- arXiv preprint arXiv:2502.14847, 2025\nLarge Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized \ncomplex problem-solving capability by enabling sophisticated agent collaboration \nthrough message-based communications. While the communication framework is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14847&hl=en&sa=X&d=4575843784399184237&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaebRRgkSW7o0WE8Wa_tm7jBC&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?", "first_label": ["Large Language Models"], "second_label": [], "data": "L Pan, A Liu, S Huang, Y Lu, X Hu, L Wen, I King\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe radioactive nature of Large Language Model (LLM) watermarking enables the \ndetection of watermarks inherited by student models when trained on the outputs of \nwatermarked teacher models, making it a promising tool for preventing unauthorized\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11598&hl=en&sa=X&d=12264103332853854678&ei=RZDbZ6yNOJGu6rQP96u16Qo&scisig=AFWwaeZ9XviGOEemzrPnk9_AhBxL&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "first_label": [], "second_label": [], "data": "M Oyama, H Yamagiwa, Y Takase, H Shimodaira\\xc2\\xa0- arXiv preprint arXiv:2502.16173, 2025\nTo compare autoregressive language models at scale, we propose using log-\nlikelihood vectors computed on a predefined text set as model features. This \napproach has a solid theoretical basis: when treated as model coordinates, their\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16173&hl=en&sa=X&d=1172765274563837678&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeZTv7upoMviITq65L9yzqfB&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "first_label": [], "second_label": [], "data": "A Narayan, D Biderman, S Eyuboglu, A May\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe investigate an emerging setup in which a small, on-device language model (LM) \nwith access to local data communicates with a frontier, cloud-hosted LM to solve real-\nworld tasks involving financial, medical, and scientific reasoning over long\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15964&hl=en&sa=X&d=9683035044090951544&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeaw3_fDH9NhBejdO8K5wsfH&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision", "first_label": [], "second_label": [], "data": "D Zhu, X Wei, G Zhao, W Wu, H Zou, J Ran, X Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advances in Large Language Models (LLMs) have highlighted the challenge \nof handling long-context tasks, where models need to reason over extensive input \ncontexts to aggregate target information. While Chain-of-Thought (CoT) prompting\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20790&hl=en&sa=X&d=15555072442700286297&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaebsBDLwA9frPKCINS5aODkL&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Process-based Self-Rewarding Language Models", "first_label": [], "second_label": [], "data": "S Zhang, X Liu, X Zhang, J Liu, Z Luo, S Huang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models have demonstrated outstanding performance across \nvarious downstream tasks and have been widely applied in multiple scenarios. \nHuman-annotated preference data is used for training to further improve LLMs'\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.03746&hl=en&sa=X&d=9535134590722729967&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeYmBjvjsP_YEsr1ofmAH2_n&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Language Models Can Predict Their Own Behavior", "first_label": [], "second_label": [], "data": "D Ashok, J May\\xc2\\xa0- arXiv preprint arXiv:2502.13329, 2025\nAutoregressive Language Models output text by sequentially predicting the next \ntoken to generate, with modern methods like Chain-of-Thought (CoT) prompting \nachieving state-of-the-art reasoning capabilities by scaling the number of generated\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13329&hl=en&sa=X&d=15514504307712566951&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaebsBF3m-dOwUfEoW7VuVtXb&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?", "first_label": [], "second_label": [], "data": "Y Zhang, L Wang, M Fang, Y Du, C Huang, J Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDistilling large language models (LLMs) typically involves transferring the teacher \nmodel's responses through supervised fine-tuning (SFT). However, this approach \nneglects the potential to distill both data (output content) and reward signals (quality\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19557&hl=en&sa=X&d=12438440425156467205&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeZ-M5iqzVfklEqAxg19PnsO&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "first_label": [], "second_label": [], "data": "J Tae, H Ivison, S Kumar, A Cohan\\xc2\\xa0- arXiv preprint arXiv:2502.13917, 2025\nWe introduce TESS 2, a general instruction-following diffusion language model that \noutperforms contemporary instruction-tuned diffusion models, as well as matches \nand sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13917%3F&hl=en&sa=X&d=1855260706280837543&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeZ-HiKumWKVlqKpcJBDV9eE&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning", "first_label": ["Large Language Models"], "second_label": [], "data": "W Wu, Y Jing, Y Wang, W Hu, D Tao\\xc2\\xa0- arXiv preprint arXiv:2503.01642, 2025\nRecent large language model (LLM) reasoning, despite its success, suffers from \nlimited domain knowledge, susceptibility to hallucinations, and constrained \nreasoning depth, particularly in small-scale models deployed in resource\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01642&hl=en&sa=X&d=14568014839624178945&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeZXCM0_kavqMkjD18ZX17c0&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Li, J Xu, T Liang, X Chen, Z He, Q Liu, R Wang\\xe2\\x80\\xa6\nEnhancing the reasoning capabilities of large language models (LLMs), particularly \nfor complex tasks requiring multi-step logical deductions, remains a significant \nchallenge. Traditional inference time scaling methods utilize scalar reward signals\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Jiahao-Xu-18/publication/389864892_Dancing_with_Critiques_Enhancing_LLM_Reasoning_with_Stepwise_Natural_Language_Self-Critique/links/67d53319e62c604a0dd9bb3f/Dancing-with-Critiques-Enhancing-LLM-Reasoning-with-Stepwise-Natural-Language-Self-Critique.pdf&hl=en&sa=X&d=14392526401291100925&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeZyiz58wslhRQxVTHSTvLy3&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation", "first_label": [], "second_label": [], "data": "Y Feng, J Han, Z Yang, X Yue, S Levine, J Luo\\xc2\\xa0- arXiv preprint arXiv:2502.16707, 2025\nSolving complex long-horizon robotic manipulation problems requires sophisticated \nhigh-level planning capabilities, the ability to reason about the physical world, and \nreactively choose appropriate motor skills. Vision-language models (VLMs)\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16707&hl=en&sa=X&d=9962992784011623356&ei=RZDbZ8eONfCj6rQPiOC_-A8&scisig=AFWwaeaAv-DpJGso5IwaV2hHNzIv&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "AI-Generated code for cloud devOps: Automating infrastructure as code", "first_label": ["Code"], "second_label": [], "data": "DV Talati\\xc2\\xa0- International Journal of Science and Research Archive, 2025\nThe adoption of cloud computing and DevOps practices has transformed the IT \nlandscape, enabling organizations to rapidly deploy and manage their infrastructure. \nThis paper explores the role of AI-generated code in automating the Infrastructure as \nCode process, which is a critical component of the DevOps approach.\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAi for devsecops: A landscape and future opportunities\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Dhruvitkumar-Talati/publication/389880051_AI-Generated_code_for_cloud_devOps_Automating_infrastructure_as_code/links/67d5c4c6be849d39d67a904c/AI-Generated-code-for-cloud-devOps-Automating-infrastructure-as-code.pdf&hl=en&sa=X&d=538010850182640047&ei=RZDbZ9zeNoC96rQPqJDrkQY&scisig=AFWwaeb-3Ul1sEWm2pO8hVrgkelG&oi=scholaralrt&hist=apJ4fD8AAAAJ:4465730527138788254:AFWwaebhnVuF-27TSh32-dm_KGTR&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Michael Fu"]}
{"title": "Small sample smart contract vulnerability detection method based on multi-layer feature fusion", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection"], "data": "J Fan, Y He, H Wu\\xc2\\xa0- Complex & Intelligent Systems, 2025\nThe identification of vulnerabilities in smart contracts is necessary for ensuring their \nsecurity. As a pre-trained language model, BERT has been employed in the \ndetection of smart contract vulnerabilities, exhibiting high accuracy in tasks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s40747-025-01782-3&hl=en&sa=X&d=9070163655454212135&ei=RZDbZ4uwO-OO6rQPqfLkuQg&scisig=AFWwaebHz4RVDViMhZrB1X8ahfTv&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Improving Deep Assertion Generation via Fine-Tuning Retrieval-Augmented Pre-trained Language Models", "first_label": [], "second_label": ["Generation"], "data": "Q Zhang, C Fang, Y Zheng, Y Zhang, Y Zhao, R Huang\\xe2\\x80\\xa6\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUnit testing validates the correctness of the units of the software system under test \nand serves as the cornerstone in improving software quality and reliability. To reduce \nmanual efforts in writing unit tests, some techniques have been proposed to generate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3721128&hl=en&sa=X&d=7994025115950886329&ei=RZDbZ4uwO-OO6rQPqfLkuQg&scisig=AFWwaebGza7bXsWUQ2XMouPDx_vl&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "C Cummins, V Seeker, D Grubisic, B Roziere\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 34th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across \na variety of software engineering and coding tasks. However, their application in the \ndomain of code and compiler optimization remains underexplored. Training LLMs is\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712691&hl=en&sa=X&d=15248179687375146706&ei=RZDbZ4uwO-OO6rQPqfLkuQg&scisig=AFWwaeaQLmVGmhuYPirTSRdIm2C5&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=2&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Computer Science Review", "first_label": [], "second_label": [], "data": "L Ochoa, M Hammad, G Giray, \\xc3\\x96 Babur, K Bennin\nABSTRACT API use has become prevalent in current times and its purposeful \nmanagement is of foremost importance to avoid undesired effects on client code. A \nplethora of studies focusing on the isolated investigation of different types of harmful \nAPI uses (eg, API misuse and security vulnerabilities) have been conducted before. \nHowever, a comprehensive overview of possible harmful API uses is required to help \nboth library and client developers on the management of implemented and used\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAPIfix: Output-oriented Program Synthesis for Combating Breaking\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Goerkem-Giray-2/publication/389867008_Characterising_harmful_API_uses_and_repair_techniques_Insights_from_a_systematic_review/links/67d5397ee62c604a0dd9bdea/Characterising-harmful-API-uses-and-repair-techniques-Insights-from-a-systematic-review.pdf&hl=en&sa=X&d=38162900978530351&ei=RZDbZ4z6OZ-_6rQP9pm80AE&scisig=AFWwaebWDCPYDRl_HMNFcljc1MV1&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Abhik Roychoudhury", "1 new citation to articles by Hong Jin Kang"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "Poisoned source code detection in code models", "first_label": ["Code"], "second_label": ["Detection"], "data": "E Ghannoum, M Ghafari\\xc2\\xa0- Journal of Systems and Software, 2025\nDeep learning models have gained popularity for conducting various tasks involving \nsource code. However, their black-box nature raises concerns about potential risks. \nOne such risk is a poisoning attack, where an attacker intentionally contaminates the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13459&hl=en&sa=X&d=6875396652207410319&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaeZ_hcNzoEiXth4lEVB3Rdg4&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "REFLECTA: Reflection-based Scalable and Semantic Scripting Language Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "C Zhang, G Lee, Q Liu, M Payer - 2025\nScripting languages such as Python and JavaScript have revolutionized modern \nsoftware development thanks to their flexibility and rich functionalities. However, \nscripting languages provide a large attack surface, allowing adversaries to exploit\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://nebelwelt.net/publications/files/25AsiaCCS.pdf&hl=en&sa=X&d=4212490224602162326&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaeYwLqCscLO_LxY8j6LmOli-&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Poster: FuzzLGen: Logical Seed Generation for Smart Contract Fuzzing via LLM-based Agents and Program Analysis", "first_label": ["Smart Contracts", "Large Language Models", "Fuzzing"], "second_label": ["Generation", "Agent"], "data": "S Ji, M Xu, J Wu, J Dong\nSmart contracts play a pivotal role in blockchain applications but are increasingly \ntargeted by attackers, resulting in significant financial losses. As their adoption grows \nacross various industries, ensuring their security has become more important than\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-poster-23.pdf&hl=en&sa=X&d=16053450712575666137&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaeY0MIVvxTjziPH4D5in5TpN&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Cao, S Apel, A Singla, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.15835, 2025\nLarge Language Models (LLMs) have demonstrated impressive potential in \ntranslating natural language (NL) instructions into program code. However, user \ninstructions often contain inherent ambiguities, making it challenging for LLMs to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15835&hl=en&sa=X&d=6927995321596801340&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaebpCZ-l4eJjTQ3XaxQi2c08&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Fully Autonomous Programming using Iterative Multi-Agent Debugging with Large Language Models", "first_label": ["Large Language Models", "Bug"], "second_label": ["Agent"], "data": "A Grishina, V Liventsev, A H\\xc3\\xa4rm\\xc3\\xa4, L Moonen\\xc2\\xa0- ACM Transactions on Evolutionary\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nProgram synthesis with Large Language Models (LLMs) suffers from a \\xe2\\x80\\x9cnear-miss \nsyndrome\\xe2\\x80\\x9d: the generated code closely resembles a correct solution but fails unit \ntests due to minor errors. We address this with a multi-agent framework called\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3719351&hl=en&sa=X&d=14886956962007206250&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaeY7OYiGi7vaZtjP_oFCIOEe&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Fork State-Aware Differential Fuzzing for Blockchain Consensus Implementations", "first_label": ["Fuzzing"], "second_label": [], "data": "W Kim, H Nam, M Tran, A Jalilov, Z Liang, SK Cha\\xe2\\x80\\xa6\\xc2\\xa0- 2025 IEEE/ACM 47th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nBlockchain networks allow multiple client implementations of the same consensus \nalgorithm by different developers to coexist in the same system. Ensuring correct \nimplementations among these heterogeneous clients is crucial, as even slight\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://softsec.kaist.ac.kr/~sangkilc/papers/kim-icse25.pdf&hl=en&sa=X&d=1159190808292938384&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaebAILOPL0j40YsdnjR0urVT&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=7&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Q Zhao, L Zhang, F Liu, X Lian, Q Meng, Z Jiao, Z Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation is a latency-sensitive task that demands high timeliness, but the \nautoregressive decoding mechanism of Large Language Models (LLMs) leads to \npoor inference efficiency. Existing LLM inference acceleration methods mainly focus\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.17139&hl=en&sa=X&d=12309021167084800557&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaebM1YnN4NurSD2BsoIiHS26&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Finding Missed Code Size Optimizations in Compilers using Large Language Models", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "D Italiano, C Cummins\\xc2\\xa0- Proceedings of the 34th ACM SIGPLAN International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCompilers are complex, and significant effort has been expended on testing them. \nTechniques such as random program generation and differential testing have proved \nhighly effective and have uncovered thousands of bugs in production compilers. The\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712686&hl=en&sa=X&d=12644704441073603186&ei=RZDbZ-7wPI-j6rQP872FuQ0&scisig=AFWwaebmDjw0vkGLQNvf3JLizzLp&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, R Qian, P Lin, Y Liu, C Ying, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.18793, 2025\nLarge language models (LLMs) have transformed code generation. However, most \nexisting approaches focus on mainstream languages such as Python and Java, \nneglecting the Solidity language, the predominant programming language for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18793&hl=en&sa=X&d=18108558928985012184&ei=RpDbZ8r5BYC96rQPqJDrkQY&scisig=AFWwaeYB8M09Myp3zooUoYE61pwf&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=0&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "LLM-enhanced evolutionary test generation for untyped languages", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "R Yang, X Xu, R Wang\\xc2\\xa0- Automated Software Engineering, 2025\nDynamic programming languages, such as Python, are widely used for their flexibility \nand support for rapid development. However, the absence of explicit parameter type \ndeclarations poses significant challenges in generating automated test cases. This\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00496-7&hl=en&sa=X&d=5209764566405071124&ei=RpDbZ8r5BYC96rQPqJDrkQY&scisig=AFWwaeZZcNNRu-1nh6WZ8bsL9w5L&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Wei, O Duchenne, J Copet, Q Carbonneaux, L Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe recent DeepSeek-R1 release has demonstrated the immense potential of \nreinforcement learning (RL) in enhancing the general reasoning capabilities of large \nlanguage models (LLMs). While DeepSeek-R1 and other follow-up work primarily\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18449&hl=en&sa=X&d=1711375529222221971&ei=RpDbZ8r5BYC96rQPqJDrkQY&scisig=AFWwaeavh1KD65WcmHgHtnD4-4_C&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers", "first_label": ["Large Language Models"], "second_label": [], "data": "C Yang, Z Zhao, Z Xie, H Li, L Zhang\\xc2\\xa0- arXiv preprint arXiv:2503.09002, 2025\nStatic analysis is a powerful technique for bug detection in critical systems like \noperating system kernels. However, designing and implementing static analyzers is \nchallenging, time-consuming, and typically limited to predefined bug patterns. While\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.09002&hl=vi&sa=X&d=15643934552074292819&ei=RpDbZ7H3CJ-_6rQP9pm80AE&scisig=AFWwaeb6V7qERn3_LuvEYvObZk8F&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Efficient and Interactive Evaluation of Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "DP Rontogiannis\nEvaluating large language models (LLMs) comprehensively is computationally \nexpensive and often fails to capture subtle performance differences. This thesis \naddresses two key challenges in LLM evaluation\\xe2\\x80\\x94cost-effective accuracy estimation \non static benchmarks and interactive evaluation for in-depth analysis on complex \ntasks. For the first challenge, we develop methods to estimate an LLM's accuracy \nacross multi-domain benchmarks without exhaustively evaluating every instance by\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://pergamos.lib.uoa.gr/uoa/dl/object/3471407/file.pdf&hl=en&sa=X&d=8012090218204672379&ei=RpDbZ6WuCuOO6rQPqfLkuQg&scisig=AFWwaeYwQl5DcWq-FWTsDDKM1GHk&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Carlos E. Jimenez"]}
{"title": "Moneta: Ex-Vivo GPU Driver Fuzzing by Recalling In-Vivo Execution States", "first_label": ["Fuzzing"], "second_label": [], "data": "J Jung, J Jang, Y Jo, J Vinck, A Voulimeneas\\xe2\\x80\\xa6\nGraphics Processing Units (GPUs) have become an indispensable part of modern \ncomputing infrastructure. They can execute massively parallel tasks on large data \nsets and have rich user space-accessible APIs for 3D rendering and generalpurpose\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-218-paper.pdf&hl=en&sa=X&d=4536160784874836119&ei=RpDbZ7nKAaOD6rQP5a732AM&scisig=AFWwaeZSpFxKVjEvBs8doewhVJqc&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Assessing Correctness in LLM-Based Code Generation via Uncertainty Estimation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "A Sharma, C David\\xc2\\xa0- arXiv preprint arXiv:2502.11620, 2025\nIn this work, we explore uncertainty estimation as a proxy for correctness in LLM-\ngenerated code. To this end, we adapt two state-of-the-art techniques from natural \nlanguage generation--one based on entropy and another on mutual information--to\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11620&hl=vi&sa=X&d=14560493479388924450&ei=RpDbZ-WAA5uw6rQP69m7uAo&scisig=AFWwaeb4PEjPl8aeAkPacMwePWtQ&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=0&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Predicting Vulnerabilities in Computer Source Code Using Non-Investigated Software Metrics", "first_label": ["Vulnerabilities", "Code"], "second_label": [], "data": "FK Agbenyegah, J Chen, M Asante, E Akpaku\\xc2\\xa0- Software Quality Journal, 2025\nFlaws in the design of the computer systems, bugs, and vulnerabilities cause failures \nin computer systems. Various techniques such as machine learning and deep \nlearning algorithms are used to predict and detect vulnerabilities. Such techniques\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nTriet H. M. Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11219-025-09715-6&hl=en&sa=X&d=7039013015737457832&ei=RpDbZ--9B5Gu6rQP96u16Qo&scisig=AFWwaebwAuxxQdg84wh-VrGC-ZIr&oi=scholaralrt&hist=apJ4fD8AAAAJ:15725322226479601129:AFWwaeYp-8wbw5OHTjoCHLP43E0V&html=&pos=0&folt=rel", "ref": ["Triet H. M. Le - new related research"]}

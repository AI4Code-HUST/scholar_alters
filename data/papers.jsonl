{"title": "Safety Misalignment Against Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Gong, D Ran, X He, T Cong, A Wang, X Wang\nThe safety alignment of Large Language Models (LLMs) is crucial to prevent unsafe \ncontent that violates human values. To ensure this, it is essential to evaluate the \nrobustness of their alignment against diverse malicious attacks. However, the lack of \na large-scale, unified measurement framework hinders a comprehensive \nunderstanding of potential vulnerabilities. To fill this gap, this paper presents the first \ncomprehensive evaluation of existing and newly proposed safety misalignment\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1089-paper.pdf&hl=en&sa=X&d=5196537008963733896&ei=iU7PZ-jCE8uZieoP8KKJmA8&scisig=AFWwaeabrsFDQ04Nhh6CJa8UeFjj&oi=scholaralrt&hist=apJ4fD8AAAAJ:9077511576393718270:AFWwaeYjhZg9MUHEYuARvipEszZC&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Richard Fang"]}
{"title": "SmartUpdater: Enabling Transparent, Automated, and Secure Maintenance of Stateful Smart Contracts", "first_label": ["Smart Contracts"], "second_label": [], "data": "X Zhang, Y Song, Y Du, C Cai, H Cheng, K Xu, Q Li\\xc2\\xa0- IEEE Transactions on Software\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSmart contracts in the Ethereum system are stored tamper-resistant, complicating \nnecessary maintenance for offering new functionalities or fixing security \nvulnerabilities. Previous contract maintenance approaches mainly focus on logic \nmodification using delegatecall-based patterns. While popular, they fail to handle \ndata state updates (like storage layout changes), leading to impracticality and \nsecurity risks in real-world applications. To address these challenges, this paper\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSmart contract development: Challenges and opportunities\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10915552/&hl=en&sa=X&d=6330628563417334667&ei=iU7PZ-f0FI-j6rQP9eflkQQ&scisig=AFWwaeaPa0yuKAlQ56kc4UQgG0Fe&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Bach Le", "5 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Diagnosing NuGet Dependency Conflicts", "first_label": [], "second_label": [], "data": "Y Wang, SC Cheung, H Yu, Z Zhu\\xc2\\xa0- Managing Software Supply Chains, 2025\nDevelopers usually suffer from dependency conflict (DC) issues, ie, package \ndependency constraints are violated when a project's platform or dependencies are \nchanged. This problem is especially serious in .NET ecosystem due to its fragmented \nplatforms (eg, .NET Framework, .NET Core, and .NET Standard). Fixing DC issues is \nchallenging due to the complexity of dependency constraints: Multiple DC issues \noften occur in one project, solving one DC issue usually causes another DC issue\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaHistory driven program repair\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-96-1797-5_5&hl=en&sa=X&d=18320740909122224436&ei=iU7PZ-f0FI-j6rQP9eflkQQ&scisig=AFWwaeYB4HSY3Vc80GDh-EuIidFs&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=1&folt=cit", "ref": ["2 new citations to articles by Bach Le", "10 new citations to articles by Abhik Roychoudhury"]}
{"title": "MultiGLICE: Combining Graph Neural Networks and Program Slicing for Multiclass Software Vulnerability Detection", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "W de Kraker, H Vranken, A Hommersom\\xc2\\xa0- Computers, 2025\nThis paper presents MultiGLICE (Multi class Graph Neural Network with Program \nSlice), a model for static code analysis to detect security vulnerabilities. MultiGLICE \nextends our previous GLICE model with multiclass detection for a large number of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2073-431X/14/3/98&hl=en&sa=X&d=7079962426056731964&ei=iU7PZ-OMEOOO6rQP_JDSoQ0&scisig=AFWwaeahQbkPNZhuwZoOTZhxExj0&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research", "Triet H. M. Le - new related research", "Thanh Le-Cong - new related research"]}
{"title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding", "first_label": ["Code"], "second_label": [], "data": "J Li, X Guo, L Li, K Zhang, G Li, Z Tao, F Liu, C Tao\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCurrent advanced long-context language models offer great potential for real-world \nsoftware engineering applications. However, progress in this critical domain remains \nhampered by a fundamental limitation: the absence of a rigorous evaluation\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.04359&hl=en&sa=X&d=12236359998315825621&ei=iU7PZ-OMEOOO6rQP_JDSoQ0&scisig=AFWwaebAj2q-fttLuaO3C_PMJf1n&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "J Lin, D Mohaisen\nLarge Language Models (LLMs) have demonstrated strong potential in tasks such as \ncode understanding and generation. This study evaluates several advanced LLMs\\xe2\\x80\\x94\nsuch as LLaMA-2, CodeLLaMA, LLaMA-3, Mistral, Mixtral, Gemma, CodeGemma\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1491-paper.pdf&hl=en&sa=X&d=4792361613594223308&ei=iU7PZ-OMEOOO6rQP_JDSoQ0&scisig=AFWwaeY2mHkuZqyAPpG8grIXpgws&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=2&folt=rel", "ref": ["Michael Fu - new related research", "Triet H. M. Le - new related research", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Knowledge-Enhanced Program Repair for Data Science Code", "first_label": ["Automated Program Repair", "Code"], "second_label": ["Repair"], "data": "S Ouyang, JM Zhang, Z Sun, AM Penuela\\xc2\\xa0- arXiv preprint arXiv:2502.09771, 2025\nThis paper introduces DSrepair, a knowledge-enhanced program repair method \ndesigned to repair the buggy code generated by LLMs in the data science domain. \nDSrepair uses knowledge graph based RAG for API knowledge retrieval as well as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09771&hl=en&sa=X&d=3654048262265967&ei=iU7PZ-OMEOOO6rQP_JDSoQ0&scisig=AFWwaea3b7K5alddz2-3Pw0svJ0R&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=3&folt=rel", "ref": ["Michael Fu - new related research", "Bach Le - new related research", "Thanh Le-Cong - new related research"]}
{"title": "EnCus: Customizing Search Space for Automated Program Repair", "first_label": ["Automated Program Repair"], "second_label": ["Repair"], "data": "S Kim, S Jang, J Kim, J Nam\nThe primary challenge faced by Automated Program Repair (APR) techniques in \nfixing buggy programs is the search space problem. To generate a patch, APR \ntechniques must address three critical decisions: where to fix (location), how to fix\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://lifove.github.io/files/ICST_2025_EnCus.pdf&hl=en&sa=X&d=13746921339775956765&ei=iU7PZ-OMEOOO6rQP_JDSoQ0&scisig=AFWwaeZGniVL7NTqatFPJ-6HNr8S&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=4&folt=rel", "ref": ["Michael Fu - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "Improving Automated Issue Resolution via Comprehensive Repository Exploration", "first_label": [], "second_label": [], "data": "Y MA, Y Liu\\xc2\\xa0- ICLR 2025 Third Workshop on Deep Learning for Code\nThis paper presents LingmaAgent, a novel Automated Software Engineering method \ndesigned to comprehensively understand and utilize whole software repositories for \nissue resolution. LingmaAgent addresses the limitations of existing LLM-based \nagents that primarily focus on local code information. Our approach introduces a top-\ndown method to condense critical repository information into a knowledge graph, \nreducing complexity, and employs a Monte Carlo tree search based strategy\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutoCodeRover: Autonomous program improvement\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DwRHlCz4haP&hl=en&sa=X&d=18220751152747438020&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeaTGpqgU0I_dFnnAfBmyB9w&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "7 new citations to articles by Carlos E. Jimenez"]}
{"title": "Moneta: Ex-Vivo GPU Driver Fuzzing by Recalling In-Vivo Execution States", "first_label": ["Fuzzing"], "second_label": [], "data": "J Jung, J Jang, Y Jo, J Vinck, A Voulimeneas\\xe2\\x80\\xa6\nGraphics Processing Units (GPUs) have become an indispensable part of modern \ncomputing infrastructure. They can execute massively parallel tasks on large data \nsets and have rich user space-accessible APIs for 3D rendering and generalpurpose \nparallel programming. Unfortunately, the GPU drivers that bridge the gap between \nthese APIs and the underlying hardware have grown increasingly large and complex \nover the years. Many GPU drivers now expose broad attack surfaces and pose\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaStateful Greybox Fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-218-paper.pdf&hl=en&sa=X&d=4536160784874836119&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeZSpFxKVjEvBs8doewhVJqc&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "Abhik Roychoudhury - new related research"]}
{"title": "Sheep's Clothing, Wolf's Data: Detecting Server-Induced Client Vulnerabilities in Windows Remote IPC", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "F Gu, Q Guo, J Lu, Q Xie, B Zhao, K Lu, H Li, X Gong\nThe Windows operating system employs various inter-process communication (IPC) \nmechanisms, typically involving a privileged server and a less privileged client. \nHowever, scenarios exist where the client has higher privileges, such as a \nperformance monitor running as a domain controller obtaining data from a domain \nmember via IPC. In these cases, the server can be compromised and send crafted \ndata to the client. Despite the increase in Windows client applications, existing\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBLEEM: Packet Sequence Oriented Fuzzing for Protocol\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1303-paper.pdf&hl=en&sa=X&d=12294441596307211673&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaebW3WVi0-fouUdkqCkdww26&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=2&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "MALintent: Coverage Guided Intent Fuzzing Framework for Android", "first_label": ["Fuzzing"], "second_label": [], "data": "A Askar, F Fleischer, C Kruegel, G Vigna, T Kim\nIntents are the primary message-passing mechanism on Android, used for both \ncommunication between intra-app and inter-app components. Intents go across the \ntrust boundary of applications and can break the security isolation between them. \nDue to their shared API with intra-app communication, apps may unintentionally \nexpose functionality leading to important security bugs. MALintent is an open-source \nfuzzing framework that uses novel coverage instrumentation techniques and\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaTime-travel Testing of Android Apps\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-125-paper.pdf&hl=en&sa=X&d=3178905131392962367&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeaHsivTY3QNklCfKvEeyUTY&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=3&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "Hong Jin Kang - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Exploring Cross Ecosystem Vulnerability Impacts", "first_label": ["Vulnerabilities"], "second_label": [], "data": "Y Wang, SC Cheung, H Yu, Z Zhu\\xc2\\xa0- Managing Software Supply Chains, 2025\nVulnerabilities, referred to as CLV issues, are induced by cross-language \ninvocations of vulnerable libraries. Such issues greatly increase the attack surface of \nPython/Java projects due to their pervasive use of C libraries. Existing Python/Java \nbuild tools in PyPI and Maven ecosystems fail to report the dependency on \nvulnerable libraries written in other languages such as C. CLV issues are easily \nmissed by developers. In this chapter, we conduct the first empirical study on the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Patch Backporting in Linux (Experience Paper)\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-96-1797-5_7&hl=en&sa=X&d=17417787542610843162&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeauHcUbEUpvX_FL0NNHNw2b&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=4&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "5 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "GadgetMeter: Quantitatively and Accurately Gauging the Exploitability of Speculative Gadgets", "first_label": [], "second_label": ["Exploit"], "data": "Q Ling, Y Liang, Y Ren, B Kasikci, S Deng\nSince their emergence in 2018, speculative execution attacks have proven difficult to \nfully prevent without substantial performance overhead. This is because most \nmitigations hurt modern processors' speculative nature, which is essential to many \noptimization techniques. To address this, numerous scanners have been developed \nto identify vulnerable code snippets (speculative gadgets) within software \napplications, allowing mitigations to be applied selectively and thereby minimizing\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaoo7: Low-overhead defense against spectre attacks via program\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1723-paper.pdf&hl=en&sa=X&d=9851994678427264474&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeYBQIp_lhU5BI8yDwRtwELC&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=5&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Solidifying Modern SMT Solvers", "first_label": [], "second_label": [], "data": "D Winterer - 2024\nSatisfiability Modulo Theory (SMT) solvers realize one of the most powerful and \nmature classes of formal methods. They are foundational for many important \nadvances in software research and industry. For example, one large-scale \napplication is realized by AWS, which uses SMT solvers for verifying properties of \ncloud services. In almost all of their applications, SMT solvers are critical components \nsolving NP-hard problems and establish trust through proof. Hence SMT solvers\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSmart greybox fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/725868/1/Thesis_DominikWinterer.pdf&hl=en&sa=X&d=13530241746146656142&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeb7FQSwP9HFdL88iHXuV09b&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=6&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "5 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Silence False Alarms: Identifying Anti-Reentrancy Patterns on Ethereum to Refine Smart Contract Reentrancy Detection", "first_label": ["Smart Contracts"], "second_label": ["Detection"], "data": "Q Song, H Huang, X Jia, Y Xie, J Cao\nReentrancy vulnerabilities in Ethereum smart contracts have caused significant \nfinancial losses, prompting the creation of several automated reentrancy detectors. \nHowever, these detectors frequently yield a high rate of false positives due to coarse \ndetection rules, often misclassifying contracts protected by anti-reentrancy patterns \nas vulnerable. Thus, there is a critical need for the development of specialized \nautomated tools to assist these detectors in accurately identifying anti-reentrancy\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaDirected greybox fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-167-paper.pdf&hl=en&sa=X&d=18241554667497232549&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaeaRS7qQ1PpFPIjhDFC88fRJ&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=8&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A Comprehensive Memory Safety Analysis of Bootloaders", "first_label": [], "second_label": [], "data": "J Wang, M Wang, Q Wang, N Langius, L Shi, A Abbasi\\xe2\\x80\\xa6\nThe bootloader plays an important role during the boot process, as it connects two \ncrucial components: the firmware and the operating system. After powering on, the \nbootloader takes control from the firmware, prepares the early boot environment, and \nthen hands control over to the operating system. Modern computers often use a \nfeature called secure boot to prevent malicious software from loading at startup. As a \nkey part of the secure boot chain, the bootloader is responsible for verifying the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaCoverage-based greybox fuzzing as markov chain\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-330-paper.pdf&hl=en&sa=X&d=8453972940816259793&ei=iU7PZ4TTDtSyieoPooS_iQo&scisig=AFWwaebDdn8oUaaJ6VFpP74eFeAj&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=9&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Code2JSON: Can a Zero-Shot LLM Agent Extract Code Features for Code RAG?", "first_label": ["Large Language Models", "Code"], "second_label": ["Agent"], "data": "A Singhal, R Ghosh, R Mundra, H Dadlani, D Dutta\\xc2\\xa0- ICLR 2025 Third Workshop on Deep\\xc2\\xa0\\xe2\\x80\\xa6\nA retrieval-augmented generation (RAG) framework that accepts natural language \n(NL) queries and returns contextual responses based on source code is crucial for \nenhancing developer productivity. However, building a code RAG system is\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DglBWrVLvKi&hl=en&sa=X&d=11144165502460504562&ei=iU7PZ8GtJNSyieoPooS_iQo&scisig=AFWwaeY8HJiVRFGiPbA1RbV7KmJo&oi=scholaralrt&hist=apJ4fD8AAAAJ:16488056128958629805:AFWwaeZVy5biUXZBZUZeh3-Oz0_I&html=&pos=1&folt=rel", "ref": ["Bach Le - new related research", "Hong Jin Kang - new related research", "Thanh Le-Cong - new related research"]}
{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention", "first_label": ["Large Language Models"], "second_label": [], "data": "S Yang, J Guo, H Tang, Q Hu, G Xiao, J Tang, Y Lin\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown remarkable potential in processing long \nsequences, yet efficiently serving these long-context models remains challenging \ndue to the quadratic computational complexity of attention in the prefilling stage and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14866%3F&hl=en&sa=X&d=15664006594860475276&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeYz5XhtIZXaHxxrx6sNKSYk&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "H Chiu, R Hachiuma, CY Wang, SF Smith, YCF Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCurrent autonomous driving vehicles rely mainly on their individual sensors to \nunderstand surrounding scenes and plan for future trajectories, which can be \nunreliable when the sensors are malfunctioning or occluded. To address this\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09980&hl=en&sa=X&d=15240013219582954077&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeYlIvaPITJgQzXxI4cpjd3u&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "ASDroid: Resisting Evolving Android Malware With API Clusters Derived From Source Code", "first_label": ["Code"], "second_label": [], "data": "Q Hu, W Wang, H Song, S Guo, J Zhang, S Zhang\\xc2\\xa0- IEEE Transactions on Information\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nMachine learning-based Android malware detection has consistently demonstrated \nsuperior results. However, with the continual evolution of the Android framework, the \nefficacy of the deployed models declines markedly. Existing solutions necessitate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10884652/&hl=en&sa=X&d=11327102995599250404&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeYMsBMMLEkLIoZcRuGHvBae&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?", "first_label": ["Large Language Models"], "second_label": [], "data": "L Pan, A Liu, S Huang, Y Lu, X Hu, L Wen, I King\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe radioactive nature of Large Language Model (LLM) watermarking enables the \ndetection of watermarks inherited by student models when trained on the outputs of \nwatermarked teacher models, making it a promising tool for preventing unauthorized\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11598&hl=en&sa=X&d=12264103332853854678&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeZ9XviGOEemzrPnk9_AhBxL&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "first_label": ["Large Language Models"], "second_label": [], "data": "F Teng, Z Yu, Q Shi, J Zhang, C Wu, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.12018, 2025\nLarge Language Models (LLMs) achieve superior performance through training-time \nscaling, and test-time scaling further enhances their capabilities by conducting \neffective reasoning during inference. However, as the scale of reasoning increases\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12018&hl=en&sa=X&d=5296971756206632797&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeazodUFuwbTJYXzJxtcPLeB&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Copilot Arena: A Platform for Code LLM Evaluation in the Wild", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "W Chi, V Chen, AN Angelopoulos, WL Chiang, A Mittal\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nEvaluating in-the-wild coding capabilities of large language models (LLMs) is a \nchallenging endeavor with no clear solution. We introduce Copilot Arena, a platform \nto collect user preferences for code generation through native integration into a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09328&hl=en&sa=X&d=3229167444522934279&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaea0URw9WxLsZFHBU9Ahsjdi&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Interrogating LLM design under a fair learning doctrine", "first_label": ["Large Language Models"], "second_label": [], "data": "JTZ Wei, M Wang, A Godbole, JH Choi, R Jia\\xc2\\xa0- arXiv preprint arXiv:2502.16290, 2025\nThe current discourse on large language models (LLMs) and copyright largely takes \na\" behavioral\" perspective, focusing on model outputs and evaluating whether they \nare substantially similar to training data. However, substantial similarity is difficult to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.16290&hl=en&sa=X&d=6524917216695590297&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeYrA86e0kVlVPm3bLVWoqIL&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning", "first_label": ["Large Language Models"], "second_label": [], "data": "T Xie, Z Gao, Q Ren, H Luo, Y Hong, B Dai, J Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInspired by the success of DeepSeek-R1, we explore the potential of rule-based \nreinforcement learning (RL) in large reasoning models. To analyze reasoning \ndynamics, we use synthetic logic puzzles as training data due to their controllable\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14768&hl=en&sa=X&d=11806421076571041592&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeZNJUBzVDx4LJM6wPl4G0un&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Human-LLM Coevolution: Evidence from Academic Writing", "first_label": ["Large Language Models"], "second_label": [], "data": "M Geng, R Trotta\\xc2\\xa0- arXiv preprint arXiv:2502.09606, 2025\nWith a statistical analysis of arXiv paper abstracts, we report a marked drop in the \nfrequency of several words previously identified as overused by ChatGPT, such as\" \ndelve\", starting soon after they were pointed out in early 2024. The frequency of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09606&hl=en&sa=X&d=14993182510478052659&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeaZy5fAark3RNIZuLWcdxps&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM-enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information", "first_label": ["Large Language Models"], "second_label": ["Detection"], "data": "R Yang, J Ma, W Gao, H Lin\\xc2\\xa0- ACM Transactions on Intelligent Systems and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe proliferation of misinformation, such as rumors on social media, has drawn \nsignificant attention, prompting various expressions of stance among users. Although \nrumor detection and stance detection are distinct tasks, they can complement each\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3716856&hl=en&sa=X&d=15732701514671975914&ei=iU7PZ8f-DJ-_6rQPucOumQ0&scisig=AFWwaeYnw7gAUhI1Oi3zaXW6UlcK&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Type-Aware Constraining for Code LLMs", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "N M\\xc3\\xbcndler, J He, H Wang, K Sen, D Song, M Vechev\\xc2\\xa0- ICLR 2025 Third Workshop on Deep\\xc2\\xa0\\xe2\\x80\\xa6\nLarge Language Models (LLMs) have achieved notable success in code generation. \nHowever, they still frequently produce invalid code, as they do not precisely model \nformal aspects of programming languages. Constrained decoding is a promising \napproach to alleviate this issue and has been successfully applied to domain-\nspecific languages and syntactic features, but is not able to enforce more semantic \nfeatures, such as well-typedness. To address this issue, we introduce* type-aware\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DDNAapYMXkc&hl=en&sa=X&d=11491721645478756561&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaeZzg-ChAigJly751DCCc149&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Agentic Knowledgeable Self-awareness", "first_label": [], "second_label": ["Agent"], "data": "S Qiao, Z Qiu, B Ren, X Wang, X Ru, N Zhang, X Chen\\xe2\\x80\\xa6\\xc2\\xa0- Workshop on Reasoning and\\xc2\\xa0\\xe2\\x80\\xa6\nLarge language agents have achieved considerable performance across various \nagentic planning tasks. However, most current agent learning methods are spoon-\nfeeding, with gold trajectories, external feedback and knowledge mindlessly feeding \ninto agent models regardless of their actual needs, resulting in a lack of self-\nconsciousness during the planning process. In this paper, we introduce KnowSelf, a \ndata-centric approach that enables agents to have knowledgeable self-awareness\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-agent: Agent-computer interfaces enable automated software\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DPGdSLjYwMT&hl=en&sa=X&d=6469184348617056120&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaea5pcKA6NzykNw7_QKKgXr9&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=2&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez"]}
{"title": "Diagnosing Robotics Systems Issues with Large Language Models\\xe2\\x80\\x93A Case Study", "first_label": ["Large Language Models"], "second_label": [], "data": "JE Herrmann, AM Gopinath, M Norrlof, MN Mueller\\xc2\\xa0- ICLR 2025 Third Workshop on Deep\\xc2\\xa0\\xe2\\x80\\xa6\nQuickly resolving issues reported in industrial robotics applications is crucial to \nminimize economic impact. However, the required data analysis makes diagnosing \nthe underlying root causes a challenging and time-consuming task, even for experts. \nIn contrast, large language models (LLMs) excel at quickly analyzing large amounts \nof data. Indeed, prior work in AI-Ops demonstrates their effectiveness for IT systems. \nHere, we extend this work to the challenging and largely unexplored domain of\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DfBCi9XsFdS&hl=en&sa=X&d=9526161706277619953&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaeb7a46YSWjLH2OSK6bYPVfr&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=3&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez"]}
{"title": "Tasks, Challenges, and Paths Towards AI for Software Engineering", "first_label": [], "second_label": [], "data": "A Gu, N Jain, WD Li, M Shetty, K Ellis, K Sen\\xe2\\x80\\xa6\\xc2\\xa0- ICLR 2025 Third Workshop on\\xc2\\xa0\\xe2\\x80\\xa6\nAI for software engineering has made remarkable progress, becoming a notable \nsuccess within generative AI. Despite this, achieving fully automated software \nengineering is still a significant challenge, requiring research efforts across both \nacademia and industry. In this position paper, our goal is threefold. First, we provide \na taxonomy of measures and tasks to categorize work towards AI software \nengineering. Second, we outline the key bottlenecks permeating today's approaches\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dt0QaoJqjdw&hl=en&sa=X&d=13081790019098657231&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaeZkd63XLOrn709-74EIlkwE&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=4&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez"]}
{"title": "Automated Benchmark Generation for Repository-Level Coding Tasks", "first_label": [], "second_label": ["Generation"], "data": "MN Mueller, M Vechev\\xc2\\xa0- ICLR 2025 Third Workshop on Deep Learning for Code\nCode Agent development is an extremely active research area, where a reliable \nperformance metric is critical for tracking progress and guiding new developments. \nThis demand is underscored by the meteoric rise in popularity of SWE-Bench\\xe2\\x80\\x93a \nbenchmark that challenges code agents to generate patches addressing GitHub \nissues given the full repository as context and then evaluates their correctness by \nexecuting the human-written test suite extracted from the repository after the issue's\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DBQA7dkV3iZ&hl=en&sa=X&d=14847475577159228722&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaea8CnpO9Xbuz4e01VKQkILD&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=5&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez"]}
{"title": "EnvBench: A Benchmark for Automated Environment Setup", "first_label": [], "second_label": [], "data": "A Eliseeva, A Kovrigin, I Kholkin, E Bogomolov\\xe2\\x80\\xa6\\xc2\\xa0- ICLR 2025 Third Workshop on\\xc2\\xa0\\xe2\\x80\\xa6\nRecent advances in Large Language Models (LLMs) have enabled researchers to \nfocus on practical repository-level tasks in software engineering domain. In this work, \nwe consider a cornerstone task for automating work with software repositories---\nenvironment setup, ie, a task of configuring a repository-specific development \nenvironment on a system. Existing studies on environment setup introduce \ninnovative agentic strategies, but their evaluation is often based on small datasets\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dizy1oaAOeX&hl=en&sa=X&d=348648690138425190&ei=iU7PZ6fuIpGu6rQPj4_WmQ0&scisig=AFWwaeY1W7e6xy31w-0frOUHTuuy&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=6&folt=cit", "ref": ["7 new citations to articles by Carlos E. Jimenez"]}
{"title": "Experimental Evaluation of Parameter-Efficient Fine-Tuning for Software Engineering Tasks", "first_label": [], "second_label": [], "data": "W Zou, Z Shen, Q Li, J Ge, C Li, X Chen, X Shen\\xe2\\x80\\xa6\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nPre-trained models (PTMs) have succeeded in various software engineering (SE) \ntasks following the \\xe2\\x80\\x9cpre-train then fine-tune\\xe2\\x80\\x9d paradigm. As fully fine-tuning all \nparameters of PTMs can be computationally expensive, a potential solution is \nparameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra \nparameters. Although PEFT methods have been applied to SE tasks, researchers \noften focus on specific scenarios and lack a comprehensive comparison of PTMs\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaCompressing pre-trained models of code into 3 mb\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3722107&hl=en&sa=X&d=7648647949529261260&ei=iU7PZ9SeGZuw6rQP3pK8kQ0&scisig=AFWwaebCcjky2l_jOJJm7yu-1fsb&oi=scholaralrt&hist=apJ4fD8AAAAJ:11486195984023826531:AFWwaebYo-fw1j0PJswL-CdomZqY&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Hong Jin Kang", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=iU7PZ4umFqOD6rQPgq2AmQM&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=2&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Towards cost-efficient vulnerability detection with cross-modal adversarial reprogramming", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "Z Tian, R Qiu, Y Teng, J Sun, Y Chen, L Chen\\xc2\\xa0- Journal of Systems and Software, 2025\nWhile deep learning has advanced the automatic detection of software \nvulnerabilities, current DL-based methods still face two major obstacles: the scarcity \nof vulnerable code samples and the high computational cost of training models from\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0164121225000330&hl=en&sa=X&d=17446884386547932313&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeZtqZ9sta5N5NchBizwtudz&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Combining Large Language Models with Static Analyzers for Code Review Generation", "first_label": ["Large Language Models", "Code Review", "Code"], "second_label": ["Generation"], "data": "I Jaoua, OB Sghaier, H Sahraoui\\xc2\\xa0- arXiv preprint arXiv:2502.06633, 2025\nCode review is a crucial but often complex, subjective, and time-consuming activity in \nsoftware development. Over the past decades, significant efforts have been made to \nautomate this process. Early approaches focused on knowledge-based systems\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.06633&hl=en&sa=X&d=15947875415862330768&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeZNoEKeFXzps09u7HRXyGKK&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Lightweight Concolic Testing via Path-Condition Synthesis for Deep Learning Libraries", "first_label": [], "second_label": [], "data": "S Kim, Y Kim, D Park, Y Jeon, J Yi, M Kim\nMany techniques have been recently developed for testing deep learning (DL) \nlibraries. Although these techniques have effectively improved API and code \ncoverage and detected unknown bugs, they rely on blackbox fuzzing for input\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.jooyongyi.com/papers/ICSE25.pdf&hl=en&sa=X&d=8556081671027224537&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeY1z_V3GkFVvQ4yKrS6-u2H&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Exploring the Boundaries Between LLM Code Clone Detection and Code Similarity Assessment on Human and AI-Generated Code", "first_label": ["Large Language Models", "Code"], "second_label": ["Detection"], "data": "Z Zhang, T Saber\\xc2\\xa0- Big Data and Cognitive Computing, 2025\nAs Large Language Models (LLMs) continue to advance, their capabilities in code \nclone detection have garnered significant attention. While much research has \nassessed LLM performance on human-generated code, the proliferation of LLM\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2504-2289/9/2/41&hl=en&sa=X&d=7163896401567065808&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeY_HW0udh7HBf1OBMjNkIQs&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Beyond Code Generation: LLM-supported Exploration of the Program Design Space", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "JD Zamfirescu-Pereira, E Jun, M Terry, Q Yang\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 2025\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn this work, we explore explicit LLM-powered support for the iterative design of \ncomputer programs (LLM: Large Language Models). Program design, like other \ndesign activity, is characterized by navigating a space of alternative problem\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Qian-Yang-19/publication/389352001_Beyond_Code_Generation_LLM-supported_Exploration_of_the_Program_Design_Space/links/67bf36fbf5cb8f70d5c07e50/Beyond-Code-Generation-LLM-supported-Exploration-of-the-Program-Design-Space.pdf&hl=en&sa=X&d=16434837762286584924&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeYWRDPz93MgnZ1UwNYe_lq3&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging", "first_label": ["Code", "Bug"], "second_label": ["Generation", "Agent"], "data": "MA Islam, ME Ali, MR Parvez\\xc2\\xa0- arXiv preprint arXiv:2502.05664, 2025\nLarge Language Models (LLMs) have made significant strides in code generation \nand problem solving. Current approaches employ external tool-based iterative \ndebuggers that use compiler or other tool-based runtime feedback to refine coarse\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.05664&hl=en&sa=X&d=12341045938762212071&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeZPC32jk5OxisWuzI9Ifz0k&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=7&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Distinguishing GUI Component States for Blind Users using Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "M Zhang, H Liu, C Du, T Wang, H Li, P Huang, C Chen\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nGraphical User Interfaces (GUIs) serve as the primary medium for user interaction \nwith mobile applications (apps). Within these GUIs, editable text views, buttons, and \nother visual elements exhibit different states following user actions. However\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3722106&hl=en&sa=X&d=1868474899543317373&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaea9XhQBYk3y6qKw_Jww3W_L&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "MARIN: A Research-Centric Interface for Querying Software Artifacts on Maven Repositories", "first_label": [], "second_label": [], "data": "J D\\xc3\\xbcsing, J Chiaramonte, B Hermann\nMaven Central is the largest open repository for JVM libraries, hosting just under 15 \nmillion artifacts as of November 2024. Its popularity has made it a prime target for \nmalicious actors to upload malware or exploit vulnerabilities\\xe2\\x80\\x93one in eight open\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://sse.cs.tu-dortmund.de/storages/sse-cs/r/Publications/Preprints/marin-paper-main.pdf&hl=en&sa=X&d=14248612257349812073&ei=iU7PZ-LmEdmlieoP3v630Q8&scisig=AFWwaeZYlg4cHvGKIk9iDebOX8g6&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Statically Discover Cross-Entry Use-After-Free Vulnerabilities in the Linux Kernel", "first_label": ["Vulnerabilities"], "second_label": [], "data": "H Zhang, J Kim, C Yuan, Z Qian, T Kim\nUse-After-Free (UAF) is one of the most widely spread and severe memory safety \nissues, attracting lots of research efforts toward its automatic discovery. Existing UAF \ndetection approaches include two major categories: dynamic and static. While \ndynamic methods like fuzzing can detect UAF issues with high precision, they are \ninherently limited in code coverage. Static approaches, on the other hand, can \nusually only discover simple sequential UAF cases, despite that many real-world\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaSAVER: scalable, precise, and safe memory-error repair\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-559-paper.pdf&hl=vi&sa=X&d=15645029291428186661&ei=iU7PZ9ySHoa56rQPpO3LmAY&scisig=AFWwaeZPFOzxb5F6vIdCMD4MxO3Y&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=2&folt=cit", "ref": ["5 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Boosting the Propagation of Vulnerability Fixes in the npm Ecosystem", "first_label": ["Vulnerabilities"], "second_label": [], "data": "Y Wang, SC Cheung, H Yu, Z Zhu\\xc2\\xa0- Managing Software Supply Chains, 2025\nVulnerabilities are known reported security threats that affect a large amount of \npackages in the npm ecosystem. To mitigate these security threats, the open-source \ncommunity strongly suggests vulnerable packages to timely publish vulnerability \nfixes and recommends affected packages to update their dependencies. However, \nthere are still serious lags in the propagation of vulnerability fixes in the ecosystem. \nIn our preliminary study on the latest versions of 356,283 active npm packages, we\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaVuddy: A scalable approach for vulnerable code clone discovery\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-96-1797-5_8&hl=vi&sa=X&d=18335818146879854513&ei=iU7PZ9ySHoa56rQPpO3LmAY&scisig=AFWwaebTnyQgMsa3visEgoRU7AyK&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=4&folt=cit", "ref": ["5 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Mtzk: Testing and exploring bugs in zero-knowledge (zk) compilers", "first_label": ["Bug"], "second_label": [], "data": "D Xiao, Z Liu, Y Peng, S Wang\\xc2\\xa0- NDSS, 2025\nZero-knowledge (ZK) proofs have been increasingly popular in privacy-preserving \napplications and blockchain systems. To facilitate handy and efficient ZK proof \ngeneration for normal users, the industry has designed domain-specific languages\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-530-paper.pdf&hl=vi&sa=X&d=7133253247485155458&ei=iU7PZ-WeIeehieoP5YbikA4&scisig=AFWwaebyAygYgWus5d4YlweA6xJL&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Shedding Light on Task Decomposition in Program Synthesis: The Driving Force of the Synthesizer Model", "first_label": [], "second_label": [], "data": "J Zenkner, T Sesterhenn, C Bartelt\\xc2\\xa0- ICLR 2025 Third Workshop on Deep Learning for Code\nTask decomposition is a fundamental mechanism in program synthesis, enabling \ncomplex problems to be broken down into manageable subtasks. ExeDec, a state-of-\nthe-art program synthesis framework, employs this approach by combining a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D3vqnyd2Xe8&hl=vi&sa=X&d=5681135367372230282&ei=iU7PZ-WeIeehieoP5YbikA4&scisig=AFWwaeY19BSVm706oTJCIHDkL0EO&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=2&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Model and Program Repair via Group Actions and Structure Unwinding", "first_label": ["Automated Program Repair"], "second_label": ["Repair"], "data": "PC Attie, WL Cocke\\xc2\\xa0- ACM Transactions on Computational Logic, 2025\nGiven a program, one can construct a Kripke structure. Model checking verifies that \nsatisfies a behavioral property given by a temporal logic formula by checking that \nmodels. However, can be exponentially large in. The action of a symmetry group on\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3719008&hl=vi&sa=X&d=2109452603573455180&ei=iU7PZ-WeIeehieoP5YbikA4&scisig=AFWwaeYEIeEE-XieRhvi0b7RSE76&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=4&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "On Pretraining For Project-Level Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "M Sapronov, E Glukhov\\xc2\\xa0- ICLR 2025 Third Workshop on Deep Learning for Code\nRepository-level pretraining is commonly used to enable large language models for \ncode to leverage codebase-wide context. This enhances their ability to generate \naccurate and context-aware code completions. In this work, we investigate how\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dt9RN9WX4Ic&hl=vi&sa=X&d=254312521362447426&ei=iU7PZ9vRF5uoieoPoZvUwQ4&scisig=AFWwaeYy31n38ph2hZiLE0WoVG9x&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=0&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "CodeTransEngine: Ready-to-use Backend for LLM-based Code Translation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "M Macedo, Y Tian, B Adams\\xc2\\xa0- ICLR 2025 Third Workshop on Deep Learning for Code\nCode translation, the process of converting a program from one programming \nlanguage to another, plays a significant role in modernizing legacy systems, \nensuring cross-platform compatibility, and improving performance of programs. With\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D9WHJuzjPJf&hl=vi&sa=X&d=7115567169628787236&ei=iU7PZ9vRF5uoieoPoZvUwQ4&scisig=AFWwaeYVQSkic-gX7RBwnMwH7PJt&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=1&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM-enhanced evolutionary test generation for untyped languages", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "R Yang, X Xu, R Wang\\xc2\\xa0- Automated Software Engineering, 2025\nDynamic programming languages, such as Python, are widely used for their flexibility \nand support for rapid development. However, the absence of explicit parameter type \ndeclarations poses significant challenges in generating automated test cases. This\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00496-7&hl=en&sa=X&d=5209764566405071124&ei=iU7PZ8WVG5m7ieoPlNmJgQ4&scisig=AFWwaeZZcNNRu-1nh6WZ8bsL9w5L&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks", "first_label": [], "second_label": [], "data": "L\\xc3\\x83\\xc4\\xbb Vaugrante, F Carlon, M Menke, T Hagendorff\\xc2\\xa0- arXiv preprint arXiv:2502.08301, 2025\nRecent research on large language models (LLMs) has demonstrated their ability to \nunderstand and employ deceptive behavior, even without explicit prompting. \nHowever, such behavior has only been observed in rare, specialized cases and has\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.08301&hl=en&sa=X&d=1314709450220270892&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaebOnmwnilqF5sVkJCLG7eZp&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?", "first_label": [], "second_label": [], "data": "J Nielsen, P Schneider-Kamp, L Galke\\xc2\\xa0- arXiv preprint arXiv:2502.11895, 2025\nLarge language models (LLMs) require immense resources for training and \ninference. Quantization, a technique that reduces the precision of model parameters, \noffers a promising solution for improving LLM efficiency and sustainability. While post\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11895&hl=en&sa=X&d=5315475009734524900&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeac1W1T5CLpz9ocqASaiX6z&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Language Models Can Predict Their Own Behavior", "first_label": [], "second_label": [], "data": "D Ashok, J May\\xc2\\xa0- arXiv preprint arXiv:2502.13329, 2025\nAutoregressive Language Models output text by sequentially predicting the next \ntoken to generate, with modern methods like Chain-of-Thought (CoT) prompting \nachieving state-of-the-art reasoning capabilities by scaling the number of generated\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13329&hl=en&sa=X&d=15514504307712566951&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaebsBF3m-dOwUfEoW7VuVtXb&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?", "first_label": [], "second_label": [], "data": "Y He, Y Li, J Wu, Y Sui, Y Chen, B Hooi\\xc2\\xa0- arXiv preprint arXiv:2502.12206, 2025\nAs large language models (LLMs) continue to evolve, ensuring their alignment with \nhuman goals and values remains a pressing challenge. A key concern is\\\\textit \n{instrumental convergence}, where an AI system, in optimizing for a given objective\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12206&hl=en&sa=X&d=5833515845218376624&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeZB0gZ0Nl88Ufuj-qhTc1MK&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "first_label": [], "second_label": [], "data": "Y Wang, S Rao, JU Lee, M Jobanputra, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.12992, 2025\nPost-hoc explanation methods for black-box models often struggle with faithfulness \nand human interpretability due to the lack of explainability in current neural models. \nMeanwhile, B-cos networks have been introduced to improve model explainability\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12992&hl=en&sa=X&d=7039606964488193257&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaebRHmCZCdnPUGexBI1c1bjs&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Wu, I Hua, Y Ding\\xc2\\xa0- arXiv preprint arXiv:2502.11256, 2025\nLarge language models (LLMs) offer powerful capabilities but come with significant \nenvironmental costs, particularly in carbon emissions. Existing studies benchmark \nthese emissions but lack a standardized basis for comparison across models. To\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.11256&hl=en&sa=X&d=839300699424972036&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeYhFYoT1V_AEDvHWQ4CZ4HT&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "C Zhou, X Zhang, D Song, X Chen, W Gu, H Ma, Y Tian\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation has attracted increasing attention with the rise of Large Language \nModels (LLMs). Many studies have developed powerful code LLMs by synthesizing \ncode-related instruction data and applying supervised fine-tuning. However, these\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.09183&hl=en&sa=X&d=4921929302450311894&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeb5DexZAETaatPK1XsuBUJ2&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub", "first_label": [], "second_label": [], "data": "MM Imran, R Zita, R Copeland, P Chatterjee\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSoftware projects thrive on the involvement and contributions of individuals from \ndifferent backgrounds. However, toxic language and negative interactions can hinder \nthe participation and retention of contributors and alienate newcomers. Proactive\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.02191&hl=en&sa=X&d=15149573248957324571&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeZ6hP8Fw2mRbLHp6UyhwPq1&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Leveraging Large Language Model for Enhanced Text-to-SQL Parsing", "first_label": ["Large Language Models"], "second_label": [], "data": "Z Zhan, E Haihong, M Song\\xc2\\xa0- IEEE Access, 2025\nText-to-SQL conversion, the process of generating SQL queries from natural \nlanguage input, has gained significant attention due to its potential to simplify \ndatabase interaction. Although benchmarks in this task have driven advancements in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10878990.pdf&hl=en&sa=X&d=7295255887749706478&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaeZI-HAeWzeYORn4hN-IErKh&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "Y Zhuang, J Yang, H Jiang, X Liu, K Cheng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDue to the scarcity of agent-oriented pre-training data, LLM-based autonomous \nagents typically rely on complex prompting or extensive fine-tuning, which often fails \nto introduce new capabilities while preserving strong generalizability. We introduce\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.06589&hl=en&sa=X&d=667679063374418144&ei=iU7PZ4OcC5Gu6rQPj4_WmQ0&scisig=AFWwaebDGwnx6g1HXW-CAG3y4s-D&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}

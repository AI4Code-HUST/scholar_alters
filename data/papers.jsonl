{"title": "Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging", "first_label": ["Large Language Models", "Bug"], "second_label": [], "data": "P Straubinger, M Kreis, S Lukasczyk, G Fraser\\xc2\\xa0- arXiv preprint arXiv:2503.08182, 2025\nLarge Language Models (LLMs) can generate plausible test code. Intuitively they \ngenerate this by imitating tests seen in their training data, rather than reasoning \nabout execution semantics. However, such reasoning is important when applying\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08182&hl=vi&sa=X&d=2332004138139485912&ei=GdHfZ4_9J6OD6rQPx5mGgAY&scisig=AFWwaeYcNR65MJHLfE5YUPXL-MLn&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=0&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "ICSQuartz: Scan Cycle-Aware and Vendor-Agnostic Fuzzing for Industrial Control Systems", "first_label": ["Fuzzing"], "second_label": [], "data": "C Villa, C Doumanidis, H Lamri, PHN Rajput\\xe2\\x80\\xa6\nIndustrial Control Systems (ICS) ensure the automation and safe operation of critical \nindustry, energy, and commerce processes. Despite its importance, ICS code often \ncannot be evaluated as rigorously as software on traditional computing platforms, as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-795-paper.pdf&hl=en&sa=X&d=10524862650214833634&ei=GdHfZ8XOJpm7ieoP6cWmgQE&scisig=AFWwaeZZ_2ARI6V22CenXEygxr-a&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Exploring Depths of WebAudio: Advancing Greybox Fuzzing for Vulnerability Detection in Safari", "first_label": ["Vulnerabilities", "Fuzzing"], "second_label": ["Detection"], "data": "J Wang, J Wang, J Xie, Z Li, Y Chen, P Qian\nWebAudio is a widely used audio processing API in popular browsers, which \nprovides rich audio support for the exclusive browser Safari on macOS. Given its \nwidespread use, it is critical to thoroughly test WebAudio to ensure its reliability\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://users.cs.northwestern.edu/~ychen/Papers/WebAudio_APSEC.pdf&hl=en&sa=X&d=14421413917870641412&ei=GdHfZ8XOJpm7ieoP6cWmgQE&scisig=AFWwaeaAZBPJhY_hEZJJoC6kkjo6&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=1&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "SwFuzz: Structure-Sensitive WebAssembly Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "J Wang, Z Guo, X Ying, P Qian, Y Chen\nWebAssembly (WASM) has rapidly emerged as a ubiquitous target for web browsers, \nserver-side applications, and blockchain platforms, with promising performance and \nportability. As WASM grows in popularity, ensuring its security and resilience\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://users.cs.northwestern.edu/~ychen/Papers/SWFuzz_ASIA.pdf&hl=en&sa=X&d=3575353584380034570&ei=GdHfZ8XOJpm7ieoP6cWmgQE&scisig=AFWwaeZNlmOmuNwoN0uMt4pWuU0v&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=2&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "\\xd0\\x90\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd0\\xb7\\xd0\\xb0\\xd1\\x86\\xd1\\x96\\xd1\\x8f \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd0\\xbb\\xd1\\x8f\\xd0\\xb4\\xd1\\x83 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xba\\xd0\\xbe\\xd0\\xb4\\xd1\\x83 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbe\\xd1\\x81\\xd0\\xbd\\xd0\\xbe\\xd0\\xb2\\xd1\\x96 \\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd0\\xbe\\xd0\\xb4\\xd1\\x96\\xd0\\xb2 \\xd0\\xbc\\xd0\\xb0\\xd1\\x88\\xd0\\xb8\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd1\\x87\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f", "first_label": [], "second_label": [], "data": "\\xd0\\x93\\xd0\\x9e \\xd0\\x97\\xd0\\xb0\\xd1\\x81\\xd0\\xbf\\xd0\\xb0, \\xd0\\x92\\xd0\\x92 \\xd0\\x91\\xd1\\x96\\xd0\\xbb\\xd0\\xb8\\xd0\\xba\\xc2\\xa0- \\xd0\\x90\\xd0\\xba\\xd1\\x82\\xd1\\x83\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x96 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb1\\xd0\\xbb\\xd0\\xb5\\xd0\\xbc\\xd0\\xb8 \\xd0\\xb0\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0\\xd1\\x82\\xd0\\xb8\\xd0\\xb7\\xd0\\xb0\\xd1\\x86\\xd1\\x96\\xd1\\x97 \\xd1\\x82\\xd0\\xb0\\xc2\\xa0\\xe2\\x80\\xa6, 2024\n\\xd0\\x90\\xd0\\xbd\\xd0\\xbe\\xd1\\x82\\xd0\\xb0\\xd1\\x86\\xd1\\x96\\xd1\\x8f \\xd0\\x9f\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd0\\xbb\\xd1\\x8f\\xd0\\xb4 \\xd0\\xba\\xd0\\xbe\\xd0\\xb4\\xd1\\x83 \\xd1\\x94 \\xd0\\xb4\\xd1\\x96\\xd1\\x8f\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x96\\xd1\\x81\\xd1\\x82\\xd1\\x8e \\xd1\\x96\\xd0\\xb7 \\xd0\\xb7\\xd0\\xb0\\xd0\\xb1\\xd0\\xb5\\xd0\\xb7\\xd0\\xbf\\xd0\\xb5\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x8f\\xd0\\xba\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd1\\x96 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \n\\xd0\\xb7\\xd0\\xb0\\xd0\\xb1\\xd0\\xb5\\xd0\\xb7\\xd0\\xbf\\xd0\\xb5\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f, \\xd0\\xb4\\xd0\\xb5 \\xd0\\xb2\\xd0\\xb8\\xd0\\xba\\xd0\\xbe\\xd0\\xbd\\xd1\\x83\\xd1\\x94\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb2\\xd1\\x96\\xd1\\x80\\xd0\\xba\\xd0\\xb0 \\xd0\\xba\\xd0\\xbe\\xd0\\xb4\\xd1\\x83 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x8f\\xd0\\xba\\xd1\\x96\\xd1\\x81\\xd1\\x82\\xd1\\x8c \\xd0\\xb9\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0\\xd0\\xbf\\xd0\\xb8\\xd1\\x81\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x82\\xd0\\xb0 \n\\xd0\\xbd\\xd0\\xb0\\xd1\\x8f\\xd0\\xb2\\xd0\\xbd\\xd1\\x96\\xd1\\x81\\xd1\\x82\\xd1\\x8c \\xd0\\xbf\\xd0\\xbe\\xd0\\xbc\\xd0\\xb8\\xd0\\xbb\\xd0\\xbe\\xd0\\xba \\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xb8\\xd0\\xbc \\xd0\\xb0\\xd0\\xb1\\xd0\\xbe \\xd0\\xba\\xd1\\x96\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd1\\x96\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xbc\\xd0\\xb8, \\xd1\\x96 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0\\xd0\\xb9\\xd0\\xbc\\xd0\\xbd\\xd1\\x96 \\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd0\\xbd \\xd0\\xb7 \\xd1\\x86\\xd0\\xb8\\xd1\\x85 \n\\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd1\\x96\\xd1\\x81\\xd1\\x82\\xd1\\x96\\xd0\\xb2 \\xd0\\xbd\\xd0\\xb5 \\xd1\\x94 \\xd0\\xb0\\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xbc \\xd0\\xba\\xd0\\xbe\\xd0\\xb4\\xd1\\x83. \\xd0\\x92 \\xd0\\xb4\\xd0\\xb0\\xd0\\xbd\\xd1\\x96\\xd0\\xb9 \\xd1\\x80\\xd0\\xbe\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd1\\x96 \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xb4\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd0\\xb5\\xd0\\xbd\\xd0\\xbe \\xd0\\xb4\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd1\\x96\\xd0\\xb4\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \n\\xd0\\xb7\\xd0\\xb0\\xd1\\x81\\xd1\\x82\\xd0\\xbe\\xd1\\x81\\xd1\\x83\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x82\\xd0\\xb5\\xd1\\x85\\xd0\\xbd\\xd0\\xbe\\xd0\\xbb\\xd0\\xbe\\xd0\\xb3\\xd1\\x96\\xd0\\xb9 \\xd0\\xbc\\xd0\\xb0\\xd1\\x88\\xd0\\xb8\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe \\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd1\\x87\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x83 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x86\\xd0\\xb5\\xd1\\x81\\xd1\\x96 \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd0\\xbb\\xd1\\x8f\\xd0\\xb4\\xd1\\x83 \\xd0\\xba\\xd0\\xbe\\xd0\\xb4\\xd1\\x83 \\xd0\\xb7 \\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd0\\xbe\\xd1\\x8e \n\\xd0\\xbf\\xd1\\x96\\xd0\\xb4\\xd0\\xb2\\xd0\\xb8\\xd1\\x89\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x8f\\xd0\\xba\\xd0\\xbe\\xd1\\x81\\xd1\\x82\\xd1\\x96 \\xd1\\x82\\xd0\\xb0 \\xd0\\xb7\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd0\\xb2\\xd0\\xb8\\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd1\\x82 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x86\\xd1\\x96\\xd1\\x94\\xd1\\x97 \\xd0\\xb2\\xd0\\xb0\\xd0\\xb6\\xd0\\xbb\\xd0\\xb8\\xd0\\xb2\\xd0\\xbe\\xd1\\x97 \\xd0\\xb2 \\xd1\\x81\\xd1\\x84\\xd0\\xb5\\xd1\\x80\\xd1\\x96\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaGeneration-based Code Review Automation: How Far Are We?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nXin ZHOU\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://actualproblems.dp.ua/index.php/APAIT/article/download/257/228&hl=vi&sa=X&d=12677006549824059636&ei=GdHfZ_SpLJuw6rQP15iV4Qw&scisig=AFWwaeaRXynIpsQMExLxpZdGpYDN&oi=scholaralrt&hist=apJ4fD8AAAAJ:11724652424841979500:AFWwaeb06hHZ-3j7Bb1sOMTsP9ed&html=&pos=0&folt=cit", "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Bugfix: a standard language, database schema and repository for research on bugs and automatic program repair", "first_label": ["Automated Program Repair", "Bug"], "second_label": ["Repair"], "data": "V Kananchuk, I Mustafin, B Meyer\\xc2\\xa0- arXiv preprint arXiv:2502.15599, 2025\nAutomatic Program Repair (APR) is a brilliant idea: when detecting a bug, also \nprovide suggestions for correcting the program. Progress towards that goal is \nhindered by the absence of a common frame of reference for the multiplicity of APR\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15599&hl=en&sa=X&d=10642094292575353027&ei=GdHfZ7--M8uZieoPiNWegAU&scisig=AFWwaebYZkmLIso9Pt-1zoBNSzF7&oi=scholaralrt&hist=apJ4fD8AAAAJ:16488056128958629805:AFWwaeZVy5biUXZBZUZeh3-Oz0_I&html=&pos=0&folt=rel", "ref": ["Bach Le - new related research", "Thanh Le-Cong - new related research", "Hong Jin Kang - new related research", "Michael Fu - new related research"]}
{"title": "Automated deployment and rollback strategies for docker containers in continuous integration/continuous deployment (CI/CD) pipelines", "first_label": ["Continuous Integration"], "second_label": [], "data": "A Gogineni\nThis report explores the significance of automated deployment and rollback \nstrategies within Continuous Integration/Continuous Deployment (CI/CD) pipelines, a \ncornerstone of modern software delivery practices. With the current growing trends in\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nTriet H. M. Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Anila_Gogineni/publication/389788384_Automated_deployment_and_rollback_strategies_for_docker_containers_in_continuous_integrationcontinuous_deployment_CICD_pipelines/links/67d265a4e62c604a0dd75e36/Automated-deployment-and-rollback-strategies-for-docker-containers-in-continuous-integration-continuous-deployment-CI-CD-pipelines.pdf&hl=en&sa=X&d=4552676981410064249&ei=GdHfZ8O1L9SyieoPwLeW8AY&scisig=AFWwaeZz8_vtDevpu7lVQrcZeGZo&oi=scholaralrt&hist=apJ4fD8AAAAJ:15725322226479601129:AFWwaeYp-8wbw5OHTjoCHLP43E0V&html=&pos=0&folt=rel", "ref": ["Triet H. M. Le - new related research"]}
{"title": "Simple Fault Localization using Execution Traces", "first_label": ["Fault Localization"], "second_label": [], "data": "JA Prenner, R Robbes\\xc2\\xa0- arXiv preprint arXiv:2503.04301, 2025\nTraditional spectrum-based fault localization (SBFL) exploits differences in a \nprogram's coverage spectrum when run on passing and failing test cases. However, \nsuch runs can provide a wealth of additional information beyond mere coverage\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.04301&hl=en&sa=X&d=2093297660534139222&ei=GdHfZ-jnKoWlieoP9b6EiA0&scisig=AFWwaebKjdYjJYByY7wZO158t1Ec&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=0&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Code2JSON: Can a Zero-Shot LLM Agent Extract Code Features for Code RAG?", "first_label": ["Large Language Models", "Code"], "second_label": ["Agent"], "data": "A Singhal, R Ghosh, R Mundra, H Dadlani, D Dutta\\xc2\\xa0- ICLR 2025 Third Workshop on Deep\\xc2\\xa0\\xe2\\x80\\xa6\nA retrieval-augmented generation (RAG) framework that accepts natural language \n(NL) queries and returns contextual responses based on source code is crucial for \nenhancing developer productivity. However, building a code RAG system is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DglBWrVLvKi&hl=en&sa=X&d=11144165502460504562&ei=GdHfZ-jnKoWlieoP9b6EiA0&scisig=AFWwaeY8HJiVRFGiPbA1RbV7KmJo&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Investigating Execution-Aware Language Models for Code Optimization", "first_label": ["Code"], "second_label": [], "data": "F Di Menna, L Traini, G Bavota, V Cortellessa\\xc2\\xa0- arXiv preprint arXiv:2503.08228, 2025\nCode optimization is the process of enhancing code efficiency, while preserving its \nintended functionality. This process often requires a deep understanding of the code \nexecution behavior at run-time to identify and address inefficiencies effectively\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.08228&hl=en&sa=X&d=11464541261744981902&ei=GdHfZ-jnKoWlieoP9b6EiA0&scisig=AFWwaeZOrw28MJs1HKINGUJCROdQ&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=3&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Q Zhao, L Zhang, F Liu, X Lian, Q Meng, Z Jiao, Z Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation is a latency-sensitive task that demands high timeliness, but the \nautoregressive decoding mechanism of Large Language Models (LLMs) leads to \npoor inference efficiency. Existing LLM inference acceleration methods mainly focus\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.17139&hl=en&sa=X&d=12309021167084800557&ei=GdHfZ-jnKoWlieoP9b6EiA0&scisig=AFWwaebM1YnN4NurSD2BsoIiHS26&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=4&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Improving Graph Learning-Based Fault Localization with Tailored Semi-supervised Learning", "first_label": ["Fault Localization"], "second_label": [], "data": "C LI, HUI LI, Z LI, M PAN, X LI - 2025\nAuthors' Contact Information: Chun Li, Nanjing University, State Key Laboratory for \nNovel Software Technology, Nanjing, China, chunli@ smail. nju. edu. cn; Hui Li, \nSamsung Electronics (China) R&D Centre, Nanjing, China, hui. li@ samsung. com;\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://pppppkun.github.io/files/fse25.pdf&hl=en&sa=X&d=3684507761350432065&ei=GdHfZ-jnKoWlieoP9b6EiA0&scisig=AFWwaeaCU1iGZOL3wdyrA6_vPx0l&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=5&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "LLM Compiler: Foundation Language Models for Compiler Optimization", "first_label": ["Large Language Models"], "second_label": [], "data": "C Cummins, V Seeker, D Grubisic, B Roziere\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the 34th\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across \na variety of software engineering and coding tasks. However, their application in the \ndomain of code and compiler optimization remains underexplored. Training LLMs is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3708493.3712691&hl=en&sa=X&d=15248179687375146706&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeaQLmVGmhuYPirTSRdIm2C5&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Lightweight Concolic Testing via Path-Condition Synthesis for Deep Learning Libraries", "first_label": [], "second_label": [], "data": "S Kim, Y Kim, D Park, Y Jeon, J Yi, M Kim\\xc2\\xa0- 2025 IEEE/ACM 47th International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nMany techniques have been recently developed for testing deep learning (DL) \nlibraries. Although these techniques have effectively improved API and code \ncoverage and detected unknown bugs, they rely on blackbox fuzzing for input\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.jooyongyi.com/papers/ICSE25.pdf&hl=en&sa=X&d=10264194252454271239&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeY1z_V3GkFVvQ4yKrS6-u2H&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Guessing as a service: large language models are not yet ready for vulnerability detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "F Panebianco, A Isgro, S Longari, S Zanero\\xe2\\x80\\xa6\\xc2\\xa0- Guessing As A Service\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe growing number of reported software vulnerabilities underscores the need for \nefficient detection methods, especially for resource-limited organizations. While \ntraditional techniques like fuzzing and symbolic execution are effective, they require\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://re.public.polimi.it/bitstream/11311/1284205/1/_ITASEC__Survey_LLMs_for_Vulnerability_Detection.pdf&hl=en&sa=X&d=9488022294044051922&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeZfyMM1sIcP_FxrcS4WQQmp&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=2&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, R Qian, P Lin, Y Liu, C Ying, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.18793, 2025\nLarge language models (LLMs) have transformed code generation. However, most \nexisting approaches focus on mainstream languages such as Python and Java, \nneglecting the Solidity language, the predominant programming language for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18793&hl=en&sa=X&d=18108558928985012184&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeYB8M09Myp3zooUoYE61pwf&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Large language models are qualified benchmark builders: Rebuilding pre-training datasets for advancing code intelligence tasks", "first_label": ["Large Language Models", "Code"], "second_label": [], "data": "K Yang, X Mao, S Wang, Y Wang, T Zhang, Y Qin, B Lin\\xe2\\x80\\xa6 - 2025\nPre-trained code models are essential for various code intelligence tasks. Yet, their \neffectiveness is heavily influenced by the quality of the pre-training dataset, \nparticularly human-written reference comments, which usually serve as a bridge\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://shangwenwang.github.io/files/ICPC-25.pdf&hl=en&sa=X&d=14609002688787455145&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeYDGpCyYS_ZxRBozcu-uBSN&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Evaluating Spectrum-based Fault Localization on Deep Learning Libraries", "first_label": ["Fault Localization"], "second_label": [], "data": "M Yan, J Chen, T Jiang, J Jiang, Z Wang\\xc2\\xa0- IEEE Transactions on Software\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDeep learning (DL) libraries have become increasingly popular and their quality \nassurance is also gaining significant attention. Although many fault detection \ntechniques have been proposed, effective fault localization techniques tailored to DL\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10930847/&hl=en&sa=X&d=13097586511829065482&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeZIPeEWx0xchtsAVxGkT88r&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "From Large to Mammoth: A Comparative Evaluation of Large Language Models in Vulnerability Detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "J Lin, D Mohaisen\nLarge Language Models (LLMs) have demonstrated strong potential in tasks such as \ncode understanding and generation. This study evaluates several advanced LLMs\\xe2\\x80\\x94\nsuch as LLaMA-2, CodeLLaMA, LLaMA-3, Mistral, Mixtral, Gemma, CodeGemma\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-1491-paper.pdf&hl=en&sa=X&d=4792361613594223308&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeY2mHkuZqyAPpG8grIXpgws&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=7&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems", "first_label": [], "second_label": [], "data": "F Mohammad, D Ryu\\xc2\\xa0- arXiv preprint arXiv:2502.20806, 2025\nIn recent years, the rise of autonomous driving technologies has highlighted the \ncritical importance of reliable software for ensuring safety and performance. This \npaper proposes a novel approach for just-in-time software defect prediction (JIT\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20806&hl=en&sa=X&d=3195013519886661176&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaebzwIiPbIJ5E8l5WxPgCa_3&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=8&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Effective Directed Fuzzing with Hierarchical Scheduling for Web Vulnerability Detection", "first_label": ["Vulnerabilities", "Fuzzing"], "second_label": ["Detection"], "data": "Z Lin, Y Zhang, J Dai, X Huang, B Xiang, G Yang\\xe2\\x80\\xa6\nJava web applications play a pivotal role in the modern digital landscape. Due to \ntheir widespread use and significant importance, Java web applications have been \none prime target for cyber attacks. In this work, we propose a novel directed fuzzing\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://yuanxzhang.github.io/paper/WDFuzz-security25.pdf&hl=en&sa=X&d=3557548300279399061&ei=GdHfZ4uuI4-j6rQP4qbHWQ&scisig=AFWwaeaLhhLY4fHR10500ee-UQZz&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research", "5 new citations to articles by Abhik Roychoudhury", "Michael Fu - new related research", "Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Formal Trust and Threat Modeling Using Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "Z Yao\\xc2\\xa0- 2024 Annual Computer Security Applications\\xc2\\xa0\\xe2\\x80\\xa6, 2024\nSecurity modeling, including trust and threat modeling, is a critical process of modern \nsystem design and analysis. However, the models are often described in imprecise \nnatural languages, and their inconsistent interpretations and implementations can \nlead to cybersecurity incidents. In this work, we first introduce an extended Linear \nTemporal Logic to model the multi-faceted security model of a system to capture its \ntemporal and spatial properties and security guarantees. Then, we manually write 10\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10917671/&hl=en&sa=X&d=14688613334751679992&ei=GdHfZ6inIMuZieoPiNWegAU&scisig=AFWwaebJTuwp1PUJ0uMqejB-WC9U&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "Enhancing Database Encryption: Adaptive Measures for Digital Assets Against LLMs-Based Reverse Engineering", "first_label": ["Large Language Models"], "second_label": [], "data": "K Zhou, J Qiu, Y Wang, X Ye\\xc2\\xa0- 2024 Annual Computer Security Applications\\xc2\\xa0\\xe2\\x80\\xa6, 2024\nWith the development of large language models (LLMs) technology, generative AI \n(GAI) has significantly lowered the barriers to software reverse engineering attacks. \nTraditional database system security controls, face new challenges when confronted \nwith the powerful analytical capabilities of GAI. This paper provides a detailed \ndemonstration of the reverse engineering of database cryptographic functions using \nGAI tools. It finds that existing encryption mechanisms in embedded DBSs need new\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10917597/&hl=en&sa=X&d=8313330257232076788&ei=GdHfZ6inIMuZieoPiNWegAU&scisig=AFWwaeYGGp0SXbx-uAcR1l8iSft0&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "ER-\\xf0\\x9d\\x9c\\x8b: Exhaustive Interleaving Replay for Testing Replicated Data Library Integration", "first_label": [], "second_label": [], "data": "P Mondal, E Tilevich - 2025\nModern replicated data systems often rely on libraries integrated with application \ncode. These replicated data libraries exchange asynchronous messages, whose \nexecution orderings are non-deterministic, allowing any message interleaving to \noccur during system execution. Testing the integration of application code with library \ncode requires considering all possible interleavings, whose detection and simulation \npose significant challenges for application developers. In this paper, we present ER\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaGreybox fuzzing of distributed systems\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Provakar-Mondal-2/publication/389986985_ER-_Exhaustive_Interleaving_Replay_for_Testing_Replicated_Data_Library_Integration/links/67db3450fe0f5a760f47670c/ER-Exhaustive-Interleaving-Replay-for-Testing-Replicated-Data-Library-Integration.pdf&hl=en&sa=X&d=4169933903028988829&ei=GdHfZ6inIMuZieoPiNWegAU&scisig=AFWwaeYIGYrHrPRTcoGWctyrEh6a&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=2&folt=cit", "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "Finding Metadata Inconsistencies in Distributed File Systems via Cross-Node Operation Modeling", "first_label": [], "second_label": [], "data": "F Ma, Y Chen, Y Zhou, Z Yan, H Sun, Y Jiang\nMetadata consistency is crucial for distributed file systems (DFSes) as it ensures that \ndifferent clients have a consistent view of the data. However, DFSes are inherently \nerror-prone, leading to metadata inconsistencies. Though rare, such inconsistencies \ncan have severe consequences, including data loss, service failures, and permission \nviolations. Unfortunately, there is limited understanding of metadata inconsistency \ncharacteristics, let alone an effective method for detecting them. This paper presents\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaGreybox fuzzing of distributed systems\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=http://wingtecher.com/themes/WingTecherResearch/assets/papers/paper_from_25/Horcrux_Security25.pdf&hl=en&sa=X&d=15948322227967050307&ei=GdHfZ6inIMuZieoPiNWegAU&scisig=AFWwaebSiNAiWjG_rsjt1kmAH_mY&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=3&folt=cit", "ref": ["5 new citations to articles by Abhik Roychoudhury"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=GdHfZ5XkIZuoieoPy6GmoQM&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Hardware-optimal quantum algorithms", "first_label": [], "second_label": [], "data": "S Muroya, K Chatterjee, TA Henzinger\\xc2\\xa0- Proceedings of the National Academy of\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nQuantum hardware is inherently fragile and noisy. We find that the accuracy of \ntraditional quantum error correction algorithms can be improved depending on the \nhardware. Given different hardware specifications, we automatically synthesize \nhardware-optimal algorithms for parity correction, qubit resetting, and GHZ \n(Greenberger\\xe2\\x80\\x93Horne\\xe2\\x80\\x93Zeilinger) state preparation. Using stochastic techniques from \ncomputer science, our method presents a computational tool to compute exact\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaModular component-based quantum circuit synthesis\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.pnas.org/doi/full/10.1073/pnas.2419273122&hl=vi&sa=X&d=2323115351370292722&ei=GdHfZ6nuLcCSieoPqLGJyAM&scisig=AFWwaeaM_ffTytvrZ1-Or3w-T4Yw&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit", "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh"]}
{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "first_label": [], "second_label": [], "data": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language \nand multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model \ntrained on high-quality web and synthetic data, significantly outperforming recent\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01743%3F&hl=en&sa=X&d=1080021729655208449&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeZMnVxRBi2ctuK9FSJVQwyR&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "first_label": [], "second_label": [], "data": "M Song, X Qu, J Zhou, Y Cheng\\xc2\\xa0- arXiv preprint arXiv:2503.12821, 2025\nLarge Vision-Language Models (LVLMs) have achieved significant progress in \ncombining visual comprehension with language generation. Despite this success, \nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where the data\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12821&hl=en&sa=X&d=11737421450003055590&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeYpswc1NGxvjtITVBeAhDpK&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Capturing nuanced preferences: Preference-aligned distillation for small language models", "first_label": [], "second_label": [], "data": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu\\xc2\\xa0- arXiv preprint arXiv:2502.14272, 2025\nAligning small language models (SLMs) with human values typically involves \ndistilling preference knowledge from large language models (LLMs). However, \nexisting distillation methods model preference knowledge in teacher LLMs by\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14272&hl=en&sa=X&d=3283888565285721117&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeacCk5hYRYI-fAwtLN-1pFN&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Self-Steering Language Models", "first_label": [], "second_label": [], "data": "G Grand, JB Tenenbaum, V Mansinghka, AK Lew\\xe2\\x80\\xa6\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0: VerifAI: AI Verification in the Wild\nFor many reasoning tasks, augmenting language models with test-time compute can \nsignificantly boost performance. However, scaling inference is costly for complex \nproblems that require extensive search or sampling. Nevertheless, even when LMs\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dx7E2Qt7n0V&hl=en&sa=X&d=17836731747212682693&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeYnQPOEYSmZZ5fMjuMV3S05&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Zhang, M Li, W Han, Y Yao, Z Cen, D Zhao\\xc2\\xa0- arXiv preprint arXiv:2503.05021, 2025\nLarge Language Models (LLMs) are vulnerable to jailbreak attacks that exploit \nweaknesses in traditional safety alignment, which often relies on rigid refusal \nheuristics or representation engineering to block harmful outputs. While they are\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.05021&hl=en&sa=X&d=5785695892417368823&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaea2MHnS_d1ERRbZdJf_zDWG&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Parity LLM Data Valuation", "first_label": ["Large Language Models"], "second_label": [], "data": "Y Pan, H Lin, Y Ran, J Chen, X Yu, W Zhao, D Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) heavily rely on high-quality training data, making \ndata valuation crucial for optimizing model performance, especially when working \nwithin a limited budget. In this work, we aim to offer a third-party data valuation\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01052&hl=en&sa=X&d=6331837210289932637&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaea2pohDgJbloKIzQaluufUF&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=5&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning", "first_label": [], "second_label": [], "data": "J Pan, C Liu, J Wu, F Liu, J Zhu, HB Li, C Chen\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nReasoning is a critical frontier for advancing medical image analysis, where \ntransparency and trustworthiness play a central role in both clinician trust and \nregulatory approval. Although Medical Visual Language Models (VLMs) show\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.19634&hl=en&sa=X&d=14122414530465825183&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeYKq3XbiJsqab5sdaqJdckd&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling", "first_label": [], "second_label": [], "data": "W Zhao, T Pan, X Han, Y Zhang, A Sun, Y Huang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSpeculative sampling has emerged as an important technique for accelerating the \nauto-regressive generation process of large language models (LLMs) by utilizing a \ndraft-then-verify mechanism to produce multiple tokens per forward pass. While state\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14856&hl=en&sa=X&d=4250945624494244927&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeYRHIy0HLbCZ0oP5bnAdlsn&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Multidimensional Consistency Improves Reasoning in Language Models", "first_label": [], "second_label": [], "data": "H Lai, X Zhang, M Nissim\\xc2\\xa0- arXiv preprint arXiv:2503.02670, 2025\nWhile Large language models (LLMs) have proved able to address some complex \nreasoning tasks, we also know that they are highly sensitive to input variation, which \ncan lead to different solution paths and final answers. Answer consistency across\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.02670&hl=en&sa=X&d=7890321214859696030&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaeai4iV6zDshaHZI2bm-_u3J&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Language Models can Self-Improve at State-Value Estimation for Better Search", "first_label": [], "second_label": [], "data": "E Mendes, A Ritter\\xc2\\xa0- arXiv preprint arXiv:2503.02878, 2025\nCollecting ground truth task completion rewards or human demonstrations for multi-\nstep reasoning tasks is often cost-prohibitive and time-consuming, especially in \ninteractive domains like web tasks. To address this bottleneck, we present self-taught\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.02878&hl=en&sa=X&d=7437309647568686441&ei=GdHfZ6H0GtSyieoPwLeW8AY&scisig=AFWwaebBVWmcASunkZ1dbvnH9jCb&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "CHAINFUZZ: Exploiting Upstream Vulnerabilities in Open-Source Supply Chains", "first_label": ["Vulnerabilities", "Fuzzing"], "second_label": ["Exploit"], "data": "P Deng, L Zhang, Y Meng, Z Yang, Y Zhang, M Yang\nSoftware supply chain attacks pose an increasingly severe threat to the security of \ndownstream software worldwide. A common method to mitigate these risks is \nSoftware Composition Analysis (SCA), which helps developers identify vulnerable \ndependencies. However, studies show that popular SCA approaches often suffer \nfrom high false positive rates. As a result, developers spend significant time manually \nvalidating these alerts, which delays the detection and remediation of genuinely\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaTest mimicry to assess the exploitability of library vulnerabilities\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://yuanxzhang.github.io/paper/chainfuzz-security25.pdf&hl=en&sa=X&d=1123822055454567742&ei=GdHfZ7KCJcmpieoPrOWNiQo&scisig=AFWwaeYYt1pzDFX5xglv8ko1phUl&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Bach Le", "1 new citation to articles by Hong Jin Kang"]}
{"title": "HCAST: Human-Calibrated Autonomy Software Tasks", "first_label": [], "second_label": [], "data": "D Rein, J Becker, A Deng, S Nix\nTo understand and predict the societal impacts of highly autonomous AI systems, we \nneed benchmarks with grounding, ie, metrics that directly connect AI performance to \nreal-world effects we care about. We present HCAST (Human-Calibrated Autonomy \nSoftware Tasks), a benchmark of 189 machine learning engineering, cybersecurity, \nsoftware engineering, and general reasoning tasks. We collect 563 human baselines \n(totaling over 1500 hours) from people skilled in these domains, working under\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://metr.org/hcast.pdf&hl=en&sa=X&d=11373862575171555582&ei=GdHfZ9yQMtmlieoPu-rnmA4&scisig=AFWwaebj9NOv1mCCTcQYzOyjwh4s&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Carlos E. Jimenez"]}
{"title": "Neurosymbolic Program Synthesis", "first_label": [], "second_label": [], "data": "S Chaudhuri\\xc2\\xa0- Handbook on Neurosymbolic AI and Knowledge\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe survey neurosymbolic program synthesis, an emerging research area at the \ninterface of deep learning and symbolic artificial intelligence. As in classical machine \nlearning, the goal in neurosymbolic program synthesis is to learn functions from data\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ebooks.iospress.nl/volumearticle/72308&hl=vi&sa=X&d=260497855773459396&ei=GdHfZ8TlMPCj6rQP3ebg8QI&scisig=AFWwaeYnF_PmWIIOWHmepZZkoEVq&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "X Liu, S Liang, M Han, Y Luo, A Liu, X Cai, Z He, D Tao\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nGenerative large language models are crucial in natural language processing, but \nthey are vulnerable to backdoor attacks, where subtle triggers compromise their \nbehavior. Although backdoor attacks against LLMs are constantly emerging, existing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18511&hl=en&sa=X&d=2326898847250844616&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeYsxLaPZ35g7ZwCcXLowDSz&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models", "first_label": [], "second_label": [], "data": "X Liu, X Jia, Y Xun, H Zhang, X Cao\\xc2\\xa0- arXiv preprint arXiv:2502.16167, 2025\nDiffusion models (DMs) have revolutionized data generation, particularly in text-to-\nimage (T2I) synthesis. However, the widespread use of personalized generative \nmodels raises significant concerns regarding privacy violations and copyright\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2502.16167&hl=en&sa=X&d=4329161943110226749&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeafrJsvZfvDQ-TQGz6g7XVq&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration", "first_label": ["Large Language Models"], "second_label": [], "data": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) are increasingly leveraged as foundational \nbackbones in the development of advanced recommender systems, offering \nenhanced capabilities through their extensive knowledge and reasoning. Existing llm\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14735&hl=en&sa=X&d=426769658566315280&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeYh4wl6YqJyh-StysItru7m&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "ECCOS: Efficient Capability and Cost Coordinated Scheduling for Multi-LLM Serving", "first_label": ["Large Language Models"], "second_label": [], "data": "K Mei, W Xu, S Lin, Y Zhang\\xc2\\xa0- arXiv preprint arXiv:2502.20576, 2025\nAs large language models (LLMs) are increasingly deployed as service endpoints in \nsystems, the surge in query volume creates significant scheduling challenges. \nExisting scheduling frameworks mainly target at latency optimization while\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20576&hl=en&sa=X&d=6964741309843660616&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeY5lGWkGMncsxfNykK6Rig1&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Steca: Step-level trajectory calibration for llm agent learning", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "H Wang, J Wang, CT Leong, W Li\\xc2\\xa0- arXiv preprint arXiv:2502.14276, 2025\nLarge language model (LLM)-based agents have shown promise in tackling complex \ntasks by interacting dynamically with the environment. Existing work primarily \nfocuses on behavior cloning from expert demonstrations and preference learning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14276&hl=en&sa=X&d=2705428630298436&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeajtxhd1DlqlX1x2usJD25c&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Test-Time Backdoor Detection for Object Detection Models", "first_label": [], "second_label": ["Detection"], "data": "H Zhang, Y Wang, S Yan, C Zhu, Z Zhou, L Hou, S Hu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nObject detection models are vulnerable to backdoor attacks, where attackers poison \na small subset of training samples by embedding a predefined trigger to manipulate \nprediction. Detecting poisoned samples (ie, those containing triggers) at test time can\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15293&hl=en&sa=X&d=6052108469819470987&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeaFXqT9l6pRpWrFFtCi-fSJ&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Llm as a broken telephone: Iterative generation distorts information", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "A Mohamed, M Geng, M Vazirgiannis, G Shang\\xc2\\xa0- arXiv preprint arXiv:2502.20258, 2025\nAs large language models are increasingly responsible for online content, concerns \narise about the impact of repeatedly processing their own outputs. Inspired by the\" \nbroken telephone\" effect in chained human communication, this study investigates\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.20258%3F&hl=en&sa=X&d=14349451129841775652&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeYpPtaHLlzSyniRMSHyq27c&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models", "first_label": ["Large Language Models"], "second_label": [], "data": "X Zhu, H Xue, Z Zhao, W Xu, J Huang, M Guo, Q Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nText-Attributed Graphs (TAGs), where each node is associated with text descriptions, \nare ubiquitous in real-world scenarios. They typically exhibit distinctive structure and \ndomain-specific knowledge, motivating the development of a Graph Foundation\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.03313&hl=en&sa=X&d=3379316778714972074&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeYnEzzKb30nPQSDWnq341vD&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Serving models, fast and slow: optimizing heterogeneous LLM inferencing workloads at scale", "first_label": ["Large Language Models"], "second_label": [], "data": "S Jaiswal, K Jain, Y Simmhan, A Parayil, A Mallick\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Model (LLM) inference workloads handled by global cloud \nproviders can include both latency-sensitive and insensitive tasks, creating a diverse \nrange of Service Level Agreement (SLA) requirements. Managing these mixed\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14617&hl=en&sa=X&d=6366170882212700007&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeZDqEMNNzq5TdboiBFtoaTr&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models", "first_label": ["Large Language Models"], "second_label": [], "data": "M Rajeev, R Ramamurthy, P Trivedi, V Yadav\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe investigate the robustness of reasoning models trained for step-by-step problem \nsolving by introducing query-agnostic adversarial triggers-short, irrelevant text that, \nwhen appended to math problems, systematically mislead models to output incorrect\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01781&hl=en&sa=X&d=134661470153268257&ei=GdHfZ4-_HpyV6rQP4Zul2Qo&scisig=AFWwaeb1VHCvSYtOTrXSeH2Vu4Gc&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "AdVul: Adversarial Attack against ML-based Vulnerability Detection", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "M Katoh, W Pei, Y Xie\\xc2\\xa0- 2024 Annual Computer Security Applications\\xc2\\xa0\\xe2\\x80\\xa6, 2024\nA fundamental problem in software security, detecting software vulnerabilities (ie \nvulnerabilities), has achieved tremendous progress by leveraging machine learning \n(ML) techniques. These ML-based approaches provide an efficient solution for \nidentifying vulnerabilities and have demonstrated high detection accuracy in recent \nstudies. However, it remains unclear to what extent ML-based vulnerability detection \nis vulnerable to widespread adversarial attacks. In this paper, we propose a\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLinevul: A transformer-based line-level vulnerability prediction\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10917677/&hl=en&sa=X&d=14605241791166743568&ei=GdHfZ66uHNmlieoPu-rnmA4&scisig=AFWwaeYqoVC72txG9NyOwT2McVra&oi=scholaralrt&hist=apJ4fD8AAAAJ:4465730527138788254:AFWwaebhnVuF-27TSh32-dm_KGTR&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Michael Fu"]}

{"title": "Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution", "first_label": ["Code"], "second_label": ["Agent"], "data": "Z Chen, W Ma, L Jiang\\xc2\\xa0- arXiv preprint arXiv:2503.12374, 2025\nAI-driven software development has rapidly advanced with the emergence of \nsoftware development agents that leverage large language models (LLMs) to tackle \ncomplex, repository-level software engineering tasks. These agents go beyond just \ngeneration of final code; they engage in multi-step reasoning, utilize various tools for \ncode modification and debugging, and interact with execution environments to \ndiagnose and iteratively resolve issues. However, most existing evaluations focus\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaTowards Effective Static Type-Error Detection for Python\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12374&hl=vi&sa=X&d=7333533073848752907&ei=M_fcZ5ShL5yV6rQP4Zul2Qo&scisig=AFWwaebG1q3tZOuQdcgNuJjFsFrm&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit", "ref": ["2 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh", "2 new citations to articles by Thanh Le-Cong", "4 new citations to articles by Bach Le", "5 new citations to articles by Carlos E. Jimenez"]}
{"title": "PredicateFix: Repairing Static Analysis Alerts with Bridging Predicates", "first_label": [], "second_label": ["Repair"], "data": "YA Xiao, W Wang, D Liu, J Zhou, S Cheng, Y Xiong\\xc2\\xa0- arXiv preprint arXiv:2503.12205, 2025\nUsing Large Language Models (LLMs) to fix static analysis alerts in program code is \nbecoming increasingly popular and helpful. However, these models often have the \nproblem of hallucination and perform poorly for complex and less common alerts, \nlimiting their performance. Retrieval-augmented generation (RAG) aims to solve this \nproblem by providing the model with a relevant example, but the unsatisfactory \nquality of such examples challenges the effectiveness of existing approaches. To\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaNpex: Repairing java null pointer exceptions without tests\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng l\\xe1\\xbb\\x9di tr\\xc3\\xadch d\\xe1\\xba\\xabn m\\xe1\\xbb\\x9bi trong c\\xc3\\xa1c b\\xc3\\xa0i vi\\xe1\\xba\\xbft c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12205&hl=vi&sa=X&d=9814853145614152441&ei=M_fcZ5ShL5yV6rQP4Zul2Qo&scisig=AFWwaeaZlAZAk5rQVtKdRXzIMFci&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=1&folt=cit", "ref": ["2 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Hakjoo Oh", "10 new citations to articles by Abhik Roychoudhury", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU", "Thanh Le-Cong - new related research", "4 new citations to articles by Bach Le"]}
{"title": "Vulnerability Detection: From Formal Verification to Large Language Models and Hybrid Approaches: A Comprehensive Overview", "first_label": ["Vulnerabilities", "Verification", "Large Language Models"], "second_label": ["Detection"], "data": "N Tihanyi, T Bisztray, MA Ferrag, B Cherif\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSoftware testing and verification are critical for ensuring the reliability and security of \nmodern software systems. Traditionally, formal verification techniques, such as \nmodel checking and theorem proving, have provided rigorous frameworks for \ndetecting bugs and vulnerabilities. However, these methods often face scalability \nchallenges when applied to complex, real-world programs. Recently, the advent of \nLarge Language Models (LLMs) has introduced a new paradigm for software\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAI Software Engineer: Programming with Trust\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10784&hl=en&sa=X&d=12911561823202945144&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeYhatByJp-efqzH8QuYSI72&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=0&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "A Framework for Evaluating Emerging Cyberattack Capabilities of AI", "first_label": [], "second_label": [], "data": "M Rodriguez, RA Popa, F Flynn, L Liang, A Dafoe\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs frontier models become more capable, the community has attempted to evaluate \ntheir ability to enable cyberattacks. Performing a comprehensive evaluation and \nprioritizing defenses are crucial tasks in preparing for AGI safely. However, current \ncyber evaluation efforts are ad-hoc, with no systematic reasoning about the various \nphases of attacks, and do not provide a steer on how to use targeted defenses. In this \nwork, we propose a novel approach to AI cyber capability evaluation that (1)\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSpecRover: Code Intent Extraction via LLMs\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11917&hl=en&sa=X&d=11886055875426931838&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeZBMz0XMoInjmODEmeqEFEw&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=1&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "5 new citations to articles by Carlos E. Jimenez"]}
{"title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification", "first_label": [], "second_label": [], "data": "Y Zhang, T Liu, Z Zhao, G Meng, K Chen\\xc2\\xa0- arXiv preprint arXiv:2503.11185, 2025\nLarge Language Models (LLMs) are vulnerable to jailbreak attacks, which use \ncrafted prompts to elicit toxic responses. These attacks exploit LLMs' difficulty in \ndynamically detecting harmful intents during the generation process. Traditional \nsafety alignment methods, often relying on the initial few generation steps, are \nineffective due to limited computational budget. This paper proposes DEEPALIGN, a \nrobust defense framework that fine-tunes LLMs to progressively detoxify generated\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11185&hl=en&sa=X&d=14453944055765695131&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeYK4S-foEBtClRihc80_UW5&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=3&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Empirical Computation", "first_label": [], "second_label": [], "data": "E Tang, M B\\xc3\\xb6hme\\xc2\\xa0- arXiv preprint arXiv:2503.10954, 2025\nIn this vision paper, we explore the challenges and opportunities of a form of \ncomputation that employs an empirical (rather than a formal) approach, where the \nsolution of a computational problem is returned as empirically most likely (rather than \nnecessarily correct). We call this approach as* empirical computation* and observe \nthat its capabilities and limits* cannot* be understood within the classic, rationalist \nframework of computation. While we take a very broad view of\" computational\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10954&hl=en&sa=X&d=14815585654542542537&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeaCOXHVL1sgsM6_lGxfuPw_&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=4&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Towards LLM-Assisted Vulnerability Detection and Repair for Open-Source 5G UE Implementations", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection", "Repair"], "data": "R Patir, Q Huang, K Guo, W Guo, G Gu, H Cai, H Hu\nThe rapid evolution of software systems in 5G networks has heightened the need for \nrobust security measures. Traditional code analysis methods often fail to detect \nvulnerabilities specific to 5G, particularly vulnerabilities stemming from complex \nprotocol interactions. In this work, we explore the potential of LLM-assisted \ntechniques in vulnerability detection and repair in open-source 5G implementations. \nWe introduce a novel framework leveraging Chain-of-Thought (CoT) prompting in\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/futureg25-21.pdf&hl=en&sa=X&d=8627900248790396003&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaebouaCyEnSYoq1GfzzQLbvK&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=5&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "Hong Jin Kang - new related research", "Triet H. M. Le - new related research"]}
{"title": "Unlimited Practice Opportunities: Automated Generation of Comprehensive, Personalized Programming Tasks", "first_label": [], "second_label": ["Generation"], "data": "S Jacobs, H Peters, S Jaschke, N Kiesler\\xc2\\xa0- arXiv preprint arXiv:2503.11704, 2025\nGenerative artificial intelligence (GenAI) offers new possibilities for generating \npersonalized programming exercises, addressing the need for individual practice. \nHowever, the task quality along with the student perspective on such generated tasks \nremains largely unexplored. Therefore, this paper introduces and evaluates a new \nfeature of the so-called Tutor Kai for generating comprehensive programming tasks, \nincluding problem descriptions, code skeletons, unit tests, and model solutions. The\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSynthesizing Tasks for Block-based Programming\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11704&hl=en&sa=X&d=15238272178281079298&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeaeY1DFfL_5xyyMN0w_8yEm&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=6&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "FuzzGPT: \\xed\\x85\\x8c\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xec\\xbc\\x80\\xec\\x9d\\xb4\\xec\\x8a\\xa4 \\xec\\xb5\\x9c\\xec\\xa0\\x81\\xed\\x99\\x94\\xeb\\xa5\\xbc \\xec\\x9c\\x84\\xed\\x95\\x9c LLM \\xea\\xb8\\xb0\\xeb\\xb0\\x98 \\xed\\x94\\x84\\xeb\\xa1\\xac\\xed\\x94\\x84\\xed\\x8a\\xb8 \\xec\\x97\\x94\\xec\\xa7\\x80\\xeb\\x8b\\x88\\xec\\x96\\xb4\\xeb\\xa7\\x81 \\xed\\x8d\\xbc\\xec\\xa7\\x95 \\xea\\xb8\\xb0\\xeb\\xb2\\x95", "first_label": ["Large Language Models", "Fuzzing"], "second_label": [], "data": "\\xea\\xb9\\x80\\xec\\x97\\xb0\\xec\\xa7\\x84\\xef\\xbc\\x8c \\xec\\x9d\\xb4\\xec\\x97\\xb0\\xec\\xa7\\x80\\xef\\xbc\\x8c \\xec\\x9d\\xb4\\xec\\x9d\\xbc\\xea\\xb5\\xac\\xc2\\xa0- \\xec\\xa0\\x95\\xeb\\xb3\\xb4\\xeb\\xb3\\xb4\\xed\\x98\\xb8\\xed\\x95\\x99\\xed\\x9a\\x8c\\xeb\\x85\\xbc\\xeb\\xac\\xb8\\xec\\xa7\\x80, 2024\n\\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4 \\xec\\xa0\\x95\\xec\\x9d\\x98 \\xec\\x84\\x9c\\xeb\\xb9\\x84\\xec\\x8a\\xa4 (Software Defined Service) \\xed\\x99\\x98\\xea\\xb2\\xbd\\xec\\x9d\\xb4 \\xec\\xa0\\x84 \\xec\\x82\\xb0\\xec\\x97\\x85\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x99\\x95\\xeb\\x8c\\x80\\xeb\\x90\\xa8\\xec\\x97\\x90 \\xeb\\x94\\xb0\\xeb\\x9d\\xbc \n\\xec\\x86\\x8c\\xed\\x94\\x84\\xed\\x8a\\xb8\\xec\\x9b\\xa8\\xec\\x96\\xb4\\xec\\x9d\\x98 \\xec\\xb7\\xa8\\xec\\x95\\xbd\\xec\\xa0\\x90\\xec\\x9d\\x84 \\xed\\x9a\\xa8\\xec\\x9c\\xa8\\xec\\xa0\\x81\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x83\\x90\\xec\\xa7\\x80\\xed\\x95\\xa0 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xea\\xb8\\xb0\\xec\\x88\\xa0\\xec\\x97\\x90 \\xeb\\x8c\\x80\\xed\\x95\\x9c \\xec\\x88\\x98\\xec\\x9a\\x94\\xea\\xb0\\x80 \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xec\\x9e\\x88\\xeb\\x8b\\xa4. \n\\xec\\xa2\\x85\\xeb\\x9e\\x98\\xec\\x9d\\x98 \\xeb\\xb2\\x94\\xec\\x9a\\xa9 \\xeb\\xac\\xb4\\xec\\x9e\\x91\\xec\\x9c\\x84 \\xed\\x8d\\xbc\\xec\\xa7\\x95 \\xea\\xb8\\xb0\\xeb\\xb2\\x95\\xeb\\x93\\xa4\\xec\\x9d\\x80 \\xed\\x83\\x90\\xec\\xa7\\x80 \\xec\\x84\\xb1\\xeb\\x8a\\xa5\\xea\\xb3\\xbc \\xec\\xbb\\xa4\\xeb\\xb2\\x84\\xeb\\xa6\\xac\\xec\\xa7\\x80\\xea\\xb0\\x80 \\xec\\x9a\\xb0\\xec\\x88\\x98\\xed\\x95\\x98\\xec\\xa7\\x80\\xeb\\xa7\\x8c, \\xed\\x8d\\xbc\\xec\\xa7\\x95 \\xeb\\x8c\\x80\\xec\\x83\\x81 \n\\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\xa8\\xec\\x9d\\xb4 \\xeb\\xb3\\xb5\\xec\\x9e\\xa1\\xed\\x95\\xa0\\xec\\x88\\x98\\xeb\\xa1\\x9d \\xec\\x9d\\x98\\xeb\\xaf\\xb8 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xed\\x85\\x8c\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xec\\xbc\\x80\\xec\\x9d\\xb4\\xec\\x8a\\xa4\\xeb\\xa5\\xbc \\xec\\x83\\x9d\\xec\\x84\\xb1\\xed\\x95\\x98\\xea\\xb8\\xb0 \\xec\\x96\\xb4\\xeb\\xa0\\xb5\\xea\\xb3\\xa0 \\xec\\x98\\xa4\\xeb\\xb2\\x84\\xed\\x97\\xa4\\xeb\\x93\\x9c\\xea\\xb0\\x80 \n\\xec\\xa6\\x9d\\xea\\xb0\\x80\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xed\\x95\\x9c\\xea\\xb3\\x84\\xec\\xa0\\x90\\xec\\x9d\\xb4 \\xec\\x9e\\x88\\xeb\\x8b\\xa4. \\xeb\\x94\\xb0\\xeb\\x9d\\xbc\\xec\\x84\\x9c \\xeb\\xb3\\xb8 \\xeb\\x85\\xbc\\xeb\\xac\\xb8\\xec\\x97\\x90\\xec\\x84\\x9c\\xeb\\x8a\\x94 LLM \\xeb\\xaa\\xa8\\xeb\\x8d\\xb8\\xec\\x9d\\x84 \\xeb\\x8c\\x80\\xec\\x83\\x81\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x95\\x9c \\xed\\x94\\x84\\xeb\\xa1\\xac\\xed\\x94\\x84\\xed\\x8a\\xb8 \n\\xec\\x97\\x94\\xec\\xa7\\x80\\xeb\\x8b\\x88\\xec\\x96\\xb4\\xeb\\xa7\\x81 \\xea\\xb8\\xb0\\xeb\\xb2\\x95\\xec\\x9d\\x84 \\xed\\x99\\x9c\\xec\\x9a\\xa9\\xed\\x95\\x98\\xec\\x97\\xac \\xed\\x8d\\xbc\\xec\\xa7\\x95 \\xeb\\x8c\\x80\\xec\\x83\\x81 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\xa8\\xec\\x9d\\x98 \\xeb\\xac\\xb8\\xeb\\xa7\\xa5\\xea\\xb3\\xbc \\xea\\xb5\\xac\\xec\\xa1\\xb0\\xeb\\xa5\\xbc \\xed\\x8c\\x8c\\xec\\x95\\x85\\xed\\x95\\x98\\xec\\x97\\xac \\xec\\xb5\\x9c\\xec\\xa0\\x81\\xec\\x9d\\x98\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaFuzzing: Challenges and Reflections\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.dbpia.co.kr/Journal/articleDetail%3FnodeId%3DNODE12014735&hl=en&sa=X&d=7560579376064860739&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeZNeoSpNFgiBMkGOyui6GMJ&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=7&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Repair", "Generation"], "data": "J Gu, A Aleti, C Chen, H Zhang\\xc2\\xa0- arXiv preprint arXiv:2503.12899, 2025\nLanguage Models (LMs) are widely used in software engineering for code \ngeneration, but they may produce code with errors. Rather than repairing the \ngenerated code, an alternative way is to address the underlying failures of models. \nLM repair offers a lightweight solution to this challenge: it requires minimal data, \nreduces computational costs, and reduces the side effects. Unlike retraining, LM \nrepair focuses on applying tailored updates to targeted neurons, making it ideal for\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBaishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12899&hl=en&sa=X&d=5419242342984574836&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaeaLvcPSvoavjMM_JsZ7yzxW&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=8&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury"]}
{"title": "Uncovering the Challenges: A Study of Corner Cases in Bug-Inducing Commits", "first_label": ["Bug"], "second_label": [], "data": "A Serifoglu, E T\\xc3\\xbcz\\xc3\\xbcn\nIn software development, accurately identifying buginducing commits (BICs) is \ncrucial for maintaining code integrity and ensuring the reliability of software systems. \nThe complexities involved in pinpointing the exact commits responsible for bugs \nnecessitate a thorough investigation of the underlying issues and limitations of \nexisting tools and algorithms. This study investigates and identifies corner cases in \nBIC identification, clarifying definitions and examining issues with existing algorithms\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRegression tests to expose change interaction errors\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Atakan-Serifoglu/publication/389875422_Uncovering_the_Challenges_A_Study_of_Corner_Cases_in_Bug-Inducing_Commits/links/67d57419e62c604a0dda8963/Uncovering-the-Challenges-A-Study-of-Corner-Cases-in-Bug-Inducing-Commits.pdf&hl=en&sa=X&d=5815485862562378662&ei=M_fcZ5_7H_Cj6rQP3ebg8QI&scisig=AFWwaebAa-Rpm-6e-Bferr_bkpHH&oi=scholaralrt&hist=apJ4fD8AAAAJ:5778505219825515303:AFWwaeaDDOggOneW-z6K3HLjAzuP&html=&pos=9&folt=cit", "ref": ["10 new citations to articles by Abhik Roychoudhury", "Hong Jin Kang - new related research"]}
{"title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "P Zhu, Z Zhou, Y Zhang, S Yan, K Wang, S Su\\xc2\\xa0- arXiv preprint arXiv:2502.12575, 2025\nAs LLM-based agents become increasingly prevalent, backdoors can be implanted \ninto agents through user queries or environment feedback, raising critical concerns \nregarding safety vulnerabilities. However, backdoor attacks are typically detectable\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.12575&hl=en&sa=X&d=12812358845767902297&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaebpIXuKdKLwllisDlZhAq9F&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models", "first_label": ["Large Language Models"], "second_label": [], "data": "X Liu, S Liang, M Han, Y Luo, A Liu, X Cai, Z He, D Tao\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nGenerative large language models are crucial in natural language processing, but \nthey are vulnerable to backdoor attacks, where subtle triggers compromise their \nbehavior. Although backdoor attacks against LLMs are constantly emerging, existing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18511&hl=en&sa=X&d=2326898847250844616&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeYsxLaPZ35g7ZwCcXLowDSz&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Mitigating backdoor attacks in Federated Learning based intrusion detection systems through Neuron Synaptic Weight Adjustment", "first_label": [], "second_label": ["Detection"], "data": "U Zukaib, X Cui\\xc2\\xa0- Knowledge-Based Systems, 2025\nFederated Learning has emerged as a transformative paradigm that enables \ncollaborative model training across distributed clients while preserving data privacy. \nHowever, Federated Learning systems are vulnerable to backdoor attacks, where\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S095070512500214X&hl=en&sa=X&d=3344237733639979278&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeZiRQp24mPLFQoWAjl8T3ii&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=2&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Detecting All-to-One Backdoor Attacks in Black-Box DNNs via Differential Robustness to Noise", "first_label": [], "second_label": ["Detection"], "data": "H Fu, P Krishnamurthy, S Garg, F Khorrami\\xc2\\xa0- IEEE Access, 2025\nThe all-to-one (A2O) backdoor attack is one of the major adversarial threats against \nneural networks. Most existing A2O backdoor defenses operate in a white-box \ncontext, necessitating access to the backdoored model's architecture, hidden layer\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10891759.pdf&hl=en&sa=X&d=603250187486282372&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeZMdrX5MQAVKOvpPQnkoMkC&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Augmented Adversarial Trigger Learning", "first_label": [], "second_label": [], "data": "Z Wang, Y Qi\\xc2\\xa0- arXiv preprint arXiv:2503.12339, 2025\nGradient optimization-based adversarial attack methods automate the learning of \nadversarial triggers to generate jailbreak prompts or leak system prompts. In this \nwork, we take a closer look at the optimization objective of adversarial trigger\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12339&hl=en&sa=X&d=15043271486492304090&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeb9w0mxzOye4eVGDyGfVdWR&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks", "first_label": [], "second_label": [], "data": "C Lin, C Zhao, S Wang, L Wang, C Shen, Z Zhao\\xc2\\xa0- arXiv preprint arXiv:2503.12058, 2025\nBackdoor attacks typically place a specific trigger on certain training data, such that \nthe model makes prediction errors on inputs with that trigger during inference. \nDespite the core role of the trigger, existing studies have commonly believed a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12058&hl=en&sa=X&d=11030455453381978108&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaebYpqAYdG2Hb6njlpBzGvMY&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models", "first_label": [], "second_label": [], "data": "X Liu, X Jia, Y Xun, H Zhang, X Cao\\xc2\\xa0- arXiv preprint arXiv:2502.16167, 2025\nDiffusion models (DMs) have revolutionized data generation, particularly in text-to-\nimage (T2I) synthesis. However, the widespread use of personalized generative \nmodels raises significant concerns regarding privacy violations and copyright\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2502.16167&hl=en&sa=X&d=4329161943110226749&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeafrJsvZfvDQ-TQGz6g7XVq&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration", "first_label": ["Large Language Models"], "second_label": [], "data": "M Hong, Y Xia, Z Wang, J Zhu, Y Wang, S Cai, X Yang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) are increasingly leveraged as foundational \nbackbones in the development of advanced recommender systems, offering \nenhanced capabilities through their extensive knowledge and reasoning. Existing llm\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14735&hl=en&sa=X&d=426769658566315280&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeYh4wl6YqJyh-StysItru7m&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "first_label": [], "second_label": [], "data": "M D'Inc\\xc3\\xa0, E Peruzzo, X Xu, H Shi, N Sebe, M Mancini\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nVision-language models (VLMs) often inherit the biases and unsafe associations \npresent within their large-scale training dataset. While recent approaches mitigate \nunsafe behaviors, their evaluation focuses on how safe the model is on unsafe\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11742&hl=en&sa=X&d=8746883662886027280&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaeZDBalTyRdSBcXsc1cd7qjG&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research", "Carlos E. Jimenez - new related research"]}
{"title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models", "first_label": ["Large Language Models"], "second_label": [], "data": "CV Kumar, A Urlana, G Kanumolu, BM Garlapati\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAdvancements in Large Language Models (LLMs) have increased the performance \nof different natural language understanding as well as generation tasks. Although \nLLMs have breached the state-of-the-art performance in various tasks, they often\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11985&hl=en&sa=X&d=11387802359555301241&ei=M_fcZ-WkHpuw6rQP15iV4Qw&scisig=AFWwaebbpG-1AKw6ti9uIaO5xgtj&oi=scholaralrt&hist=apJ4fD8AAAAJ:4513401344136555010:AFWwaea8pA4W9ESmXpw9yvMxc7-7&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Quality In, Quality Out: Investigating Training Data's Role in AI Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "C Improta, R Tufano, P Liguori, D Cotroneo, G Bavota\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDeep Learning-based code generators have seen significant advancements in \nrecent years. Tools such as GitHub Copilot are used by thousands of developers with \nthe main promise of a boost in productivity. However, researchers have recently \nquestioned their impact on code quality showing, for example, that code generated \nby DL-based tools may be affected by security vulnerabilities. Since DL models are \ntrained on large code corpora, one may conjecture that low-quality code they output\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRefining chatgpt-generated code: Characterizing and mitigating\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11402&hl=en&sa=X&d=7171030817651579681&ei=M_fcZ6PPGdSyieoPwLeW8AY&scisig=AFWwaeauPprv9R4l_2zYKxGmunTA&oi=scholaralrt&hist=apJ4fD8AAAAJ:1878193813677419122:AFWwaebnAK6dY8A06r0yyM87AWUg&html=&pos=0&folt=cit", "ref": ["2 new citations to articles by Thanh Le-Cong", "4 new citations to articles by Bach Le", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Eliminating Backdoors in Neural Code Models for Secure Code Understanding", "first_label": ["Code"], "second_label": [], "data": "W SUN, Y CHEN, C FANG, Y FENG, Y XIAO, AN GUO\\xe2\\x80\\xa6 - 2025\nIn essence, the nature and architecture of NCMs are also deep neural networks, so \nthey also inherit the vulnerability of neural networks. In recent years, the security of \nNCMs has gained traction in SE, artificial intelligence (AI), and security communities. \nSeveral existing works [2, 13, 31, 33, 41, 49] have revealed that NCMs are \nvulnerable to a security threat called backdoor attacks. Such attacks, also called \ntrojan attacks [19], aim to inject a backdoor pattern into the learned model with the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaStealthy backdoor attack for code models\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://wssun.github.io/papers/2025-FSE-EliBadCode.pdf&hl=en&sa=X&d=14118212492887768466&ei=M_fcZ7z4KeOO6rQP_I3iiA0&scisig=AFWwaeaZRkuriytODFXVLOFpLYu8&oi=scholaralrt&hist=apJ4fD8AAAAJ:11486195984023826531:AFWwaebYo-fw1j0PJswL-CdomZqY&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Hong Jin Kang", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "PLM: Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing", "first_label": [], "second_label": [], "data": "C Deng, L Sun, J Jiang, Y Zeng, X Wu, W Zhao, Q Xiao\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWhile scaling laws have been continuously validated in large language models \n(LLMs) with increasing model parameters, the inherent tension between the \ninference demands of LLMs and the limited resources of edge devices poses a \ncritical challenge to the development of edge intelligence. Recently, numerous small \nlanguage models have emerged, aiming to distill the capabilities of LLMs into \nsmaller footprints. However, these models often retain the fundamental architectural\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nTr\\xc3\\xadch d\\xe1\\xba\\xabn: \\xe2\\x80\\xaaCodeultrafeedback: An llm-as-a-judge dataset for aligning large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12167&hl=vi&sa=X&d=1000611124414137436&ei=M_fcZ7_cLbutieoP9Om12Qc&scisig=AFWwaeYw3rdCGchOFAE4B54qD4hl&oi=scholaralrt&hist=apJ4fD8AAAAJ:11724652424841979500:AFWwaeb06hHZ-3j7Bb1sOMTsP9ed&html=&pos=1&folt=cit", "ref": ["3 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Xin ZHOU"]}
{"title": "Mechanistic Understanding of Language Models in Syntactic Code Completion", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Miller, D Rai, Z Yao\\xc2\\xa0- arXiv preprint arXiv:2502.18499, 2025\nRecently, language models (LMs) have shown impressive proficiency in code \ngeneration tasks, especially when fine-tuned on code-specific datasets, commonly \nknown as Code LMs. However, our understanding of the internal decision-making\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18499&hl=en&sa=X&d=16766167717063467915&ei=M_fcZ9yjK5m7ieoP6cWmgQE&scisig=AFWwaeZXFo3QPd08t686n6v36dI4&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research", "Michael Fu - new related research", "Bach Le - new related research"]}
{"title": "Bugfix: a standard language, database schema and repository for research on bugs and automatic program repair", "first_label": ["Automated Program Repair", "Bug"], "second_label": ["Repair"], "data": "V Kananchuk, I Mustafin, B Meyer\\xc2\\xa0- arXiv preprint arXiv:2502.15599, 2025\nAutomatic Program Repair (APR) is a brilliant idea: when detecting a bug, also \nprovide suggestions for correcting the program. Progress towards that goal is \nhindered by the absence of a common frame of reference for the multiplicity of APR\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15599&hl=en&sa=X&d=10642094292575353027&ei=M_fcZ9yjK5m7ieoP6cWmgQE&scisig=AFWwaebYZkmLIso9Pt-1zoBNSzF7&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research", "Michael Fu - new related research", "Bach Le - new related research"]}
{"title": "Pragmatic Reasoning improves LLM Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Cao, S Apel, A Singla, V Demberg\\xc2\\xa0- arXiv preprint arXiv:2502.15835, 2025\nLarge Language Models (LLMs) have demonstrated impressive potential in \ntranslating natural language (NL) instructions into program code. However, user \ninstructions often contain inherent ambiguities, making it challenging for LLMs to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.15835&hl=en&sa=X&d=6927995321596801340&ei=M_fcZ9yjK5m7ieoP6cWmgQE&scisig=AFWwaebpCZ-l4eJjTQ3XaxQi2c08&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=3&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Q Zhao, L Zhang, F Liu, X Lian, Q Meng, Z Jiao, Z Zhou\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation is a latency-sensitive task that demands high timeliness, but the \nautoregressive decoding mechanism of Large Language Models (LLMs) leads to \npoor inference efficiency. Existing LLM inference acceleration methods mainly focus\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.17139&hl=en&sa=X&d=12309021167084800557&ei=M_fcZ9yjK5m7ieoP6cWmgQE&scisig=AFWwaebM1YnN4NurSD2BsoIiHS26&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=4&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Teamwork makes the dream work: LLMs-Based Agents for GitHub README. MD Summarization", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "DSH Nguyen, BG Truong, PT Nguyen, J Di Rocco\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe proliferation of Large Language Models (LLMs) in recent years has realized \nmany applications in various domains. Being trained with a huge of amount of data \ncoming from various sources, LLMs can be deployed to solve different tasks\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10876&hl=en&sa=X&d=17853566658424395822&ei=M_fcZ9yjK5m7ieoP6cWmgQE&scisig=AFWwaeb2__fKinr9h1T631iZ54op&oi=scholaralrt&hist=apJ4fD8AAAAJ:11631047573362457156:AFWwaeYhbBKL65h4pzyKCNru3s-R&html=&pos=5&folt=rel", "ref": ["Thanh Le-Cong - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Exploit"], "data": "Y Luo, H Zhou, M Zhang, D De La Rosa, H Ahmed\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs an emerging programming language, Rust has rapidly gained popularity and \nrecognition among developers due to its strong emphasis on safety. It employs a \nunique ownership system and safe concurrency practices to ensure robust safety. \nDespite these safeguards, security in Rust still presents challenges. Since 2018, 442 \nRust-related vulnerabilities have been reported in real-world applications. The \nlimited availability of data has resulted in existing vulnerability detection tools\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLinevul: A transformer-based line-level vulnerability prediction\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nMichael Fu\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10793&hl=en&sa=X&d=5829414047755650757&ei=M_fcZ6XmHNmlieoPu-rnmA4&scisig=AFWwaeY9qSDL2ktzIVFeZhrU-G2d&oi=scholaralrt&hist=apJ4fD8AAAAJ:4465730527138788254:AFWwaebhnVuF-27TSh32-dm_KGTR&html=&pos=0&folt=cit", "ref": ["1 new citation to articles by Michael Fu", "Hong Jin Kang - new related research", "Triet H. M. Le - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Smart Contracts and Decentralized Applications", "first_label": ["Smart Contracts"], "second_label": [], "data": "A Agnihotri, A Pandey, R Verma, N Dhanda\\xc2\\xa0- The Confluence of Cryptography\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe digital era has ushered in a vast landscape of opportunities for content creation \nand distribution. However, it has also brought forth challenges in safeguarding \nintellectual property rights. In this chapter, we embark on an exploration of the \ntransformative interplay between smart contracts, decentralized applications \n(DApps), and Video Digital Rights Management (DRM), unraveling their \ncollaborative potential in redefining content protection paradigms. The advent of the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSmart contract development: Challenges and opportunities\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DxOVOEQAAQBAJ%26oi%3Dfnd%26pg%3DPA164%26ots%3DS8alV97fvs%26sig%3DlUN2wWudezz16wwMQXCB5JqnfXQ&hl=en&sa=X&d=9630678649507647070&ei=M_fcZ_yAJYC96rQPjOH0iAk&scisig=AFWwaeZh8I7eyenVtwNAp5EoXduD&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=3&folt=cit", "ref": ["4 new citations to articles by Bach Le"]}
{"title": "Lightweight Concolic Testing via Path-Condition Synthesis for Deep Learning Libraries", "first_label": [], "second_label": [], "data": "S Kim, Y Kim, D Park, Y Jeon, J Yi, M Kim\\xc2\\xa0- 2025 IEEE/ACM 47th International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nMany techniques have been recently developed for testing deep learning (DL) \nlibraries. Although these techniques have effectively improved API and code \ncoverage and detected unknown bugs, they rely on blackbox fuzzing for input\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.jooyongyi.com/papers/ICSE25.pdf&hl=en&sa=X&d=10264194252454271239&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaeY1z_V3GkFVvQ4yKrS6-u2H&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation", "first_label": ["Large Language Models", "Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, R Qian, P Lin, Y Liu, C Ying, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2502.18793, 2025\nLarge language models (LLMs) have transformed code generation. However, most \nexisting approaches focus on mainstream languages such as Python and Java, \nneglecting the Solidity language, the predominant programming language for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.18793&hl=en&sa=X&d=18108558928985012184&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaeYB8M09Myp3zooUoYE61pwf&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "KNighter: Transforming Static Analysis with LLM-Synthesized Checkers", "first_label": ["Large Language Models"], "second_label": [], "data": "C Yang, Z Zhao, Z Xie, H Li, L Zhang\\xc2\\xa0- arXiv preprint arXiv:2503.09002, 2025\nStatic analysis is a powerful technique for bug detection in critical systems like \noperating system kernels. However, designing and implementing static analyzers is \nchallenging, time-consuming, and typically limited to predefined bug patterns. While\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.09002&hl=en&sa=X&d=15643934552074292819&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaeb6V7qERn3_LuvEYvObZk8F&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "An Empirical Study on the Relationship Between Defects and Source Code's Unnaturalness", "first_label": ["Code"], "second_label": [], "data": "Y Jiang, H Liu, J Liu, Y Zhang, W Ji, H Zhong, L Zhang\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nNatural languages are \\xe2\\x80\\x9cnatural\\xe2\\x80\\x9d in that texts in natural languages are repetitive and \npredictable. Recent research indicates that programming languages share similar \ncharacteristics (naturalness), with source code displaying patterns of repetition and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3718083&hl=en&sa=X&d=7306116860520413653&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaeYhQXymsslwMKE9PXdq4Ud6&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "A Sentence-Level Approach to Understanding Software Vulnerability Fixes", "first_label": ["Vulnerabilities"], "second_label": [], "data": "A Gao, Z Zhang, S Wang, L Huang, S Wei, V Ng\\xc2\\xa0- arXiv preprint arXiv:2503.10877, 2025\nUnderstanding software vulnerabilities and their resolutions is crucial for securing \nmodern software systems. This study presents a novel traceability model that links a \npair of sentences describing at least one of the three types of semantics (triggers\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10877&hl=en&sa=X&d=6276809591042715782&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaeYpTT8UUKqa9lNMLk_YUULi&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research", "Michael Fu - new related research", "Triet H. M. Le - new related research"]}
{"title": "KotSuite: Unit Test Generation for Kotlin Programs in Android Applications", "first_label": ["Unit Test"], "second_label": ["Generation"], "data": "F Yang, Q Xin, Z Ren, J Xuan\nUnit testing plays a pivotal role in safeguarding functional requirements and \nsupporting the maintainence during the development of Android applications. The \nKotlin programming language emerges in developing Android applications since\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://qixin5.github.io/files/pdf/research/icpc25kotsuite.pdf&hl=en&sa=X&d=4160603672644182246&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaebk9skrRKkb80KaNdrfD3RV&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Design pattern recognition: a study of large language models", "first_label": ["Large Language Models"], "second_label": [], "data": "SK Pandey, S Chand, J Horkoff, M Staron, M Ochodek\\xe2\\x80\\xa6\\xc2\\xa0- Empirical Software\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAbstract Context As Software Engineering (SE) practices evolve due to extensive \nincreases in software size and complexity, the importance of tools to analyze and \nunderstand source code grows significantly. Objective This study aims to evaluate\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10625-1&hl=en&sa=X&d=8546412004218051293&ei=M_fcZ8KtI5uoieoPy6GmoQM&scisig=AFWwaealng_j1iX3a2iIiURF0bTp&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=9&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "LLM Agents for Education: Advances and Applications", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "Z Chu, S Wang, J Xie, T Zhu, Y Yan, J Ye, A Zhong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Model (LLM) agents have demonstrated remarkable capabilities in \nautomating tasks and driving innovation across diverse educational applications. In \nthis survey, we provide a systematic review of state-of-the-art research on LLM \nagents in education, categorizing them into two broad classes:(1)\\\\emph \n{Pedagogical Agents}, which focus on automating complex pedagogical tasks to \nsupport both teachers and students; and (2)\\\\emph {Domain-Specific Educational\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSWE-bench Multimodal: Do AI Systems Generalize to Visual\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11733&hl=en&sa=X&d=4226071890866678953&ei=M_fcZ6rCNNSyieoPwLeW8AY&scisig=AFWwaebvzZb9VMh87-h7qv2Osu2G&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=1&folt=cit", "ref": ["5 new citations to articles by Carlos E. Jimenez"]}
{"title": "A Survey on the Optimization of Large Language Model-based Agents", "first_label": ["Large Language Models"], "second_label": ["Agent"], "data": "S Du, J Zhao, J Shi, Z Xie, X Jiang, Y Bai, L He\\xc2\\xa0- arXiv preprint arXiv:2503.12434, 2025\nWith the rapid development of Large Language Models (LLMs), LLM-based agents \nhave been widely adopted in various fields, becoming essential for autonomous \ndecision-making and interactive tasks. However, current work typically relies on \nprompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to \nlimited effectiveness or suboptimal performance in complex agent-related \nenvironments. Although LLM optimization techniques can improve model\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSWE-bench Multimodal: Do AI Systems Generalize to Visual\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12434&hl=en&sa=X&d=11666181406528668769&ei=M_fcZ6rCNNSyieoPwLeW8AY&scisig=AFWwaeYD9qMA6WvHKWg6eiuJxLck&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=2&folt=cit", "ref": ["5 new citations to articles by Carlos E. Jimenez"]}
{"title": "Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "Z Yu, W Gu, Y Wang, X Jiang, Z Zeng, J Wang, W Ye\\xe2\\x80\\xa6\nAbstract Large Language Models excel at code generation yet struggle with complex \nprogramming tasks that demand sophisticated reasoning. To bridge this gap, \ntraditional process supervision relies on learned reward models requiring costly \ntraining data and suffering from reward misalignment, while outcome supervision \nfails for complex tasks needing coordinated intermediate steps. We introduce \nOutcome Refining Process Supervision, which unifies process and outcome\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSwe-bench: Can language models resolve real-world github issues?\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://zhuohaoyu.github.io/assets/files/orps_icml_preprint.pdf&hl=en&sa=X&d=6355483286590305639&ei=M_fcZ6rCNNSyieoPwLeW8AY&scisig=AFWwaeYZVfrCQExVIUTCK0U7b1z0&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=4&folt=cit", "ref": ["5 new citations to articles by Carlos E. Jimenez"]}
{"title": "The Road to Hybrid Quantum Programs: Characterizing the Evolution from Classical to Hybrid Quantum Software", "first_label": [], "second_label": [], "data": "V De Maio, I Brandic, E Deelman, J Cito\\xc2\\xa0- arXiv preprint arXiv:2503.11450, 2025\nQuantum computing exhibits the unique capability to natively and efficiently encode \nvarious natural phenomena, promising theoretical speedups of several orders of \nmagnitude. However, not all computational tasks can be efficiently executed on\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11450&hl=vi&sa=X&d=17241695614173477576&ei=M_fcZ_7jMsCSieoPqLGJyAM&scisig=AFWwaeZN6wIuDNXh3QSA7Hvt6F9u&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=0&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Chord: Towards a Unified Detection of Blockchain Transaction Parallelism Bugs", "first_label": ["Bug"], "second_label": ["Detection"], "data": "Y Zhou, Z Yan, Y Chen, F Ma, T Chen, Y Jiang\\xc2\\xa0- 2025 IEEE/ACM 47th International\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nBlockchain systems have implemented various transaction parallelism mechanisms \nto improve the system throughput and reduce the latency. However, they inevitably \nintroduce bugs. Such bugs can result in severe consequences such as asset loss\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nGoogle Scholar g\\xe1\\xbb\\xadi th\\xc3\\xb4ng b\\xc3\\xa1o n\\xc3\\xa0y cho b\\xe1\\xba\\xa1n v\\xc3\\xac b\\xe1\\xba\\xa1n \\xc4\\x91ang theo d\\xc3\\xb5i nh\\xe1\\xbb\\xafng b\\xc3\\xa0i vi\\xe1\\xba\\xbft m\\xe1\\xbb\\x9bi li\\xc3\\xaan quan \\xc4\\x91\\xe1\\xba\\xbfn nghi\\xc3\\xaan c\\xe1\\xbb\\xa9u c\\xe1\\xbb\\xa7a \nHakjoo Oh\n.\nLi\\xe1\\xbb\\x87t k\\xc3\\xaa c\\xe1\\xba\\xa3nh b\\xc3\\xa1o\nH\\xe1\\xbb\\xa7y th\\xc3\\xb4ng b\\xc3\\xa1o\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/proceedings-article/icse/2025/056900a742/251mHl8bRbW&hl=vi&sa=X&d=10778375171135274618&ei=M_fcZ_7jMsCSieoPqLGJyAM&scisig=AFWwaeYmoL_7N7eGNDxmjKfxzqNP&oi=scholaralrt&hist=apJ4fD8AAAAJ:16065687014273664109:AFWwaeYpvD7V4gPm0ywHhNT6YvSk&html=&pos=1&folt=rel", "ref": ["Hakjoo Oh - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Evaluating Biased Synthetic Data Effects on Large Language Model-Based Software Vulnerability Detection", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "LB Germano, LQ Vieira, RR Goldschmidt, JC Duarte\\xe2\\x80\\xa6\nSoftware security ensures data privacy and system reliability. Vulnerabilities in the \ndevelopment cycle can lead to privilege escalation, causing data exfiltration or denial \nof service attacks. Static code analyzers, based on predefined rules, often fail to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.scitepress.org/Papers/2025/131568/131568.pdf&hl=en&sa=X&d=2607917738470903508&ei=M_fcZ53oIcuZieoPiNWegAU&scisig=AFWwaeZ0Q2EahjIXC8AC_4u3u9GS&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=0&folt=rel", "ref": ["Michael Fu - new related research", "Triet H. M. Le - new related research", "Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "GPT-Based Automated Induction: Vulnerability Detection in Medical Software", "first_label": ["Vulnerabilities", "Large Language Models"], "second_label": ["Detection"], "data": "L Deng, H Lei, F Khan, G Srivastava, J Chen, M Haque\\xc2\\xa0- IEEE Journal of Biomedical\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIntegrating Natural Language Processing (NLP) with Generative Pre-trained \nTransformer (GPT) models plays a pivotal role in enhancing the accuracy and \nefficiency of healthcare software, which is essential for patient safety and providing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10899829/&hl=en&sa=X&d=4257828238536637457&ei=M_fcZ53oIcuZieoPiNWegAU&scisig=AFWwaebLUMgz30_kE6q-kcisdZTu&oi=scholaralrt&hist=apJ4fD8AAAAJ:6234092987365270793:AFWwaeZHIN6aK_iU38VPuuMoYcVu&html=&pos=1&folt=rel", "ref": ["Michael Fu - new related research"]}
{"title": "Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization", "first_label": ["Code"], "second_label": [], "data": "W Sun, Y Zhang, J Zhu, Z Wang, C Fang, Y Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCommenting code is a crucial activity in software development, as it aids in \nfacilitating future maintenance and updates. To enhance the efficiency of writing \ncomments and reduce developers' workload, researchers has proposed various\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.10737&hl=vi&sa=X&d=17720882860030193124&ei=M_fcZ8idKJGu6rQPuPKc2A8&scisig=AFWwaeaXyrj6Io13BxHt_fNslTAH&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=0&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Modularization is Better: Effective Code Generation with Modular Prompting", "first_label": ["Code"], "second_label": ["Generation"], "data": "R Pan, H Zhang\\xc2\\xa0- arXiv preprint arXiv:2503.12483, 2025\nLarge Language Models are transforming software development by automatically \ngenerating code. Current prompting techniques such as Chain-of-Thought (CoT) \nsuggest tasks step by step and the reasoning process follows a linear structure\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12483&hl=vi&sa=X&d=14254932694001679206&ei=M_fcZ8idKJGu6rQPuPKc2A8&scisig=AFWwaebJFNOBSO1Oa5RjObIyQw5e&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=4&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Prompt Alchemy: Automatic Prompt Refinement for Enhancing Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Ye, Z Sun, G Wang, L Guo, Q Liang, Z Li, Y Liu\\xc2\\xa0- arXiv preprint arXiv:2503.11085, 2025\nCode generation has emerged as a key task to automate software development by \nconverting high-level descriptions into executable code. Large language models \n(LLMs) excel at this but depend heavily on input prompt quality. Manual prompt\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11085&hl=vi&sa=X&d=10898369780965750435&ei=M_fcZ8idKJGu6rQPuPKc2A8&scisig=AFWwaeZPL9ZJQvxfs90qhRZOJYIS&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=6&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM-Driven Multi-step Translation from C to Rust using Static Analysis", "first_label": ["Large Language Models"], "second_label": ["Generation"], "data": "T Zhou, H Lin, S Jha, M Christodorescu, K Levchenko\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nTranslating software written in legacy languages to modern languages, such as C to \nRust, has significant benefits in improving memory safety while maintaining high \nperformance. However, manual translation is cumbersome, error-prone, and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12511&hl=vi&sa=X&d=8841093014565201462&ei=M_fcZ8idKJGu6rQPuPKc2A8&scisig=AFWwaeaqFbiuIGiODgu9c93gWe3Z&oi=scholaralrt&hist=apJ4fD8AAAAJ:11355862984917483435:AFWwaeZvT_NNWQMu4_zZrEW644gW&html=&pos=8&folt=rel", "ref": ["Xin ZHOU - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "first_label": [], "second_label": [], "data": "A Abouelenin, A Ashfaq, A Atkinson, H Awadalla\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language \nand multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model \ntrained on high-quality web and synthetic data, significantly outperforming recent\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.01743%3F&hl=en&sa=X&d=1080021729655208449&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeZMnVxRBi2ctuK9FSJVQwyR&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Hyperbolic Safety-Aware Vision-Language Models", "first_label": [], "second_label": [], "data": "T Poppi, T Kasarla, P Mettes, L Baraldi, R Cucchiara\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAddressing the retrieval of unsafe content from vision-language models such as \nCLIP is an important step towards real-world integration. Current efforts have relied \non unlearning techniques that try to erase the model's knowledge of unsafe\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12127&hl=en&sa=X&d=12085876808579258154&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaebJKdHqmPXQePNjCCCrM9xE&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "first_label": [], "second_label": [], "data": "M Song, X Qu, J Zhou, Y Cheng\\xc2\\xa0- arXiv preprint arXiv:2503.12821, 2025\nLarge Vision-Language Models (LVLMs) have achieved significant progress in \ncombining visual comprehension with language generation. Despite this success, \nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where the data\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12821&hl=en&sa=X&d=11737421450003055590&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeYpswc1NGxvjtITVBeAhDpK&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging Questions", "first_label": [], "second_label": [], "data": "Z Xu, D Chen, Z Ling, Y Li, Y Shen\\xc2\\xa0- arXiv preprint arXiv:2503.09499, 2025\nLarge vision-language models (VLMs) face challenges in achieving robust, \ntransferable reasoning abilities due to reliance on labor-intensive manual instruction \ndatasets or computationally expensive self-supervised methods. To address these\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.09499&hl=en&sa=X&d=16234010004755996134&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaebdFOya33LLwHosgVd-7KtX&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models", "first_label": [], "second_label": [], "data": "Y Gu, J Li, S Huang, X Zou, Z Li, X Hu\\xc2\\xa0- arXiv preprint arXiv:2502.14272, 2025\nAligning small language models (SLMs) with human values typically involves \ndistilling preference knowledge from large language models (LLMs). However, \nexisting distillation methods model preference knowledge in teacher LLMs by\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.14272&hl=en&sa=X&d=3283888565285721117&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeacCk5hYRYI-fAwtLN-1pFN&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Self-Steering Language Models", "first_label": [], "second_label": [], "data": "G Grand, JB Tenenbaum, V Mansinghka, AK Lew\\xe2\\x80\\xa6\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0: VerifAI: AI Verification in the Wild\nFor many reasoning tasks, augmenting language models with test-time compute can \nsignificantly boost performance. However, scaling inference is costly for complex \nproblems that require extensive search or sampling. Nevertheless, even when LMs\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dx7E2Qt7n0V&hl=en&sa=X&d=17836731747212682693&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeYnQPOEYSmZZ5fMjuMV3S05&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=6&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Reasoning-Grounded Natural Language Explanations for Language Models", "first_label": [], "second_label": [], "data": "V Cahlik, R Alves, P Kordik\\xc2\\xa0- arXiv preprint arXiv:2503.11248, 2025\nWe propose a large language model explainability technique for obtaining faithful \nnatural language explanations by grounding the explanations in a reasoning \nprocess. When converted to a sequence of tokens, the outputs of the reasoning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.11248&hl=en&sa=X&d=4808909619091708037&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeYBmclKBgul_ucUlUBjHlDA&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=7&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Seeing Delta Parameters as JPEG Images: Data-Free Delta Compression with Discrete Cosine Transform", "first_label": [], "second_label": [], "data": "C Huang, P Ye, X Wang, S Zheng, B Qi, L Bai\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith transformer-based models and the pretrain-finetune paradigm becoming \nmainstream, the high storage and deployment costs of individual finetuned models \non multiple tasks pose critical challenges. Delta compression attempts to lower the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.06676&hl=en&sa=X&d=995345491034050371&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeZfZBS0V6XZUNj8Ge8yWa9K&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=8&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging", "first_label": [], "second_label": [], "data": "CA Li, TH Lin, YN Chen, H Lee\\xc2\\xa0- arXiv preprint arXiv:2502.13487, 2025\nLarge vision-language models (LVLMs) perform outstandingly across various \nmultimodal tasks. However, their ability to evaluate generated content remains \nlimited, and training vision-language reward models (VLRMs) with preference data is\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nCarlos E. Jimenez\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2502.13487&hl=en&sa=X&d=13394854825546742863&ei=M_fcZ-6XG6OD6rQPx5mGgAY&scisig=AFWwaeZODCXIAqV46WkZ3PiYLOnm&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=9&folt=rel", "ref": ["Carlos E. Jimenez - new related research"]}
{"title": "ICSQuartz: Scan Cycle-Aware and Vendor-Agnostic Fuzzing for Industrial Control Systems", "first_label": ["Fuzzing"], "second_label": [], "data": "C Villa, C Doumanidis, H Lamri, PHN Rajput\\xe2\\x80\\xa6\nIndustrial Control Systems (ICS) ensure the automation and safe operation of critical \nindustry, energy, and commerce processes. Despite its importance, ICS code often \ncannot be evaluated as rigorously as software on traditional computing platforms, as\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.ndss-symposium.org/wp-content/uploads/2025-795-paper.pdf&hl=en&sa=X&d=10524862650214833634&ei=M_fcZ8_WJsmpieoPrOWNiQo&scisig=AFWwaeZZ_2ARI6V22CenXEygxr-a&oi=scholaralrt&hist=apJ4fD8AAAAJ:11137134570824175991:AFWwaeZJgvZkFmSwNlRigHvrI7d8&html=&pos=0&folt=rel", "ref": ["Abhik Roychoudhury - new related research"]}

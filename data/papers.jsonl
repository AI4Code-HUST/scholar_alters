{"title": "Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System", "first_label": ["LLM"], "second_label": ["Agent", "Exploit"], "data": "Y Liu, Y Xie, M Luo, Z Liu, Z Zhang, K Zhang, Z Li- arXiv preprint arXiv, 2025\nLLM-based agentic systems leverage large language models to handle user queries, \nmake decisions, and execute external tools for complex tasks across domains like \nchatbots, customer service, and software engineering. A critical component of these", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.05755%3F&hl=en&sa=X&d=7380962265010499197&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b_WNN5iEJzU4LQ3fLBGGpF6&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm", "first_label": ["LLM"], "second_label": [], "data": "Y Pang, W Meng, X Liao, T Wang- arXiv preprint arXiv:2509.07287, 2025\nWith the rapid development of large language models, the potential threat of their \nmalicious use, particularly in generating phishing content, is becoming increasingly \nprevalent. Leveraging the capabilities of LLMs, malicious users can synthesize", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.07287&hl=en&sa=X&d=841142860163656335&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9zUiVve_NVtg5_7UjR-gXJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Leveraging CVAE Encoding for Backdoor Attacks in Few-Shot Learning with Prototypical Networks", "first_label": [], "second_label": [], "data": "Q Yan, S Liang, A Ullah- IEEE Transactions on Dependable and Secure, 2025\nFew-shot learning (FSL) has demonstrated tremendous potential when challenged \nwith limited training data, but the assessment of its vulnerability to backdoor attacks is \nstill at an early stage. However, recent research revealed this deep learning", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11152502/&hl=en&sa=X&d=6687930075784700670&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9ptsbEWBr80FGSyzcZR2Er&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Prototype-Guided Robust Learning against Backdoor Attacks", "first_label": [], "second_label": [], "data": "W Guo, M Pintor, A Demontis, B Biggio- arXiv preprint arXiv:2509.08748, 2025\nBackdoor attacks poison the training data to embed a backdoor in the model, \ncausing it to behave normally on legitimate inputs but maliciously when specific \ntrigger signals appear. Training a benign model from a dataset poisoned by", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.08748&hl=en&sa=X&d=9079518275239981489&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b8nbcMJrz3EP9De5YM82FpF&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Oyster-I: Beyond Refusal--Constructive Safety Alignment for Responsible Language Models", "first_label": ["LLM"], "second_label": [], "data": "R Duan, J Liu, X Jia, S Zhao, R Cheng, F Wang, C Wei- arXiv preprint arXiv, 2025\nLarge language models (LLMs) typically deploy safety mechanisms to prevent \nharmful content generation. Most current approaches focus narrowly on risks posed \nby malicious actors, often framing risks as adversarial events and relying on", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.01909%3F&hl=en&sa=X&d=6590160868100905669&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b8p2pmNGwnqdehrNy547NkZ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Jailbreak Attack with Multimodal Virtual Scenario Hypnosis for Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "X Shi, S Chen, G Zhang, W Wei, Y Li, Z Fan, J Liu- Pattern Recognition, 2025\nDue to the inherent vulnerabilities of large Vision-Language Models (VLMs), security \ngovernance has emerged as a critical concern, particularly given the risks posed by \nnoisy and biased training data as well as adversarial attacks, including data", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0031320325010520&hl=en&sa=X&d=18404876866865125967&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9xJx3YOX0I9CBN24pCw_YG&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Why language models hallucinate", "first_label": ["LLM"], "second_label": [], "data": "AT Kalai, O Nachum, SS Vempala, E Zhang- arXiv preprint arXiv:2509.04664, 2025\nLike students facing hard exam questions, large language models sometimes guess \nwhen uncertain, producing plausible yet incorrect statements instead of admitting \nuncertainty. Such\" hallucinations\" persist even in state-of-the-art systems and", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.04664&hl=en&sa=X&d=17470905045322269269&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9-xvJdrsXYLi4Kkc78fX8P&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair", "first_label": ["APR", "LLM", "Bug"], "second_label": ["Repair"], "data": "P Przymus, A Happe, J Cito- arXiv preprint arXiv:2509.05372, 2025\nLarge Language Model (LLM)-based Automated Program Repair (APR) systems are \nincreasingly integrated into modern software development workflows, offering \nautomated patches in response to natural language bug reports. However, this", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.05372&hl=en&sa=X&d=17856431439942945890&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b9iNtxXJTJlEQXG_3JYQbs1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees", "first_label": ["LLM"], "second_label": [], "data": "S Zeighami, S Shankar, A Parameswaran- arXiv preprint arXiv:2509.02896, 2025\nLarge Language Models (LLMs) are being increasingly used as a building block in \ndata systems to process large text datasets. To do so, LLM model providers offer \nmultiple LLMs with different sizes, spanning various cost-quality trade-offs when", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.02896%3F&hl=en&sa=X&d=17574372354989562785&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b-DvnVUyd5iBucpFadx2SoL&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System", "first_label": ["LLM"], "second_label": ["Exploit"], "data": "P Reddy, AS Gujral- arXiv preprint arXiv:2509.10540, 2025\nLarge language model (LLM) assistants are increasingly integrated into enterprise \nworkflows, raising new security concerns as they bridge internal and external data \nsources. This paper presents an in-depth case study of EchoLeak (CVE-2025\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.10540%3F&hl=en&sa=X&d=3830339454672368349&ei=jtrdaIfIJMSz6rQPyKGw-Ag&scisig=AAZF9b-2-nfVtjM51-72VPmeR1Yc&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "first_label": ["LLM", "Bug", "Repository-Level"], "second_label": [], "data": "J Liu, Z Liu, Z Cheng, M He, X Shi, Y Guo, X Zhu, Y Guo- arXiv preprint arXiv, 2025\nLarge Language Models (LLMs) have exhibited significant proficiency in code \ndebugging, especially in automatic program repair, which may substantially reduce \nthe time consumption of developers and enhance their efficiency. Significant\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.04078%3F&hl=en&sa=X&d=12419661760319839544&ei=jNrdaL2QCJXP6rQP9ZK58Q0&scisig=AAZF9b8SHef7li0_lXitWcja8LI-&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "Generating Software Architectural Model from Source Code Using Module Clustering", "first_label": ["Code"], "second_label": [], "data": "B Arasteh, SS Sefati, H Kusetogullari, F Kiani- Symmetry, 2025\nSoftware maintenance is one of the most expensive phases in software \ndevelopment, especially when complex source code is the only available artifact. \nClustering software modules and generating a structured architectural model can\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2073-8994/17/9/1523&hl=en&sa=X&d=9037379356857496352&ei=jNrdaKWYLISb6rQPt9iYgAM&scisig=AAZF9b-K0RiHQ1pE7QZ3IUfLNE6M&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Aligning Requirement for Large Language Model's Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "Z Tian, J Chen- arXiv preprint arXiv:2509.01313, 2025\nCode generation refers to the automatic generation of source code based on a given \nprogramming specification, which has garnered significant attention particularly with \nthe advancement of large language models (LLMs). However, due to the inherent", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.01313&hl=en&sa=X&d=13370697676538068499&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b9B7L937IdcMo1J6Zsy7VAv&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis", "first_label": ["Fuzzing"], "second_label": ["Generation"], "data": "R Saravanan, S Paria, A Dasgupta, S Bhunia- arXiv preprint arXiv:2509.20808, 2025\nHardware Fuzzing emerged as one of the crucial techniques for finding security flaws \nin modern hardware designs by testing a wide range of input scenarios. One of the \nmain challenges is creating high-quality input seeds that maximize coverage and", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2509.20808&hl=en&sa=X&d=11635584275403043134&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b-EaDXCg2a3mA1rI2gL4oLU&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Performance and interpretability analysis of code generation large language models", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "VS Pendyala, NB Thakur- Neurocomputing, 2025\nAbstract As Large Language Models (LLMs) are increasingly getting integrated into \nsoftware development workflows, understanding their reliability, error patterns and \ninterpretability in real-world development scenarios is crucial for establishing their\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0925231225021332&hl=en&sa=X&d=12519052331836719178&ei=jtrdaJSmPPKOieoP85S7oQk&scisig=AAZF9b_3S5NkeiHUCnc01CtGOqCD&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Moving Towards Robust and Reliable AI: Vulnerability Detection Framework Through Red Teaming", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "V Raman, A Desarkar, A Sen- Data Science and Communication Engineering, 2025\nEnsuring the reliability of AI-based systems is a crucial challenge in today's AI-driven \nenvironment. However, robustness is a key component of reli-able AI, as the failure \nof these systems could have severe consequences in the critical domains such as in \nhealthcare, transportation and finance. Eventually, the systems in these domains are \nlargely dependent on various language models. Hence, measuring the robustness of \nthose language models ultimately determines the end success though no such\nCites: Large language model for vulnerability detection and repair\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3Dli-KEQAAQBAJ%26oi%3Dfnd%26pg%3DPA209%26ots%3DWIkWfTeWxS%26sig%3DBsjJf3VFapSd3RlrgCNs85_xYMU&hl=en&sa=X&d=6448497152658089733&ei=jtrdaIOUC46IieoP7dK5sA0&scisig=AAZF9b8sv3RIed1EIJJXX70RAtS0&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["1 new citation to articles by Xin ZHOU"]}
{"title": "Fuzzing as editor feedback", "first_label": ["Fuzzing"], "second_label": [], "data": "M Garus, J Lincke, R Hirschfeld- Companion Proceedings of the 9th International, 2025\nLive programming requires concrete examples, but coming up with examples takes \neffort. However, there are ways to execute code without specifying examples, such \nas fuzzing. Fuzzing is a technique that synthesizes program inputs to find bugs in", "link": "https://scholar.google.com/scholar_url?url=https://drops.dagstuhl.de/storage/01oasics/oasics-vol134-programming2025/OASIcs.Programming.2025.8/OASIcs.Programming.2025.8.pdf&hl=en&sa=X&d=4846729257551256103&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b_a936tJ2euestamYofC9hC&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "PromeFuzz: A Knowledge-Driven Approach to Fuzzing Harness Generation with Large Language Models", "first_label": ["LLM", "Fuzzing"], "second_label": ["Generation"], "data": "Y Liu, J Deng, X Jia, Y Wang, M Wang, L Huang, T Wei\nFuzzing has long been recognized as an effective technique for uncovering security \nvulnerabilities by automatically generating and executing a diverse set of inputs [2, 3, \n6, 8, 14, 15, 18, 20, 24, 26, 30, 32, 34, 46, 52, 53, 55, 58]. Traditional fuzzing tools", "link": "https://scholar.google.com/scholar_url?url=https://pvz122.github.io/pdf/25-promefuzz.pdf&hl=en&sa=X&d=12396611312661296556&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b-wSitwHBsEc3RIko2Rlc5c&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Fuzzing JavaScript Engines by Fusing JavaScript and WebAssembly", "first_label": ["Fuzzing"], "second_label": [], "data": "J Lin, C Luo, M Zhang, L Lin, P Li, C Qian - 2026\nJavaScript engines are a fundamental part of modern browsers, and many efforts \nhave been invested in testing them to enhance their security. However, the \nincorporation of WebAssembly into JavaScript engines introduces new attack\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://peng-hui.github.io/data/paper/icse26:mad-eye.pdf&hl=en&sa=X&d=7025301240690243176&ei=jdrdaP25D46IieoP7dK5sA0&scisig=AAZF9b87H2l6opoFTjGi99T54n3u&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}

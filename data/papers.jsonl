{"title": "Improving Quality in AI-Generated Code through Prompt Engineering: An Evaluation of the Impact of Prompting Techniques on Software Quality in Code Generated by", "first_label": ["Code"], "second_label": [], "data": "M Paulsson - 2025\nAbstract The use of Large Language Models (LLMs) for code generation is rapidly \nincreasing. While previous investigations on the correctness of AI-generated code \nare widely present, less focus has been placed on the code's further quality attributes \nand how it can be improved. An aspect that can be equally important. This thesis \naims to fill this gap by investigating how LLMs can be guided to generate code that \naligns with software quality principles. Using a mixed methods approach, the study\nCites: Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.diva-portal.org/smash/get/diva2:1999528/FULLTEXT01.pdf&hl=en&sa=X&d=16193075811331607910&ei=QEFCad-GIo2v6rQP-5Lh8QE&scisig=ALhkC2TTDz5-urwF1NNor-xk3yYw&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:ALhkC2SEiW6V5LLNyO9vTSEOmBLL&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["1 new citation to articles by Bach Le", "1 new citation to articles by Thanh Le-Cong"]}
{"title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "first_label": ["LLM"], "second_label": [], "data": "G Lin, X Yu, J Keung, X Hu, X Xia, AX Liu- arXiv preprint arXiv:2511.21022, 2025\nPre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) \nhave demonstrated strong performance in code completion tasks. However, their \nembedded knowledge is constrained by the timeliness of training data, which often", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.21022&hl=en&sa=X&d=4266302194864055298&ei=QEFCaYeQN_O16rQPzoDK8Ag&scisig=ALhkC2SQN5t75dyVOyXYpV0C9R6I&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:ALhkC2THo_3s8tBI3qmyNrpmj0cv&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "first_label": ["Code"], "second_label": [], "data": "H Yang, A Velasco, S Fang, B Xu, D Poshyvanyk- arXiv preprint arXiv:2512.07814, 2025\nLarge language models for code (LLM4Code) have greatly improved developer \nproductivity but also raise privacy concerns due to their reliance on open-source \nrepositories containing abundant personally identifiable information (PII). Prior work", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.07814&hl=en&sa=X&d=4516944301368392231&ei=QEFCaYeQN_O16rQPzoDK8Ag&scisig=ALhkC2QQ_nY4jJQYogoZFhpcgl2l&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:ALhkC2THo_3s8tBI3qmyNrpmj0cv&html=&pos=1&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "A Hybrid Approach for EMF Code Generation: Code Templates Meet Large Language Models", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "X He, R Chen, Z Zhang, Y Wang, Q Dong- arXiv preprint arXiv:2512.05498, 2025\nTemplate-based and LLM-based code generation are both key enablers of \nautomated software development. The former provides correctness guarantees but \nare rigid for complex requirements, whereas LLMs offer high flexibility at the risk of\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.05498&hl=en&sa=X&d=6490927253740960650&ei=QEFCaYeQN_O16rQPzoDK8Ag&scisig=ALhkC2SkoRkFIHGmozxFjn8uel9C&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:ALhkC2THo_3s8tBI3qmyNrpmj0cv&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "An Empirical Study on the Security Vulnerabilities of GPTs", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "T Wu, W Wu, Z Zheng- arXiv preprint arXiv:2512.00136, 2025\nEquipped with various tools and knowledge, GPTs, one kind of customized AI agents \nbased on OpenAI's large language models, have illustrated great potential in many \nfields, such as writing, research, and programming. Today, the number of GPTs has\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.00136&hl=en&sa=X&d=13668712987477158056&ei=QUFCaZKtIf3D6rQP75fFuQs&scisig=ALhkC2SWxFGyYLsV4-w0YxoUAqdn&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:ALhkC2SS1P3l44RDJXsNEdNzDLiT&html=&pos=0&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Adapting Language Models for Low-Resource Programming Languages", "first_label": ["LLM"], "second_label": [], "data": "A Singha, M Singh, H Hasanbeig, A Radhakrishna- NeurIPS 2025 Fourth Workshop on\nLarge Language Models (LLMs) have achieved remarkable success in code \ngeneration, yet their capabilities remain predominantly concentrated in well-\nresourced programming languages such as Python and Java. In contrast, low\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D2ctRK8h3AZ&hl=en&sa=X&d=10639384588679978541&ei=QEFCafvWC6GvieoPtIGfoAw&scisig=ALhkC2TU1E8-xJojBcX0Q0YHF0cb&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:ALhkC2RZmRqgZGCFQXCJyP9vthGu&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "Deep-Reproducer: From Paper Understanding to Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "P Chen, N Yan, Z Zhao, Y Lin, H Chen, Y Hu, Q Bai- NeurIPS 2025 Fourth Workshop on\nRecent Large Language Models (LLMs) demonstrate strong code generation \ncapabilities, however, they often fall short in translating complex, multi-component \nresearch methodologies into a coherent, functional codebase and automatic", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3Dzw2DpSxnXn&hl=en&sa=X&d=13649947434595596049&ei=P0FCac6YMdOyieoP1qvf4A4&scisig=ALhkC2SJQA_ssGWn8RKtaHJK2CoE&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:ALhkC2QkVwCdvNzUylYiTyCmheSL&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "The Cost of AI-Assisted Coding: Energy vs. Accuracy in Language Models", "first_label": ["LLM"], "second_label": [], "data": "N Alizadeh, B Belchev, N Saurabh, P Kelbert - 2025\nGenerative Large Language Models (LLMs) have become widely accessible since \nthe release of ChatGPT in late 2022 [2], and their adoption nearly doubled in under \nsix months [3]. In addition, the majority of developers find code-specific AI models\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://benevol2025.github.io/pre/paper03.pdf&hl=en&sa=X&d=3874342652444288509&ei=P0FCac6YMdOyieoP1qvf4A4&scisig=ALhkC2Rn562zMHnFoj6JiddHdWsr&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:ALhkC2QkVwCdvNzUylYiTyCmheSL&html=&pos=1&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research", "Hong Jin Kang - new related research"]}
{"title": "LLM-Boofuzz: Generation-Based Black-Box Fuzzing for Network Protocols via LLMs", "first_label": ["LLM", "Fuzzing"], "second_label": ["Generation"], "data": "T Wang, Y Li, Z Pan, Q Chen, Z Li, Y Zhang, Y Shen- Electronics, 2025\nIdentifying network protocol vulnerabilities is critical for cyberspace security. \nGeneration-based black-box protocol fuzzing is widely used but faces challenges: \nover-reliance on manual protocol analysis and script writing, single-threaded fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2079-9292/14/23/4550&hl=en&sa=X&d=9654097801574034546&ei=QUFCabTVM7PFieoPper8uAM&scisig=ALhkC2T6u0npAy6u6q8OwxTjlnFS&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:ALhkC2Tff526TIEbfh2mbT-kfB65&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Citadel: Context Similarity Based Deep Learning Framework Bug Finding", "first_label": ["Bug"], "second_label": [], "data": "Z XIAOYU, J ZHAI, S MA, S WANG, C SHEN - 2025\nWith the development of Deep Learning (DL) techniques, DL-powered systems are \nplaying an increasingly significant role in software development. For example, \nMicrosoft has developed a new search engine powered by DL techniques to", "link": "https://scholar.google.com/scholar_url?url=https://shiningrain.github.io/papers/Zhang2025TOSEM-1.pdf&hl=en&sa=X&d=17476988302280239495&ei=QkFCab2rGfO16rQPzoDK8Ag&scisig=ALhkC2S1GZlIiWI2SbX3lq_WxO5T&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:ALhkC2T269WwPtyd5qvti3WNZV40&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Quality Assurance in Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "D Song - 2025\nAbstract Large Language Models (LLMs) have demonstrated impressive capabilities \nacross a wide range of tasks in natural language processing (NLP) and software \nengineering (SE). However, their increasing adoption has raised critical concerns", "link": "https://scholar.google.com/scholar_url?url=https://ualberta.scholaris.ca/bitstreams/182b82d1-f9e0-4e10-a493-6beba20a3db2/download&hl=en&sa=X&d=9868273145291117306&ei=QkFCab2rGfO16rQPzoDK8Ag&scisig=ALhkC2RPW6TGX7QUf3cLjujpLt7X&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:ALhkC2T269WwPtyd5qvti3WNZV40&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "A Li, M Liu, Z Chen, Z Pei, Z Li, D Dai, Y Wang, Z Zheng- arXiv preprint arXiv, 2025\nAutomated unit test generation using large language models (LLMs) holds great \npromise but often struggles with generating tests that are both correct and \nmaintainable in real-world projects. This paper presents KTester, a novel framework\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.14224&hl=en&sa=X&d=10983934054869938261&ei=QkFCab2rGfO16rQPzoDK8Ag&scisig=ALhkC2SnqeAXUhGFyjUYbpo2ILAS&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:ALhkC2T269WwPtyd5qvti3WNZV40&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Quality Evaluation of Generative AI Systems: Processes, Metrics, Methods, and Frameworks for Industrial Software Engineering", "first_label": [], "second_label": [], "data": "L Yu - 2026\nAbstract Generative Artificial Intelligence (GenAI) is being rapidly adopted in software \nengineering, introducing a paradigm shift toward human-AI co-creation. However, \nthe non-deterministic, probabilistic, and often black-box nature of GenAI models \npresents challenges for traditional software quality assurance. Conventional \nverification and validation techniques are insufficient to handle outputs that are \nneither predictably correct nor incorrect, but rather stochastically plausible. This\nCites: AutoCodeRover: Autonomous program improvement\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://www.diva-portal.org/smash/record.jsf%3Fpid%3Ddiva2:2018519&hl=en&sa=X&d=5262498350567037753&ei=QUFCadmWDuaQ6rQP6fX46As&scisig=ALhkC2SAGdeI3wOCtzpqXuey2vgU&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:ALhkC2SLNtxsIYV7y8T6Ni2J4ruF&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["1 new citation to articles by Abhik Roychoudhury"]}

{"title": "Transfer learning for software vulnerability prediction using Transformer models", "first_label": ["Vulnerabilities"], "second_label": [], "data": "I Kalouptsoglou, M Siavvas, A Ampatzoglou\\xe2\\x80\\xa6\\xc2\\xa0- Journal of Systems and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecently software security community has exploited text mining and deep learning \nmethods to identify vulnerabilities. To this end, the progress in the field of Natural \nLanguage Processing (NLP) has opened a new direction in constructing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0164121225001165&hl=en&sa=X&d=1704083859104944765&ei=FUr_Z-X2F_Cj6rQPwoSn6A4&scisig=AFWwaebqThnUMAyzykWvaSgtXFmK&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=0&folt=rel", "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "Xin ZHOU - new related research", "Richard Fang - new related research"]}
{"title": "Automated Assessment in Mobile Programming Courses: Leveraging GitHub Classroom and Flutter for Enhanced Student Outcomes", "first_label": [], "second_label": [], "data": "P Alves, BP Cipriano\\xc2\\xa0- arXiv preprint arXiv:2504.04230, 2025\nThe growing demand for skilled mobile developers has made mobile programming \ncourses an essential component of computer science curricula. However, these \ncourses face unique challenges due to the complexity of mobile development\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.04230&hl=en&sa=X&d=4084265759440791903&ei=FUr_Z-X2F_Cj6rQPwoSn6A4&scisig=AFWwaeaqETS3vYteQlWJK9mF3OwV&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=1&folt=rel", "ref": ["David Lo - new related research"]}
{"title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets", "first_label": ["LLM", "Code"], "second_label": [], "data": "H Jelodar, M Meymani, R Razavi-Far\\xc2\\xa0- arXiv preprint arXiv:2503.17502, 2025\nLarge language models (LLMs) and transformer-based architectures are \nincreasingly utilized for source code analysis. As software systems grow in \ncomplexity, integrating LLMs into code analysis workflows becomes essential for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17502&hl=en&sa=X&d=13932223716190867094&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaeb1dh07lugR2sszh6eusTe5&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=0&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "DALO-APR: LLM-based automatic program repair with data augmentation and loss function optimization", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "S Wang, L Lu, S Qiu, Q Tian, H Lin\\xc2\\xa0- The Journal of Supercomputing, 2025\nAutomatic program repair (APR) has made significant strides with the advent of large \nlanguage models (LLMs) such as T5 and CodeT5. However, LLM-based APR \nmodels may rely on repetitive repair patterns due to limited training data diversity\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11227-025-07102-3&hl=en&sa=X&d=10480458481806281342&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaeZgISIeIz-af4x6cYmuf3Oy&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=1&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Automatic High-Level Test Case Generation using Large Language Models", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "NB Hasan, MA Islam, JY Khan, S Senjik, A Iqbal\\xc2\\xa0- arXiv preprint arXiv:2503.17998, 2025\nWe explored the challenges practitioners face in software testing and proposed \nautomated solutions to address these obstacles. We began with a survey of local \nsoftware companies and 26 practitioners, revealing that the primary challenge is not\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17998&hl=en&sa=X&d=2435370287861060433&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaeb0yzfBEwJ9p6RT6YLxQ0vY&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=3&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models", "first_label": ["LLM", "Code Review", "Code"], "second_label": [], "data": "HY Lin, C Liu, H Gao, P Thongtanunam, C Treude\\xc2\\xa0- arXiv preprint arXiv:2503.16167, 2025\nState-of-the-art large language models (LLMs) have demonstrated impressive code \ngeneration capabilities but struggle with real-world software engineering tasks, such \nas revising source code to address code reviews, hindering their practical use. Code\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16167&hl=en&sa=X&d=13987149689206803549&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaeZRD8EhNW4NSPG4ypaH_WWQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=4&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Enhancing LLM-based Code Translation in Repository Context via Triple Knowledge-Augmented", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "G Ou, M Liu, Y Chen, X Du, S Wang, Z Zhang, X Peng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have behaved well in function-level code translation \nwithout repository-level context. However, the performance of LLMs in repository-\nlevel context code translation remains suboptimal due to complex dependencies and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.18305&hl=en&sa=X&d=1496301354611823227&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaebZgjNWSGlwwMWRyrQ2cYwe&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=5&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "J Xiang, T Ye, P Liu, Y Zhang, W Wang\\xc2\\xa0- arXiv preprint arXiv:2503.18460, 2025\nModelica is a widely adopted language for simulating complex physical systems, yet \neffective model creation and optimization require substantial domain expertise. \nAlthough large language models (LLMs) have demonstrated promising capabilities\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.18460%3F&hl=en&sa=X&d=9392276640239633917&ei=FUr_Z_jEIZyV6rQP8q-dkA0&scisig=AFWwaeZT_yvBwTF_pGcKhcsdnWzW&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=6&folt=rel", "ref": ["Hong Jin Kang - new related research"]}
{"title": "Reasoning with LLMs for Zero-Shot Vulnerability Detection", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Reasoning"], "data": "A Zibaeirad, M Vieira\\xc2\\xa0- arXiv preprint arXiv:2503.17885, 2025\nAutomating software vulnerability detection (SVD) remains a critical challenge in an \nera of increasingly complex and interdependent software systems. Despite \nsignificant advances in Large Language Models (LLMs) for code analysis, prevailing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17885&hl=en&sa=X&d=16151102993317123730&ei=FUr_Z-SaFoSlieoPpZSOmAc&scisig=AFWwaeaU1duZG60YHiU0D1fyotku&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=0&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "A Comprehensive Study of LLM Secure Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "SC Dai, J Xu, G Tao\\xc2\\xa0- arXiv preprint arXiv:2503.15554, 2025\nLLMs are widely used in software development. However, the code generated by \nLLMs often contains vulnerabilities. Several secure code generation methods have \nbeen proposed to address this issue, but their current evaluation schemes leave\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15554%3F&hl=en&sa=X&d=1363137439941333778&ei=FUr_Z-SaFoSlieoPpZSOmAc&scisig=AFWwaea70lzbx-2dnx-SlddbyIDc&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=1&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "first_label": ["LLM", "Code"], "second_label": ["Agent", "Reasoning"], "data": "A Wei, T Suresh, J Cao, N Kannan, Y Wu, K Yan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInductive program synthesis, or programming by example, requires synthesizing \nfunctions from input-output examples that generalize to unseen inputs. While large \nlanguage model agents have shown promise in programming tasks guided by\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.23145&hl=en&sa=X&d=7322261732284042571&ei=FUr_Z-SaFoSlieoPpZSOmAc&scisig=AFWwaeaAH4_AfF9MmAImwDIX-Jtw&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=2&folt=rel", "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "first_label": [], "second_label": [], "data": "X Jia, S Gao, S Qin, K Ma, X Li, Y Huang, W Dong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge pre-trained vision-language models (VLMs), such as CLIP, demonstrate \nimpressive generalization but remain highly vulnerable to adversarial examples \n(AEs). Previous work has explored robust text prompts through adversarial training\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.12874&hl=en&sa=X&d=4288320110718600484&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaebZ47dtXpHPS7gkzptvLmTz&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=0&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "XSShield: Defending Against Stored XSS Attacks Using LLM-Based Semantic Understanding", "first_label": ["LLM"], "second_label": [], "data": "Y Zhou, E Wang, W Yang, W Ge, S Yang, Y Zhang\\xe2\\x80\\xa6\\xc2\\xa0- Applied Sciences, 2025\nCross-site scripting attacks represent one of the major security threats facing web \napplications, with Stored XSS attacks becoming the predominant form. Compared to \nreflected XSS, stored XSS attack payloads exhibit temporal and spatial asynchrony\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2076-3417/15/6/3348&hl=en&sa=X&d=11849643047241211005&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeZJp-xCesiNk9t9IXhXxSLJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=1&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Dapo: An open-source llm reinforcement learning system at scale", "first_label": ["LLM"], "second_label": [], "data": "Q Yu, Z Zhang, R Zhu, Y Yuan, X Zuo, Y Yue, T Fan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInference scaling empowers LLMs with unprecedented reasoning ability, with \nreinforcement learning as the core technique to elicit complex reasoning. However, \nkey technical details of state-of-the-art reasoning LLMs are concealed (such as in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14476&hl=en&sa=X&d=1968980266662184520&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaebXNS6d8Y_C8FG4sfvmXe0u&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=3&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Test-Time Backdoor Detection for Object Detection Models", "first_label": ["Software Testing"], "second_label": ["Detection"], "data": "H Zhang, Y Wang, S Yan, C Zhu, Z Zhou, L Hou, S Hu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nObject detection models are vulnerable to backdoor attacks, where attackers poison \na small subset of training samples by embedding a predefined trigger to manipulate \nprediction. Detecting poisoned samples (ie, those containing triggers) at test time can\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15293%3F&hl=en&sa=X&d=6052108469819470987&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeZFwCtybcnsCXtBxoexifa2&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=4&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Why Do Multi-Agent LLM Systems Fail?", "first_label": ["LLM"], "second_label": ["Agent"], "data": "M Cemri, MZ Pan, S Yang, LA Agrawal, B Chopra\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDespite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM \nagents collaborate to accomplish tasks, their performance gains across popular \nbenchmarks remain minimal compared to single-agent frameworks. This gap\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.13657%3F&hl=en&sa=X&d=6760601859756774675&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeaAVNboT1uLyYYlymPHECf1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=5&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Aligning Multimodal LLM with Human Preference: A Survey", "first_label": ["LLM"], "second_label": [], "data": "T Yu, C Fu, J Wu, J Lu, K Wang, X Lu, Y Shen, G Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) can handle a wide variety of general tasks with \nsimple prompts, without the need for task-specific training. Multimodal Large \nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14504&hl=en&sa=X&d=1315835665997175613&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeaYxO6Scdjrz2Sdyj1j9_2t&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=6&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM-Based Automation of COSMIC Functional Size Measurement from Use Cases", "first_label": ["LLM"], "second_label": [], "data": "G De Vito, S Di Martino, F Ferrucci, C Gravino\\xe2\\x80\\xa6\\xc2\\xa0- IEEE Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCOmmon Software Measurement International Consortium (COSMIC) Functional \nSize Measurement is a method widely used in the software industry to quantify user \nfunctionality and measure software size, which is crucial for estimating development\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://fpalomba.github.io/pdf/Journals/J77.pdf&hl=en&sa=X&d=10857265925413407421&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeYYT0Q2dK2V6Z2SyX7XCGnO&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=7&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "Temporal Consistency for LLM Reasoning Process Error Identification", "first_label": ["LLM"], "second_label": ["Reasoning"], "data": "J Guo, Y Wu, J Qiu, K Huang, X Juan, L Yang, M Wang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nVerification is crucial for effective mathematical reasoning. We present a new \ntemporal consistency method where verifiers iteratively refine their judgments based \non the previous assessment. Unlike one-round verification or multi-model debate\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14495&hl=en&sa=X&d=18020288792776836625&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeba3ShxiqHc2VTrBXbMnsjA&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=8&folt=rel", "ref": ["Richard Fang - new related research"]}
{"title": "LLM Generated Persona is a Promise with a Catch", "first_label": ["LLM"], "second_label": [], "data": "A Li, H Chen, H Namkoong, T Peng\\xc2\\xa0- arXiv preprint arXiv:2503.16527, 2025\nThe use of large language models (LLMs) to simulate human behavior has gained \nsignificant attention, particularly through personas that approximate individual \ncharacteristics. Persona-based simulations hold promise for transforming disciplines\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16527&hl=en&sa=X&d=15287455255324878097&ei=FUr_Z_b1G8mpieoPv7aWiQ4&scisig=AFWwaeYIlIV_cXUCW5nMHAbkIUyA&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=9&folt=rel", "ref": ["Richard Fang - new related research"]}

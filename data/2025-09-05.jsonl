{"title": "Multimodal Fusion for Vulnerability Detection: Integrating Sequence and Graph-Based Analysis with LLM Augmentation", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Graph"], "data": "NDT Ngan, NH Khoa, VH Pham, PT Duy- 2025 International Conference on, 2025\nDetecting vulnerabilities in source code remains a challenging task due to the \ncomplex and diverse ways security flaws can manifest. This study investigates how to \neffectively combine sequential code semantics and graph-based structural features", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/11133716/11133719/11133833.pdf&hl=en&sa=X&d=18161264188626294235&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b_qzqk9xJ5xCaFpeq6KjmrR&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "2 new citations to articles by Richard Fang", "Bach Le - new related research", "Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Is code coverage of performance tests related to source code features? An empirical study on open-source Java systems", "first_label": ["Code", "Software Testing"], "second_label": [], "data": "M Imran, V Cortellessa, D Di Ruscio, R Rubei, L Traini- Empirical Software, 2025\nPerformance testing aims to ensure the operational efficiency of software systems. \nHowever, many factors influencing the efficacy and adoption of performance tests in \npractice are not yet fully understood. For instance, while code coverage is widely", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10712-3&hl=en&sa=X&d=4859854080979322054&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b9thlXAMwoU1FqGzi8cr7gY&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "Quang-Cuong Bui - new related research", "David Lo - new related research"]}
{"title": "Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization", "first_label": ["Vulnerabilities"], "second_label": [], "data": "C Gao, X Chen, J Wang, J Wang, G Yang- Engineering Applications of Artificial, 2025\nThe increasing complexity of software systems has led to a surge in cybersecurity \nvulnerabilities, necessitating efficient and scalable solutions for vulnerability \nassessment. However, the deployment of large pre-trained models in real-world\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.02840%3F&hl=en&sa=X&d=8585706311454434502&ei=04i3aKnVKoDXieoPk73huQQ&scisig=AAZF9b8gAdH4KkGQefk07ervBA2M&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Enhancing Fine-Grained Vulnerability Detection with Reinforcement Learning", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "Y Jiang, Z Qu, C Treude, X Su, T Wang- IEEE Transactions on Software Engineering, 2025\nThe rapid growth of vulnerabilities has significantly accelerated the development of \nautomated vulnerability detection methods, especially those based on data-driven \nmodels. However, most of them primarily focus on extracting accurate code \nrepresentations while overlooking the complex vulnerability patterns among \nvulnerable statements, thereby leaving room for improvement. To overcome this \nlimitation, we present a novel reinforcement learning framework (RLFD) for detecting\nCites: Multi-granularity detector for vulnerability fixes", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11145224/&hl=en&sa=X&d=6810495655654990921&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b_OjdD0in8cKtAZUwFCBn2Z&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "2 new citations to articles by Hong Jin Kang", "Quang-Cuong Bui - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "Thanh Le-Cong - new related research", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "Towards Automatic Vulnerability Management in Open-Source Software", "first_label": ["Vulnerabilities"], "second_label": [], "data": "X Yang - 2025\nOpen-source software (OSS) is the foundation of the digital world, yet its transparent \ndevelopment model exposes systems to security vulnerabilities. Effective OSS \nvulnerability management requires two fundamental capabilities: vulnerability \ndetection (VD), which identifies vulnerable code in repositories before exploitation, \nand vulnerability-fix detection (VFD), which monitors upstream commits to identify \nsecurity patches. While VD prevents vulnerable code from reaching production, VFD\nCites: Multi-granularity detector for vulnerability fixes", "link": "https://scholar.google.com/scholar_url?url=https://mspace.lib.umanitoba.ca/server/api/core/bitstreams/8255ecc7-6cca-4302-b7df-0bdd6d4cf36d/content&hl=en&sa=X&d=2998770214157551647&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b83Aan4rzYfxaHuYDZyKpAs&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=1&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "2 new citations to articles by Richard Fang", "3 new citations to articles by Xin ZHOU", "2 new citations to articles by Hong Jin Kang", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "LLMS vs Agents: Repairing Inter-Procedural Vulnerabilities in Real-World Code", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": ["Repair", "Agent"], "data": "J De La Rosa - 2025\nAutomated vulnerability repair has emerged as a promising alternative to traditional \nrule-based and static-analysis approaches, which are often limited by high false-\npositive rates and incomplete coverage. This thesis investigates the comparative \neffectiveness of fine-tuned large language models (LLMs) and agentic systems for \nboth vulnerability detection and repair, with a particular focus on inter-procedural \nvulnerabilities. Given that commonly used datasets (eg, BigVul, CVEFixes) suffer\nCites: Comparison of static application security testing tools and large\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/ce4c2a94e7d3a81ffceec53535d12515/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=en&sa=X&d=8899563170682244215&ei=0Yi3aMSrK4vWieoPpLaliQQ&scisig=AAZF9b9mBLioXp8dRdSaJzmp-C9v&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=2&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "3 new citations to articles by Xin ZHOU", "3 new citations to articles by Thanh Le-Cong"]}
{"title": "Smart Contract Vulnerability Detection using Prompt Engineering with Reasoning Models", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection", "Reasoning"], "data": "DT Anh Duc, HG Le, HP Chu-Nguyen, VH Pham- International Conference on, 2025\nThe increasing deployment of smart contracts has drawn significant attention to the \nurgent need for robust and scalable vulnerability detection techniques to mitigate \nsubstantial financial risks associated with their immutable nature on blockchain \nplatforms. This paper introduces structured reasoning prompts using agent-role \nchaining for vulnerability detection that utilizes model capacity to enhance smart \ncontract security through zero-shot and structured prompt engineering without fine\nCites: Large language model for vulnerability detection: Emerging results", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/11133716/11133719/11133797.pdf&hl=en&sa=X&d=4960414742317175043&ei=04i3aPmrBabT6rQPheXy8Aw&scisig=AAZF9b_IfZspn7oTQi1b8ULDalOG&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU"]}
{"title": "Breakpoint: Stress-testing systems-level reasoning in LLM agents", "first_label": ["LLM", "Software Testing"], "second_label": ["Agent", "Reasoning"], "data": "K Hariharan, U Girit, Z Wang, J Andreas- Second Conference on Language Modeling\nBenchmarks for large language models (LLMs) have predominantly assessed short-\nhorizon, localized reasoning. Existing long-horizon suites (eg SWE-lancer) rely on \nmanually curated issues, so expanding or tuning difficulty demands expensive\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DGQNojroNCH&hl=en&sa=X&d=12717619982875316177&ei=0oi3aJX9J_voieoP_t6J0Qg&scisig=AAZF9b-kI0RWDA2GmhCVc8jMru93&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "LLM-based Multi-Agents System Attack via Continuous Optimization with Discrete Efficient Search", "first_label": ["LLM"], "second_label": ["Agent", "Search"], "data": "W Yu, K Hu, T Pang, C Du, M Lin, M Fredrikson- Second Conference on Language Modeling\nLarge Language Model (LLM)-based Multi-Agent Systems (MAS) have demonstrated \nremarkable capability in complex tasks. However, emerging evidence indicates \nsignificant security vulnerabilities within these systems. In this paper, we introduce", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DED5diyzc1C&hl=en&sa=X&d=6490174810296450350&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b8Kua5VPBNgaEu8Asmv2qhJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage", "first_label": ["LLM"], "second_label": ["Agent"], "data": "Y Nie, Z Wang, Y Yu, X Wu, X Zhao, ND Bastian- Second Conference on Language\nRecent studies have discovered that large language models (LLM) may be``fooled''to \noutput private information, including training data, system prompts, and personally \nidentifiable information, under carefully crafted adversarial prompts. Existing red", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DWIfns41MAb&hl=en&sa=X&d=10259244939618973360&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_hZGCbTyeMBbRwykX_3qB9&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Cost of Thinking: Increased Jailbreak Risk in Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "F Yang- arXiv preprint arXiv:2508.10032, 2025\nThinking mode has always been regarded as one of the most valuable modes in \nLLMs. However, we uncover a surprising and previously overlooked phenomenon: \nLLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate 9", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.10032%3F&hl=en&sa=X&d=228728027978274532&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_8cp_f6WJxImUVOGsK56O4&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Gradient Surgery for Safe LLM Fine-Tuning", "first_label": ["LLM"], "second_label": [], "data": "B Yi, J Li, B Zhang, L Nie, T Li, T Huang, Z Liu- arXiv preprint arXiv:2508.07172, 2025\nFine-tuning-as-a-Service introduces a critical vulnerability where a few malicious \nexamples mixed into the user's fine-tuning dataset can compromise the safety \nalignment of Large Language Models (LLMs). While a recognized paradigm frames", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.07172&hl=en&sa=X&d=10352052812613128560&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_-UmhxxnT9CjoTuoEHfdfM&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers", "first_label": [], "second_label": [], "data": "Q Long, Y Deng, L Gan, W Wang, SJ Pan- Second Conference on Language Modeling\nDense retrieval systems have been widely used in various NLP applications. \nHowever, their vulnerabilities to potential attacks have been underexplored. This \npaper investigates a novel attack scenario where the attackers aim to mislead the", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DRsnxggqW4l&hl=en&sa=X&d=12899846075017282411&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b-mEhIe0IKyrwlGeXONAF8S&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Rethinking Safety in LLM Fine-tuning: An Optimization Perspective", "first_label": ["LLM"], "second_label": [], "data": "M Kim, JM Kwak, L Alssum, B Ghanem, P Torr- arXiv preprint arXiv, 2025\nFine-tuning language models is commonly believed to inevitably harm their safety, \nie, refusing to respond to harmful user requests, even when using harmless datasets, \nthus requiring additional safety measures. We challenge this belief through", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.12531&hl=en&sa=X&d=5765344796588629336&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b824RRmgPcbwax6O3_MzYe2&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "first_label": ["LLM"], "second_label": [], "data": "U Maskey, S Yadav, M Dras, U Naseem- arXiv preprint arXiv:2508.11290, 2025\nLLMs increasingly exhibit over-refusal behavior, where safety mechanisms cause \nmodels to reject benign instructions that superficially resemble harmful content. This \nphenomena diminishes utility in production applications that repeatedly rely on", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.11290&hl=en&sa=X&d=4418065858919549735&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_590TBu9nutdVdOeEXPNS5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "M Phute, R Balakrishnan- arXiv preprint arXiv:2508.08521, 2025\nVision Language Models (VLMs) are increasingly being used in a broad range of \napplications, bringing their security and behavioral control to the forefront. While \nexisting approaches for behavioral control or output redirection, like system", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.08521%3F&hl=en&sa=X&d=3389705349380814363&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_q3Y2_Sm63cdpM1VXa1B78&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Unlearning Mirage: A Dynamic Framework for Evaluating LLM Unlearning", "first_label": ["LLM"], "second_label": [], "data": "RS Shah, J Huang, K Murugesan, N Baracaldo- Second Conference on Language\nUnlearning in Large Language Models (LLMs) aims to enhance safety, mitigate \nbiases, and comply with legal mandates, such as the right to be forgotten. However, \nexisting unlearning methods are brittle: minor query modifications, such as multi-hop", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DexW2SFJK4H&hl=en&sa=X&d=15889298358367187024&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b91gDuml9z_8j5vYaea6CHM&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement", "first_label": ["LLM"], "second_label": ["Reasoning"], "data": "Y Shen, Z Huang, Z Guo, Y Liu, G Chen, R Yin- arXiv preprint arXiv, 2025\nThe rapid advancement of large language models (LLMs) has driven their adoption \nacross diverse domains, yet their ability to generate harmful content poses significant \nsafety challenges. While extensive research has focused on mitigating harmful\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2508.20151&hl=en&sa=X&d=341990395345011620&ei=04i3aNCNEo6i6rQPoYecoQg&scisig=AAZF9b_juyWDx8SfrGpexsWLbpK6&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Proof Engineering in Logika: Synergistically Integrating Automated and Semi-automated Program Verification", "first_label": ["Verification"], "second_label": [], "data": "S Hallerstedel, JH Robby, J Belt- Formal Methods for Industrial Critical Systems: 30th, 2025\nRecent work on industry-capable program verification tech-nology has emphasized \nthe need for greater predictability in the per-formance of SMT-based automated \nverification approaches. Moreover, foundational limitations of SMT necessitate some\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DfaSBEQAAQBAJ%26oi%3Dfnd%26pg%3DPA39%26ots%3DDrL0CvAkUA%26sig%3DnAGspZVrV0FKadLi2qKIcB7oTL8&hl=en&sa=X&d=16976827903832918058&ei=0oi3aPWRGoTXieoP4q2S-QU&scisig=AAZF9b-ObQYOvjtr5uyfYtNAsjl_&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=3&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Cutting the Root of Hallucination: Structural Trimming for Vulnerability Mitigation in Code LLMs", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": [], "data": "Y Zhang- Second Conference on Language Modeling\nWe introduce a structural perspective on hallucinations in code-generating language \nmodels, framing them as causality anchors in syntax graphs that trigger cascading \nsemantic errors and latent security flaws. This work is the first to systematically\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DdU4Y2sNfJ2&hl=en&sa=X&d=5135073847180578167&ei=04i3aNjJHoXR6rQP27rbgAs&scisig=AAZF9b9_JbzLhczFealkFDO90_BW&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "CALLME: Call Graph Augmentation with Large Language Models for Javascript", "first_label": ["LLM", "Static Analysis"], "second_label": ["Graph"], "data": "M Wang, K Pei, A Solar-Lezama- Second Conference on Language Modeling\nBuilding precise call graphs for Javascript programs is a fundamental build-ing block \nfor many important software engineering and security applications such as bug \ndetection, program repair, and refactoring. However, resolving dynamic calls using\n\u00a0\nThis message was sent by Google Scholar because you're following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DxZi2rMUcAO&hl=en&sa=X&d=14269582805361277416&ei=0Yi3aKCoOrOk6rQPk-uVkAE&scisig=AAZF9b86fszM_QdUkoZEP6__MxBh&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Overcoming model optimization bugs in machine learning frameworks", "first_label": ["Bug"], "second_label": [], "data": "H Guan - 2025\nWith the widespread adoption of machine learning models on resource-constrained \nplatforms such as mobile and IoT devices, ensuring efficient and reliable model \noptimization has become increasingly critical. However, optimization processes can \nintroduce model optimization bugs (MOBs), which threaten system reliability and \nremain largely unexplored and under-addressed in current research. This thesis \nprovides the first comprehensive study of MOBs, characterizing their patterns and\nCites: Large language model guided protocol fuzzing", "link": "https://scholar.google.com/scholar_url?url=https://espace.library.uq.edu.au/view/UQ:04c9239/s4643454_phd_thesis.pdf&hl=en&sa=X&d=2154320929638342570&ei=0oi3aML0C4fC6rQPgLuo8Qc&scisig=AAZF9b8KaFUKNMIry9_lIY9kGkYK&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}
{"title": "Solidifying AI-enabled Systems via Verification, Testing, and Monitoring", "first_label": ["Verification", "Software Testing"], "second_label": [], "data": "X Xie - 2025\nIn the information era, Artificial Intelligence (AI)-enabled systems are ubiquitous due \nto their scalability, cost-effectiveness, and the availability of diverse operation \nenvironments. They can empower the automation of tasks, analysis of data, and \nrecognition of diverse patterns. However, their trustworthiness, eg, fairness, safety, \nand truthfulness, also encounter enormous challenges when deployed in the real-\nworld environment. Therefore, how to ensure and enhance the trustworthiness of the\nCites: Coverage-based greybox fuzzing as markov chain\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you're following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert", "link": "https://scholar.google.com/scholar_url?url=https://ualberta.scholaris.ca/bitstreams/136ecc1c-1190-4ced-a982-58e0747c0a8a/download&hl=en&sa=X&d=10164661893200334674&ei=0oi3aML0C4fC6rQPgLuo8Qc&scisig=AAZF9b_hnlld3YIOkPfTdkSRdBYi&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["2 new citations to articles by Abhik Roychoudhury"]}

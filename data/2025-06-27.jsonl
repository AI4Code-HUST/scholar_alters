{"title": "Your Agent Can Defend Itself against Backdoor Attacks", "first_label": [], "second_label": ["Agent"], "data": "L Changjiang, L Jiacheng, C Bochuan, C Jinghui\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDespite their growing adoption across domains, large language model (LLM)-\npowered agents face significant security risks from backdoor attacks during training \nand fine-tuning. These compromised agents can subsequently be manipulated to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.08336&hl=en&sa=X&d=9281612337423238890&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b8EnIakTDvZQGsyv1duPVA5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "System Prompt Extraction Attacks and Defenses in Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "BC Das, MH Amini, Y Wu\\xc2\\xa0- arXiv preprint arXiv:2505.23817, 2025\nThe system prompt in Large Language Models (LLMs) plays a pivotal role in guiding \nmodel behavior and response generation. Often containing private configuration \ndetails, user roles, and operational instructions, the system prompt has become an\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23817&hl=en&sa=X&d=8359490024520200495&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b9kewGVoQUmVLeek21BFko2&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Adversarial Preference Learning for Robust LLM Alignment", "first_label": ["LLM"], "second_label": [], "data": "Y Wang, P Wang, C Xi, B Tang, J Zhu, W Wei, C Chen\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nModern language models often rely on Reinforcement Learning from Human \nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to \nadversarial attacks due to three key limitations:(1) the inefficiency and high cost of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.24369%3F&hl=en&sa=X&d=11526575995055803369&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b_ilBi9AMSJoKPXxHc_hWg5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression", "first_label": ["LLM"], "second_label": [], "data": "Y Li, S Ahn, H Jiang, AH Abdi, Y Yang, L Qiu\\xc2\\xa0- arXiv preprint arXiv:2506.12707, 2025\nLarge language models (LLMs) have achieved widespread adoption across \nnumerous applications. However, many LLMs are vulnerable to malicious attacks \neven after safety alignment. These attacks typically bypass LLMs' safety guardrails by\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.12707&hl=en&sa=X&d=8418273368991399970&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b8AOJ5PQ2S2DYr5nVOYpqTl&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge", "first_label": ["LLM"], "second_label": [], "data": "S Li, C Xu, J Wang, X Gong, C Chen, J Zhang, J Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated remarkable intelligence across \nvarious tasks, which has inspired the development and widespread adoption of LLM-\nas-a-Judge systems for automated model testing, such as red teaming and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.09443&hl=en&sa=X&d=2052322274134661445&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b9u9cyhYi06xXXt37CLZpdh&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Syntactic paraphrase-based synthetic data generation for backdoor attacks against Chinese language models", "first_label": ["LLM"], "second_label": ["Generation"], "data": "M Hu, Y Yang, D Pan, Z Guo, L Xiao, D Lin, S Zhao\\xc2\\xa0- Information Fusion, 2025\nAbstract Language Models (LMs) have shown significant advancements in various \nNatural Language Processing (NLP) tasks. However, recent studies indicate that \nLMs are particularly susceptible to malicious backdoor attacks, where attackers\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S156625352500449X&hl=en&sa=X&d=11939540759867308872&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b_omHEFDqgnSwVne_pfKwnY&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Through the Stealth Lens: Rethinking Attacks and Defenses in RAG", "first_label": [], "second_label": [], "data": "S Choudhary, N Palumbo, A Hooda, KD Dvijotham\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRetrieval-augmented generation (RAG) systems are vulnerable to attacks that inject \npoisoned passages into the retrieved set, even at low corruption rates. We show that \nexisting attacks are not designed to be stealthy, allowing reliable detection and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.04390&hl=en&sa=X&d=15468735482968103516&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b8u2OUJdUy7skXRySPRbZz3&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin", "first_label": ["LLM"], "second_label": [], "data": "S Yang, Q Zhang, Y Liu, Y Huang, X Jia, K Ning, J Yao\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) are vulnerable to safety risks during fine-tuning, \nwhere small amounts of malicious or harmless data can compromise safeguards. In \nthis paper, building on the concept of alignment direction--defined by the weight\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.08473%3F&hl=en&sa=X&d=15059004078714992755&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b-cmQJQ2nP1jAOJlM9iLuaf&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Arms Race in Deep Learning: A Survey of Backdoor Defenses and Adaptive Attacks", "first_label": [], "second_label": [], "data": "X Mo, N Sun, LY Zhang, W Luo, S Gao, Y Xiang\\xc2\\xa0- Pacific-Asia Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDeep neural networks (DNNs) face a growing threat from backdoor attacks, which \nembed hidden malicious functionalities triggered by specific inputs. This survey \nexamines the escalating arms race between backdoor defenses and increasingly\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-96-8183-9_24&hl=en&sa=X&d=4258756518287065675&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b80XRQuOtBuq86IspbsVv3h&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective", "first_label": ["Software Testing"], "second_label": ["Agent", "Reasoning"], "data": "J Kim, B Shin, J Chung, M Rhu\\xc2\\xa0- arXiv preprint arXiv:2506.04301, 2025\nLarge-language-model (LLM)-based AI agents have recently showcased impressive \nversatility by employing dynamic reasoning, an adaptive, multi-step process that \ncoordinates with external tools. This shift from static, single-turn inference to agentic\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.04301&hl=en&sa=X&d=3072272730677106970&ei=FdpcaKDUB8zM6rQPxJ--yQI&scisig=AAZF9b-t8XUoljKdp6JjL-hJnilf&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Efficient and Robust Security-Patch Localization for Disclosed OSS Vulnerabilities with Fine-Tuned LLMs in an Industrial Setting", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Localization"], "data": "D Ran, L Li, L Zhu, Y Cao, L Zhao, X Tan, G Liang\\xe2\\x80\\xa6 - 2025\nSecurity-patch localization, which links disclosed vulnerabilities in open-source \nsoftware (OSS) to corresponding patches, has become a practical technique to \nmitigate the risk of OSS vulnerabilities in a timely manner. While existing approaches \nextensively focus on estimating the correlation between individual patches and \nCommon Vulnerabilities and Exposures (CVEs), they often fail to address two major \nindustrial requirements that make a tool of security-patch localization desirable in\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaVulcurator: a vulnerability-fixing commit detector\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dezhi-ran.com/publication/fse25-taper/fse25-taper.pdf&hl=en&sa=X&d=6373928856750801940&ei=FNpcaN_TPLvM6rQP4I3MmQ4&scisig=AAZF9b8zgVXhveOMkc9bDSIkHaN0&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["1 new citation to articles by Bach Le", "David Lo - new related research", "1 new citation to articles by Thanh Le-Cong", "1 new citation to articles by Hong Jin Kang"]}
{"title": "Evaluating Large Language Models for Code Review", "first_label": ["LLM", "Code Review", "Code"], "second_label": [], "data": "U Cihan, A \\xc4\\xb0\\xc3\\xa7\\xc3\\xb6z, V Haratian, E T\\xc3\\xbcz\\xc3\\xbcn\\xc2\\xa0- arXiv preprint arXiv:2505.20206, 2025\nContext: Code reviews are crucial for software quality. Recent AI advances have \nallowed large language models (LLMs) to review and fix code; now, there are tools \nthat perform these reviews. However, their reliability and accuracy have not yet been\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.20206&hl=en&sa=X&d=15665831858314714693&ei=FNpcaN3VObXCieoPx6Gs6Q4&scisig=AAZF9b_nX5EjD1ohxo-XwLW6C4w4&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Hong Jin Kang - new related research"]}
{"title": "A First Look at Bugs in LLM Inference Engines", "first_label": ["LLM", "Bug"], "second_label": [], "data": "M Liu, S Zhong, W Bi, Y Zhang, Z Chen, Z Chen, X Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language model-specific inference engines (in short as\\\\emph {LLM inference \nengines}) have become a fundamental component of modern AI infrastructure, \nenabling the deployment of LLM-powered applications (LLM apps) across cloud and\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.09713&hl=en&sa=X&d=1829118811417287869&ei=FNpcaN3VObXCieoPx6Gs6Q4&scisig=AAZF9b9PFvonW4DF-GngDik42RDE&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "D Huang, G Zeng, J Dai, M Luo, H Weng, Y Qing, H Cui\\xe2\\x80\\xa6\\xc2\\xa0- Forty-second International\\xc2\\xa0\\xe2\\x80\\xa6\nAs large language models (LLMs) play an increasingly important role in code \ngeneration, enhancing both correctness and efficiency has become crucial. Current \nmethods primarily focus on correctness, often overlooking efficiency. To address this\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D8bgaOg1TlZ&hl=en&sa=X&d=14915156030897718878&ei=FdpcaJrDCcy8ieoPq9flOA&scisig=AAZF9b9r-VHALFoUjA9M0GRk-50r&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=0&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Can Large Language Models Understand Intermediate Representations in Compilers?", "first_label": ["LLM"], "second_label": [], "data": "H Jiang, J Zhu, Y Wan, B Fang, H Zhang, R Jin, Q Guan\\xc2\\xa0- Forty-second International\\xc2\\xa0\\xe2\\x80\\xa6\nIntermediate Representations (IRs) play a critical role in compiler design and \nprogram analysis, yet their comprehension by* Large Language Models*(LLMs) \nremains underexplored. In this paper, we present an explorative empirical study\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DzDieh7VWfN&hl=en&sa=X&d=13871753026296865586&ei=FdpcaJrDCcy8ieoPq9flOA&scisig=AAZF9b_eyCuQfdrPuOD8zGlx9lsG&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Hong Jin Kang - new related research"]}
{"title": "Automatic Qiskit Code Refactoring Using Large Language Models", "first_label": ["LLM", "Code"], "second_label": [], "data": "JM Su\\xc3\\xa1rez, LM Bibb\\xc3\\xb3, J Bogado, A Fernandez\\xc2\\xa0- arXiv preprint arXiv:2506.14535, 2025\nAs quantum software frameworks evolve, developers face increasing challenges in \nmaintaining compatibility with rapidly changing APIs. In this work, we present a novel \nmethodology for refactoring Qiskit code using large language models (LLMs). We\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.14535&hl=en&sa=X&d=4707492068215582196&ei=FdpcaJrDCcy8ieoPq9flOA&scisig=AAZF9b8ijIjD2XcJlduBt3t6lA4m&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "first_label": ["LLM", "Code"], "second_label": [], "data": "F Yao, Z Wang, L Liu, J Cui, L Zhong, X Fu, H Mai\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode generation with large language models (LLMs), often termed vibe coding, is \nincreasingly adopted in production but fails to ensure code quality, particularly in \nsecurity (eg, SQL injection vulnerabilities) and maintainability (eg, missing type\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.22704&hl=en&sa=X&d=13720297852295427429&ei=FNpcaLK0OJPN6rQPht3jyA8&scisig=AAZF9b9qU1hJff-ona5ynwmi0w3O&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": ["Detection"], "data": "X Li, J Ding, C Peng, B Zhao, X Gao, H Gao, X Gu\\xc2\\xa0- arXiv preprint arXiv:2506.05692, 2025\nThe code generation capabilities of large language models (LLMs) have emerged as \na critical dimension in evaluating their overall performance. However, prior research \nhas largely overlooked the security risks inherent in the generated code. In this work\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.05692%3F&hl=en&sa=X&d=5492650506064335063&ei=FNpcaLK0OJPN6rQPht3jyA8&scisig=AAZF9b_uwkPsOhWMHWjfdGwYFezG&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=1&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "KRAKEN: Program-Adaptive Parallel Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "A ZHOU, H HUANG, C ZHANG - 2025\nDespite numerous advances, most existing fuzzers still require more than 24 hours to \nthoroughly test the target programs to achieve satisfactory code coverage or bug \ndetection results [7, 32, 42, 64]. Recently, as cloud-based computing and multicore\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://seviezhou.github.io/files/kraken.pdf&hl=en&sa=X&d=6944396394299119143&ei=FdpcaJSYBYOuieoPsICzsQc&scisig=AAZF9b9QILhkfV1b1x87AW8NkoEo&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation", "first_label": ["Code"], "second_label": ["Reasoning"], "data": "G Yang, Y Zhou, X Chen, W Zheng, X Hu, X Zhou, D Lo\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nTrustworthy evaluation methods for code snippets play a crucial role in neural code \ngeneration. Traditional methods, which either rely on reference solutions or require \nexecutable test cases, have inherent limitation in flexibility and scalability. The recent\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.19502%3F&hl=en&sa=X&d=16683183957421599983&ei=FdpcaLq3ArWP6rQP6eGTwQY&scisig=AAZF9b-QxYjEGmNASUU4YZLrynZ2&oi=scholaralrt&hist=ylyK0_8AAAAJ:9162293956065397449:AAZF9b-XLYhOpfmwS_vhRk8lFc-r&html=&pos=0&folt=art", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new articles"]}
{"title": "An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks", "first_label": ["LLM"], "second_label": [], "data": "X Zhou, K Kim, T Zhang, M Weyssow, LF Gomes\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) and other automated techniques have been \nincreasingly used to support software developers by generating software artifacts \nsuch as code snippets, patches, and comments. However, accurately assessing the\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles written by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.20854%3F&hl=en&sa=X&d=14069950555695470496&ei=FdpcaLq3ArWP6rQP6eGTwQY&scisig=AAZF9b_ITtaPav10Hi5bAp132EBc&oi=scholaralrt&hist=ylyK0_8AAAAJ:9162293956065397449:AAZF9b-XLYhOpfmwS_vhRk8lFc-r&html=&pos=1&folt=art", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new articles"]}
{"title": "Roz\\xc5\\xa1\\xc3\\xad\\xc5\\x99en\\xc3\\xad manu\\xc3\\xa1ln\\xc3\\xadho fuzz testov\\xc3\\xa1n\\xc3\\xad o minimalizaci fuzz test\\xc5\\xaf pro EVM chytr\\xc3\\xa9 kontrakty", "first_label": ["Fuzzing", "Software Testing"], "second_label": [], "data": "Y Naoki - 2025\nTestov\\xc3\\xa1n\\xc3\\xad bezpe\\xc4\\x8dnosti smart kontrakt\\xc5\\xaf \\xc4\\x8del\\xc3\\xad zna\\xc4\\x8dn\\xc3\\xbdm v\\xc3\\xbdzv\\xc3\\xa1m p\\xc5\\x99i lad\\xc4\\x9bn\\xc3\\xad a anal\\xc3\\xbdze \nprobl\\xc3\\xa9m\\xc5\\xaf odhalen\\xc3\\xbdch p\\xc5\\x99i ru\\xc4\\x8dn\\xc4\\x9b \\xc5\\x99\\xc3\\xadzen\\xc3\\xa9m fuzzingu. Tento p\\xc5\\x99\\xc3\\xadstup k testov\\xc3\\xa1n\\xc3\\xad sice \n\\xc3\\xba\\xc4\\x8dinn\\xc4\\x9b vyhled\\xc3\\xa1v\\xc3\\xa1 zranitelnosti, ale v\\xc3\\xbdsledn\\xc3\\xa9 testovac\\xc3\\xad p\\xc5\\x99\\xc3\\xadpady \\xc4\\x8dasto obsahuj\\xc3\\xad stovky \nnebo tis\\xc3\\xadce krok\\xc5\\xaf, tak\\xc5\\xbee lad\\xc4\\x9bn\\xc3\\xad a anal\\xc3\\xbdza p\\xc5\\x99\\xc3\\xad\\xc4\\x8din jsou \\xc4\\x8dasov\\xc4\\x9b n\\xc3\\xa1ro\\xc4\\x8dn\\xc3\\xa9 a slo\\xc5\\xbeit\\xc3\\xa9. V t\\xc3\\xa9to \npr\\xc3\\xa1ci p\\xc5\\x99edstavujeme Fuzz Shrink v ru\\xc4\\x8dn\\xc4\\x9b \\xc5\\x99\\xc3\\xadzen\\xc3\\xa9m fuzzingu, nov\\xc3\\xbd p\\xc5\\x99\\xc3\\xadstup k \nautomatick\\xc3\\xa9 minimalizaci testovac\\xc3\\xadch p\\xc5\\x99\\xc3\\xadpad\\xc5\\xaf p\\xc5\\x99i zachov\\xc3\\xa1n\\xc3\\xad jejich schopnosti vyvolat\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaModel-based whitebox fuzzing for program binaries\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://dspace.cvut.cz/bitstream/handle/10467/124063/F8-BP-2025-Yoshida-Naoki-yoshinao-thesis.pdf%3Fsequence%3D-1&hl=en&sa=X&d=14273270919586062683&ei=FdpcaPb3A8mQ6rQP97eusAo&scisig=AAZF9b9xCavycDMxlg9QZRdrmL0u&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["1 new citation to articles by Abhik Roychoudhury"]}

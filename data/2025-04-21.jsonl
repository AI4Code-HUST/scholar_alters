{"title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language", "first_label": ["LLM"], "second_label": [], "data": "Z Pourbahman, F Rajabi, M Sadeghi, O Ghahroodi\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThis paper presents a comprehensive evaluation framework for aligning Persian \nLarge Language Models (LLMs) with critical ethical dimensions, including safety, \nfairness, and social norms. It addresses the gaps in existing LLM evaluation\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12553&hl=en&sa=X&d=4417832566291085341&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeZpv_cWtEy5wRhj9-GUvaCL&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "LLM Agentic Workflow for Automated Vulnerability Detection and Remediation in Infrastructure-as-Code", "first_label": ["Vulnerabilities", "LLM", "Code"], "second_label": ["Detection", "Agent"], "data": "D Toprani, VK Madisetti\\xc2\\xa0- IEEE Access, 2025\nThis paper presents a multi-agent, AI-driven strategy employing Large Language \nModels (LLMs), retrieval-augmented generation, and a continuously updated \nknowledge base for the detection and remediation of security vulnerabilities in cloud\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10965635.pdf&hl=en&sa=X&d=14219617279893468661&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeY0OWR9oTONDgJw1vBZERtQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "MOS: Towards Effective Smart Contract Vulnerability Detection through Mixture-of-Experts Tuning of Large Language Models", "first_label": ["Vulnerabilities", "Smart Contracts", "LLM"], "second_label": ["Detection"], "data": "H Yuan, L Yu, Z Huang, J Zhang, J Lu, S Cheng\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSmart contract vulnerabilities pose significant security risks to blockchain systems, \npotentially leading to severe financial losses. Existing methods face several \nlimitations:(1) Program analysis-based approaches rely on predefined patterns\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12234&hl=en&sa=X&d=18342111694248986737&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeYxjMCYW94Z84LR9l2FgOVA&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research", "David Lo - new related research", "Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research"]}
{"title": "Progent: Programmable Privilege Control for LLM Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "T Shi, J He, Z Wang, L Wu, H Li, W Guo, D Song\\xc2\\xa0- arXiv preprint arXiv:2504.11703, 2025\nLLM agents are an emerging form of AI systems where large language models \n(LLMs) serve as the central component, utilizing a diverse set of tools to complete \nuser-assigned tasks. Despite their great potential, LLM agents pose significant\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11703&hl=en&sa=X&d=12482402578480328066&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaean05whtIJoVCq_QGoLxbOp&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets", "first_label": ["Code"], "second_label": [], "data": "Y Zhang, Y Zhou, T Li, M Li, S Hu, W Luo, LY Zhang\\xc2\\xa0- arXiv preprint arXiv:2504.11990, 2025\nTransfer learning from pre-trained encoders has become essential in modern \nmachine learning, enabling efficient model adaptation across diverse tasks. \nHowever, this combination of pre-training and downstream adaptation creates an\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11990&hl=en&sa=X&d=85981534375043028&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeaIWCWz5yJjA6srUGUx7MJ1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "LLM-Based Automation of COSMIC Functional Size Measurement from Use Cases", "first_label": ["LLM"], "second_label": [], "data": "G De Vito, S Di Martino, F Ferrucci, C Gravino\\xe2\\x80\\xa6\\xc2\\xa0- IEEE Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCOmmon Software Measurement International Consortium (COSMIC) Functional \nSize Measurement is a method widely used in the software industry to quantify user \nfunctionality and measure software size, which is crucial for estimating development\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://fpalomba.github.io/pdf/Journals/J77.pdf&hl=en&sa=X&d=10857265925413407421&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeYYT0Q2dK2V6Z2SyX7XCGnO&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "first_label": ["LLM"], "second_label": [], "data": "H Li, L Lv, H Cao, Z Liu, Z Yan, Y Wang, Y Tian, Y Li\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models are increasingly used in scientific domains, especially for \nmolecular understanding and analysis. However, existing models are affected by \nhallucination issues, resulting in errors in drug design and utilization. In this paper\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12314&hl=en&sa=X&d=16221581542421106752&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeZ2t8zQ7arpvf-w8nLaU72M&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models via Simple Program Execution", "first_label": [], "second_label": ["Reasoning"], "data": "S Sun, CP Hsieh, F Ladhak, E Arakelyan, SA Serano\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nComplex reasoning tasks often rely on the ability to consistently and accurately apply \nsimple rules across incremental steps, a foundational capability which we term\" level-\n0\" reasoning. To systematically evaluate this capability, we introduce L0-Bench, a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.22832&hl=en&sa=X&d=3581934578808224243&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaebs81HMSqHZ7PO00Lpfwg7f&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Exploring Expert Failures Improves LLM Agent Tuning", "first_label": ["LLM"], "second_label": ["Agent"], "data": "LC Lan, A Bai, M Cheng, R Wang, CJ Hsieh, T Zhou\\xc2\\xa0- arXiv preprint arXiv:2504.13145, 2025\nLarge Language Models (LLMs) have shown tremendous potential as agents, \nexcelling at tasks that require multiple rounds of reasoning and interactions. \nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.13145&hl=en&sa=X&d=5973194658665503274&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaebJYiIEPQgkQxEuxx20hrR1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Unlocking LLM Repair Capabilities in Low-Resource Programming Languages Through Cross-Language Translation and Multi-Agent Refinement", "first_label": ["LLM"], "second_label": ["Repair", "Generation", "Agent"], "data": "W Luo, JW Keung, B Yang, TF Bissyande, H Tian, B Le\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advances in leveraging LLMs for APR have demonstrated impressive \ncapabilities in fixing software defects. However, current LLM-based approaches \npredominantly focus on mainstream programming languages like Java and Python\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.22512&hl=en&sa=X&d=792507422776186435&ei=Dl8FaMQDiKjqtA-e-tPxDg&scisig=AFWwaeaia2cBVyezePoHp34PQrku&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "RePurr: Automated Repair of Block-Based Learners' Programs", "first_label": ["APR"], "second_label": ["Repair"], "data": "S Schweikl, G Fraser\\xc2\\xa0- arXiv preprint arXiv:2504.12445, 2025\nProgramming is increasingly taught using block-based languages like Scratch. While \nthe use of blocks prevents syntax errors, learners can still make semantic mistakes, \nrequiring feedback and help. As teachers may be overwhelmed by help requests in a\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12445&hl=en&sa=X&d=5861734098693562368&ei=DV8FaOvqMdqs6rQPm9in0A8&scisig=AFWwaebOrtzRJxLZ0-ZeCfHzSyFL&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AFWwaeZjZJIN-8rhXrY_SmCmGQgD&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Data-efficient LLM Fine-tuning for Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "W Lv, X Xia, SJ Huang\\xc2\\xa0- arXiv preprint arXiv:2504.12687, 2025\nLarge language models (LLMs) have demonstrated significant potential in code \ngeneration tasks. However, there remains a performance gap between open-source \nand closed-source models. To address this gap, existing approaches typically\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12687&hl=en&sa=X&d=6940466691581933502&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaebqIJGq8Ev50ketPcg_AT4z&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Malicious and Unintentional Disclosure Risks in Large Language Models for Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "R Rabin, S McGregor, N Judd\\xc2\\xa0- arXiv preprint arXiv:2503.22760, 2025\nThis paper explores the risk that a large language model (LLM) trained for code \ngeneration on data mined from software repositories will generate content that \ndiscloses sensitive information included in its training data. We decompose this risk\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.22760&hl=en&sa=X&d=2222291530016894533&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeYB_Q37ozqUTTfsXXQWnm2a&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=1&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "M Liu, J Li, Y Wang, X Du, Z Ou, Q Chen, B An, Z Wei\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDespite recent advances in Large Language Models (LLMs) for code generation, the \nquality of LLM-generated code still faces significant challenges. One significant issue \nis code repetition, which refers to the model's tendency to generate structurally\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12608&hl=en&sa=X&d=15153918291755445762&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeYsVUw-cmkKiP_s8TFLK3te&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research"]}
{"title": "Automatic High-Level Test Case Generation using Large Language Models", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "NB Hasan, MA Islam, JY Khan, S Senjik, A Iqbal\\xc2\\xa0- arXiv preprint arXiv:2503.17998, 2025\nWe explored the challenges practitioners face in software testing and proposed \nautomated solutions to address these obstacles. We began with a survey of local \nsoftware companies and 26 practitioners, revealing that the primary challenge is not\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17998&hl=en&sa=X&d=2435370287861060433&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeb0yzfBEwJ9p6RT6YLxQ0vY&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=4&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "The Role of Code Readability in Large Language Model Code Summarization", "first_label": ["LLM", "Code"], "second_label": [], "data": "B Szalontai, G Szalay, T M\\xc3\\xa1rton, A Sike, P M\\xc3\\xa1tray\\xe2\\x80\\xa6\\xc2\\xa0- Advances in Signal\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have demonstrated good performance in various \nsoftware engineering tasks, including code summarization-explaining what a code \nsnippet does. In this paper, we argue that the readability of the input code is crucial to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Sergey-Yurish/publication/390829629_Advances_in_Signal_Processing_and_Artificial_Intelligence_Proceedings_of_the_7th_International_Conference_on_Advances_in_Signal_Processing_and_Artificial_Intelligence_8-10_April_2025_Innsbruck_Austria/links/67ff7640bfbe974b23aabbd6/Advances-in-Signal-Processing-and-Artificial-Intelligence-Proceedings-of-the-7th-International-Conference-on-Advances-in-Signal-Processing-and-Artificial-Intelligence-8-10-April-2025-Innsbruck-Austri.pdf%23page%3D149&hl=en&sa=X&d=15809233705507088473&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeZEzLO7Hst629xs81y1rnQB&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=5&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "VLM-Fuzz: Vision Language Model Assisted Recursive Depth-first Search Exploration for Effective UI Testing of Android Apps", "first_label": ["Fuzzing", "Software Testing"], "second_label": ["Search"], "data": "BF Demissie, YN Tun, LK Shar, M Ceccato\\xc2\\xa0- arXiv preprint arXiv:2504.11675, 2025\nTesting Android apps effectively requires a systematic exploration of the app's \npossible states by simulating user interactions and system events. While existing \napproaches have proposed several fuzzing techniques to generate various text\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11675&hl=en&sa=X&d=1988321903897613415&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeYt1LWAYgg-W7YeQueDvS05&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "LLM Benchmarking with LLaMA2: Evaluating Code Development Performance Across Multiple Programming Languages", "first_label": ["LLM", "Code"], "second_label": [], "data": "P Diehl, N Nader, M Moraru, SR Brandt\\xc2\\xa0- arXiv preprint arXiv:2503.19217, 2025\nThe rapid evolution of large language models (LLMs) has opened new possibilities \nfor automating various tasks in software development. This paper evaluates the \ncapabilities of the Llama 2-70B model in automating these tasks for scientific\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.19217&hl=en&sa=X&d=5827678979979105458&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeY1u3h3xWZXb06HdZoeUxgD&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=7&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "OpDiffer: LLM-Assisted Opcode-Level Differential Testing of Ethereum Virtual Machine", "first_label": ["LLM", "Code", "Software Testing", "Ethereum"], "second_label": [], "data": "J Ma, N He, J Xi, M Xing, H Wang, Y Gao, Y Yue\\xc2\\xa0- arXiv preprint arXiv:2504.12034, 2025\nAs Ethereum continues to thrive, the Ethereum Virtual Machine (EVM) has become \nthe cornerstone powering tens of millions of active smart contracts. Intuitively, \nsecurity issues in EVMs could lead to inconsistent behaviors among smart contracts\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12034&hl=en&sa=X&d=2329490431218418572&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaeaSjhhDKTQsvfZCNVqe-NDA&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=8&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "StarFlow: Generating Structured Workflow Outputs From Sketch Images", "first_label": [], "second_label": [], "data": "P Bechard, C Wang, A Abaskohi, J Rodriguez, C Pal\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWorkflows are a fundamental component of automation in enterprise platforms, \nenabling the orchestration of tasks, data processing, and system integrations. \nDespite being widely used, building workflows can be complex, often requiring\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.21889%3F&hl=en&sa=X&d=10715069633377113969&ei=DV8FaIiIOseUywTViqN4&scisig=AFWwaebjzq-h_QbWChR82TkIiotu&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=9&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair", "first_label": ["APR"], "second_label": ["Repair"], "data": "X Cai, L Jiang\\xc2\\xa0- arXiv preprint arXiv:2504.01523, 2025\nAutomated Program Repair (APR) aims to enhance software reliability by \nautomatically generating bug-fixing patches. Recent work has improved the state-of-\nthe-art of APR by fine-tuning pre-trained large language models (LLMs), such as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.01523&hl=en&sa=X&d=9461921674059523797&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaebGEkQCoR5RF1sA1OAr_sCy&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Reasoning with LLMs for Zero-Shot Vulnerability Detection", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Reasoning"], "data": "A Zibaeirad, M Vieira\\xc2\\xa0- arXiv preprint arXiv:2503.17885, 2025\nAutomating software vulnerability detection (SVD) remains a critical challenge in an \nera of increasingly complex and interdependent software systems. Despite \nsignificant advances in Large Language Models (LLMs) for code analysis, prevailing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17885&hl=en&sa=X&d=16151102993317123730&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaeaU1duZG60YHiU0D1fyotku&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs", "first_label": [], "second_label": ["Repair"], "data": "B Yang, H Tian, J Ren, S Jin, Y Liu, F Liu, B Le\\xc2\\xa0- arXiv preprint arXiv:2503.21710, 2025\nRepository-level software repair faces challenges in bridging semantic gaps \nbetween issue descriptions and code patches. Existing approaches, which mostly \ndepend on large language models (LLMs), suffer from semantic ambiguities, limited\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.21710&hl=en&sa=X&d=14440947091517176734&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaebBCSzZGxuJdEhcg1ET5UW8&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "first_label": ["LLM", "Code"], "second_label": ["Agent", "Reasoning"], "data": "A Wei, T Suresh, J Cao, N Kannan, Y Wu, K Yan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInductive program synthesis, or programming by example, requires synthesizing \nfunctions from input-output examples that generalize to unseen inputs. While large \nlanguage model agents have shown promise in programming tasks guided by\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.23145&hl=en&sa=X&d=7322261732284042571&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaeaAH4_AfF9MmAImwDIX-Jtw&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=5&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Hong Jin Kang - new related research"]}
{"title": "Automated Generation of Commit Messages in Software Repositories", "first_label": ["Commit Message"], "second_label": ["Generation"], "data": "VK Palakodeti, A Heydarnoori\\xc2\\xa0- arXiv preprint arXiv:2504.12998, 2025\nCommit messages are crucial for documenting software changes, aiding in program \ncomprehension and maintenance. However, creating effective commit messages is \noften overlooked by developers due to time constraints and varying levels of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12998&hl=en&sa=X&d=6245184434680463467&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaebHM5kQHptiUgSb4LG5t0up&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=6&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Hong Jin Kang - new related research"]}
{"title": "RepoChat: An LLM-Powered Chatbot for GitHub Repository Question-Answering", "first_label": ["LLM"], "second_label": [], "data": "S Abedu, L Menneron, SH Khatoonabadi, E Shihab\nSoftware repositories contain a wealth of data about the software development \nprocess, such as source code, documentation, issue tracking, and commit histories. \nHowever, accessing and extracting meaningful insights from these data is\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://das.encs.concordia.ca/pdf/abedu2025repochat.pdf&hl=en&sa=X&d=2858684303751462479&ei=DV8FaKzsNea16rQPhbTBmQc&scisig=AFWwaeYP_W9ydFqp7VcvK8oGHT45&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=7&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Xin ZHOU - new related research"]}
{"title": "ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges", "first_label": [], "second_label": ["Agent"], "data": "M Lupinacci, F Blefari, F Romeo, FA Pironti, A Furfaro\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe growing and evolving landscape of cybersecurity threats necessitates the \ndevelopment of supporting tools and platforms that allow for the creation of realistic \nIT environments operating within virtual, controlled settings as Cyber Ranges (CRs). \nCRs can be exploited for analyzing vulnerabilities and experimenting with the \neffectiveness of devised countermeasures, as well as serving as training \nenvironments for building cyber security skills and abilities for IT operators. This\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLlm agents can autonomously exploit one-day vulnerabilities\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12143&hl=en&sa=X&d=6262779132126183344&ei=DV8FaMKhNPOy6rQP1tr_uA8&scisig=AFWwaeY6gGP0HBLj3a8QyoT54mgW&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AFWwaebib5Pw9QKWi9BJ6ThKDwc5&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["2 new citations to articles by Richard Fang"]}
{"title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Agent", "Exploit"], "data": "Y Zhu, A Kellermann, A Gupta, P Li, R Fang, R Bindu\\xe2\\x80\\xa6\nLLM agents have become increasingly sophisticated, especially in the realm of \ncybersecurity. Researchers have shown that LLM agents can exploit real-world \nvulnerabilities when given a description of the vulnerability and toy capturethe-flag \nproblems. However, these agents still perform poorly on real-world vulnerabilities \nthat are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, \nwe show that teams of LLM agents can exploit real-world, zero-day vulnerabilities\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLlm agents can autonomously exploit one-day vulnerabilities\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://yuxuan18.github.io/assets/pub/teams_of_agents.pdf&hl=en&sa=X&d=17805203639256023308&ei=DV8FaMKhNPOy6rQP1tr_uA8&scisig=AFWwaeZPgyEb6hfnMeK2_WHYixOu&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AFWwaebib5Pw9QKWi9BJ6ThKDwc5&html=&pos=1&folt=cit", "author": ["Richard Fang"], "ref": ["2 new citations to articles by Richard Fang"]}
{"title": "DALO-APR: LLM-based automatic program repair with data augmentation and loss function optimization", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "S Wang, L Lu, S Qiu, Q Tian, H Lin\\xc2\\xa0- The Journal of Supercomputing, 2025\nAutomatic program repair (APR) has made significant strides with the advent of large \nlanguage models (LLMs) such as T5 and CodeT5. However, LLM-based APR \nmodels may rely on repetitive repair patterns due to limited training data diversity\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11227-025-07102-3&hl=en&sa=X&d=10480458481806281342&ei=Dl8FaMPJAZWrieoPsaHL0Qg&scisig=AFWwaeZgISIeIz-af4x6cYmuf3Oy&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AFWwaeZADCuvrSiGaZ1pge7b9bMB&html=&pos=0&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "first_label": ["LLM", "Software Testing"], "second_label": ["Reasoning"], "data": "Z Xu, J Ding, Y Lou, K Zhang, D Gong, Y Li\\xc2\\xa0- arXiv preprint arXiv:2504.12312, 2025\nLarge Language Models (LLMs) have achieved significant progress in language \nunderstanding and reasoning. Evaluating and analyzing their logical reasoning \nabilities has therefore become essential. However, existing datasets and\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12312&hl=en&sa=X&d=3686854897724625534&ei=Dl8FaMPJAZWrieoPsaHL0Qg&scisig=AFWwaeYYrn479PXqXdOr9pYNerog&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AFWwaeZADCuvrSiGaZ1pge7b9bMB&html=&pos=5&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Analyzing the impact of prompt engineering on efficiency, code quality, and security in CRUD application development", "first_label": ["Code"], "second_label": [], "data": "KAA Shanuka, J Wijayanayake, K Vidanage\\xc2\\xa0- 2025 5th International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThis research evaluates the capabilities of Large Language Models (LLMs) in \ngenerating CRUD applications using Python Flask framework, focusing on code \nquality, security, and UI design. The study analyzes five prominent LLMs: Claude 3.5 \nSonnet, Gemini, GitHub Copilot, GPT-4, and Perplexity, through automated static \ncode review tools including Code Factor, Codacy, and Code Scene. The evaluation \nreveals consistently high performance across models, with GitHub Copilot and\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRefining chatgpt-generated code: Characterizing and mitigating\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10963005/&hl=en&sa=X&d=2778415604758727878&ei=DV8FaOn-L5POieoPmpz8kA0&scisig=AFWwaeZXZiQlkYfvWGihi6SwO_gr&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AFWwaeagPGjpoAfsUTlpD2ZsD6em&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["1 new citation to articles by Thanh Le-Cong", "3 new citations to articles by Bach Le"]}
{"title": "White-box structure analysis of pre-trained language models of code for effective attacking", "first_label": ["Code"], "second_label": [], "data": "C Liu, X Ren, Y Xue\\xc2\\xa0- Information and Software Technology, 2025\nContext: Pre-trained language models of code (PLMs-C for short) have dramatically \nimproved the state-of-the-art on various programming language processing tasks. \nObjective: Due to these well-performed models being easily disturbed by slight\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584925000692&hl=en&sa=X&d=10831422306869385650&ei=Dl8FaKatA_mJ6rQP1KeGsAg&scisig=AFWwaeaYut7czcHGT0TluKBywRaC&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets", "first_label": ["LLM", "Code"], "second_label": [], "data": "H Jelodar, M Meymani, R Razavi-Far\\xc2\\xa0- arXiv preprint arXiv:2503.17502, 2025\nLarge language models (LLMs) and transformer-based architectures are \nincreasingly utilized for source code analysis. As software systems grow in \ncomplexity, integrating LLMs into code analysis workflows becomes essential for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17502%3F&hl=en&sa=X&d=13932223716190867094&ei=Dl8FaKatA_mJ6rQP1KeGsAg&scisig=AFWwaeaTqZQLIFPcoWmVZF9MztiA&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=2&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "CBGF: Callback Coverage Guided Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "H Hwang, D Moon\\xc2\\xa0- IEEE Access, 2025\nCallback functions are widely used across programming languages, libraries, and \noperating systems. While offering flexible software design, these mechanisms \nintroduce inherently complex execution flows that can serve as potential attack\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10965623.pdf&hl=en&sa=X&d=3130441307734811739&ei=Dl8FaKatA_mJ6rQP1KeGsAg&scisig=AFWwaeZoIEFQ_i6JdoG1gUz5tvcM&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=4&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models", "first_label": ["LLM", "Code Review", "Code"], "second_label": [], "data": "HY Lin, C Liu, H Gao, P Thongtanunam, C Treude\\xc2\\xa0- arXiv preprint arXiv:2503.16167, 2025\nState-of-the-art large language models (LLMs) have demonstrated impressive code \ngeneration capabilities but struggle with real-world software engineering tasks, such \nas revising source code to address code reviews, hindering their practical use. Code\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16167&hl=en&sa=X&d=13987149689206803549&ei=Dl8FaKatA_mJ6rQP1KeGsAg&scisig=AFWwaeZRD8EhNW4NSPG4ypaH_WWQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=7&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Bridging the Gap: A Comparative Study of Academic and Developer Approaches to Smart Contract Vulnerabilities", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": [], "data": "F Salzano, L Marchesi, CK Antenucci, S Scalabrino\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn this paper, we investigate the strategies adopted by Solidity developers to fix \nsecurity vulnerabilities in smart contracts. Vulnerabilities are categorized using the \nDASP TOP 10 taxonomy, and fixing strategies are extracted from GitHub commits in \nopen-source Solidity projects. Each commit was selected through a two-phase \nprocess: an initial filter using natural language processing techniques, followed by \nmanual validation by the authors. We analyzed these commits to evaluate adherence\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSmart contract development: Challenges and opportunities\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.12443&hl=en&sa=X&d=12875937947700763884&ei=DV8FaOC5N8mpieoPm-WPkAQ&scisig=AFWwaeY2zvbbEZ9Bl7QpT3Ieq1wC&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AFWwaebZb4G2z_XAHxtUtGUOv8go&html=&pos=1&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le"]}
{"title": "Web3 Multimedia Applications: Under the Impact of Decentralization", "first_label": [], "second_label": [], "data": "H Wu, M Abdallah, Y Chi, L Lin, W Cai\\xc2\\xa0- ACM Transactions on Multimedia Computing\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nTraditional multimedia applications employ a combination of a variety of media \nelements, such as text, graphics, images, audio, animation, and video, to enhance \ninformation expression and user experience [52, 147]. As such, Web3 multimedia \napplications can play a crucial role in the Web3 ecosystem by providing an attractive \nand user-friendly online space. Indeed, the utilization of Web3 multimedia \napplications in various application domains, such as art, games, and social media\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaSmart contract development: Challenges and opportunities\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://faculty.washington.edu/weicaics/paper/papers/HaoWACLC2025.pdf&hl=en&sa=X&d=3268395530192580864&ei=DV8FaOC5N8mpieoPm-WPkAQ&scisig=AFWwaeaA8doHAMnpAXrImwGUv_Vs&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AFWwaebZb4G2z_XAHxtUtGUOv8go&html=&pos=2&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le"]}

{"title": "Mono: Is Your\" Clean\" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond", "first_label": ["Vulnerabilities"], "second_label": [], "data": "Z Gao, J Zhou, B Zhang, Y He, C Zhang, Y Cui, H Wang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe quantity and quality of vulnerability datasets are essential for developing deep \nlearning solutions to vulnerability-related tasks. Due to the limited availability of \nvulnerabilities, a common approach to building such datasets is analyzing security\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03651&hl=en&sa=X&d=5611261347115113824&ei=n9ZEaMbjKL2W6rQPjI6v-QY&scisig=AAZF9b8ZqDz-8m4J-1VYjUzHamnv&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research", "Xin ZHOU - new related research", "David Lo - new related research", "2 new citations to articles by Hong Jin Kang", "Quang-Cuong Bui - new related research"]}
{"title": "Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and LLM-Guided Reinforcement Learning", "first_label": ["APR", "LLM"], "second_label": ["Repair", "Reasoning"], "data": "X Tang, J Klein, TF Bissyand\\xc3\\xa9\\xc2\\xa0- arXiv preprint arXiv:2506.03921, 2025\nSeveral closed-source LLMs have consistently outperformed open-source \nalternatives in program repair tasks, primarily due to their superior reasoning \ncapabilities and extensive pre-training. This paper introduces Repairity, a novel three\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03921&hl=en&sa=X&d=5236192915434052745&ei=n9ZEaPeQLqalieoP4J3H2A8&scisig=AAZF9b_Pyh0Or56hIIZbIHm5nDC2&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=0&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Hong Jin Kang - new related research", "6 new citations to articles by Abhik Roychoudhury", "Thanh Le-Cong - new related research", "David Lo - new related research", "Bach Le - new related research", "Quang-Cuong Bui - new related research"]}
{"title": "Empirical Evaluation of Generalizable Automated Program Repair with Large Language Models", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "V Campos, R Shariffdeen, A Ulges, Y Noller\\xc2\\xa0- arXiv preprint arXiv:2506.03283, 2025\nAutomated Program Repair (APR) proposes bug fixes to aid developers in \nmaintaining software. The state of the art in this domain focuses on using LLMs, \nleveraging their strong capabilities to comprehend specifications in natural language\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03283&hl=en&sa=X&d=200998896991037433&ei=n9ZEaPeQLqalieoP4J3H2A8&scisig=AAZF9b_9W2A-NQAXxkYzgRHwtFh8&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Hong Jin Kang - new related research", "6 new citations to articles by Abhik Roychoudhury", "2 new citations to articles by Xin ZHOU", "Thanh Le-Cong - new related research", "David Lo - new related research", "Bach Le - new related research", "2 new citations to articles by Hong Jin Kang", "Quang-Cuong Bui - new related research"]}
{"title": "EduFuncSum: a function-wise progressive transformer for code summarization in undergraduate programming education", "first_label": ["Code"], "second_label": [], "data": "Y Rong, M Xu, R Li, L Xin, W Bao\\xc2\\xa0- Journal of King Saud University Computer and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAutomatic code summarization holds significant promise for enhancing program \ncomprehension in undergraduate programming education. Traditional end-to-end \nsummarization models generate isolated descriptions for individual functions, but\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s44443-025-00075-6&hl=en&sa=X&d=4849396417901382170&ei=n9ZEaPeQLqalieoP4J3H2A8&scisig=AAZF9b-ti903CWiOCqr3nE930Xeg&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=3&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Across Programming Language Silos: A Study on Cross-Lingual Retrieval-augmented Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "Q Zhu, J Cao, X Chen, Y Lu, H Lin, X Han, L Sun\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCurrent research on large language models (LLMs) with retrieval-augmented code \ngeneration (RACG) mainly focuses on single-language settings, leaving cross-\nlingual effectiveness and security unexplored. Multi-lingual RACG systems are\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03535&hl=en&sa=X&d=14758371788704078162&ei=n9ZEaPeQLqalieoP4J3H2A8&scisig=AAZF9b-dcuDDIx5KNKIkulVIbOZM&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=4&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Hong Jin Kang - new related research", "David Lo - new related research"]}
{"title": "Selective Code Generation for Functional Guarantees", "first_label": ["Code"], "second_label": ["Generation"], "data": "J Jeong, T Kim, S Park\\xc2\\xa0- arXiv preprint arXiv:2505.13553, 2025\nLarge language models (LLMs) show human-level performance and their \nspecialized descendants, code generation models, play core roles in solving \ncomplex tasks, including mathematical reasoning and software development. On the\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13553%3F&hl=en&sa=X&d=10618887701165566141&ei=n9ZEaPeQLqalieoP4J3H2A8&scisig=AAZF9b_uNrZGuMhLrQI8_eSOoX4J&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=5&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Understanding API Usage and Testing: An Empirical Study of C Libraries", "first_label": ["Software Testing"], "second_label": [], "data": "A Zaki, C Cadar - 2025\nFor library developers, understanding how their Application Programming Interfaces \n(APIs) are used in the! eld can be invaluable. Knowing how clients are using their \nAPIs allows for data-driven decisions on prioritising bug reports, feature requests\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://srg.doc.ic.ac.uk/files/papers/libprobe-ease-25.pdf&hl=en&sa=X&d=9565695415974607721&ei=n9ZEaLHQL5LXieoP7MLq8Ak&scisig=AAZF9b_PG-CbhRwjkRcs04GlpNl9&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "How Far Are We from Predicting Missing Modalities with Foundation Models?", "first_label": [], "second_label": [], "data": "G Ke, Y Xie, X Wang, G Chao, B Wang, S He\\xc2\\xa0- arXiv preprint arXiv:2506.03530, 2025\nMultimodal foundation models have demonstrated impressive capabilities across \ndiverse tasks. However, their potential as plug-and-play solutions for missing \nmodality prediction remains underexplored. To investigate this, we categorize \nexisting approaches into three representative paradigms, encompassing a total of 42 \nmodel variants, and conduct a comprehensive evaluation in terms of prediction \naccuracy and adaptability to downstream tasks. Our analysis reveals that current\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAgentic AI Software Engineers: Programming with Trust\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03530&hl=en&sa=X&d=9230336641897558877&ei=n9ZEaIeDJrXCieoP_vz9iAU&scisig=AAZF9b9sU3X9gSWk098qVoWg622h&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["6 new citations to articles by Abhik Roychoudhury"]}
{"title": "Evaluating Large Language Models: A Systematic Review of Efficiency, Applications, and Future Directions", "first_label": ["LLM"], "second_label": [], "data": "Y Saleh, M Abu Talib, Q Nasir, F Dakalbab\\xc2\\xa0- Frontiers in Computer Science\nLarge language models, the innovative breakthrough taking the world by storm, have \nbeen applied in several fields, such as medicine, education, finance, and law. \nMoreover, large language models can integrate into those fields through their \nabilities in natural language processing, text generation, question answering, and \nseveral other use cases that benefit human interactions and decision-making. \nFurthermore, it is imperative to acknowledge the differences involved with large\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Repair of Programs from Large Language Models\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1523699/pdf&hl=en&sa=X&d=4093044423653206135&ei=n9ZEaIeDJrXCieoP_vz9iAU&scisig=AAZF9b984VNAB2RS62Pr80qOBNRH&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=2&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["6 new citations to articles by Abhik Roychoudhury"]}
{"title": "BOOSTING SYMBOLIC EXECUTION FOR VULNERABILITY DETECTION", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "H TU - 2025\nSoftware systems written by humans tend to be unreliable and insecure, hence bugs \nor vulnerabilities in them are inevitable. Symbolic execution has shown considerable \npotential in detecting diverse types of software bugs and also vulnerabilities that \nhave severe security implications. However, existing symbolic execution engines still \nsuffer from at least three fundamental limitations in memory modeling, path \nexploration, and structured input generation, which significantly impede existing\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://haoxintu.github.io/files/smu_thesis.pdf&hl=en&sa=X&d=3920640297285392836&ei=n9ZEaIeDJrXCieoP_vz9iAU&scisig=AAZF9b8T7RfLhP6QYSt_dUsdr9rq&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=3&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["6 new citations to articles by Abhik Roychoudhury"]}
{"title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "first_label": ["LLM", "Code"], "second_label": [], "data": "N Oza, I Govil, P Gupta, D Khandelwal, D Garg\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLLMs have been extensively used for the task of automated code generation. In this \nwork, we examine the applicability of LLMs for the related but relatively unexplored \ntask of code-equivalence checking, ie, given two programs, whether they are \nfunctionally equivalent or not. This is an important problem since benchmarking code \nequivalence can play a critical role in evaluating LLM capabilities for tasks such as \ncode re-writing and code translation. Towards this end, we present CETBench-Code\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Program Repair\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.04019&hl=en&sa=X&d=6260632840952853971&ei=n9ZEaIeDJrXCieoP_vz9iAU&scisig=AAZF9b8WRAQxAGhy7eO0km5XHnT0&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=5&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["6 new citations to articles by Abhik Roychoudhury", "David Lo - new related research"]}
{"title": "Real-time tracking railway intruders using multiple-agent cooperated large language models with edge stream processing engine", "first_label": ["LLM"], "second_label": ["Agent"], "data": "W Huang, X Deng\\xc2\\xa0- Journal of Network and Computer Applications, 2025\nTracking intruders is crucial for ensuring safe railway operations globally, particularly \nin high-speed railway systems. Traditional methods either rely on post-processing on \ncloud platforms or suffer from limited analytical capabilities on edge devices. \nAlthough large language models (LLMs) have shown great potential to support \ngeneral intelligence, challenges remain for edge devices in accurately and timely \ntracking of intruders along railway lines. This study proposes a novel method that\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaExploring Parameter-Efficient Fine-Tuning Techniques for Code\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S1084804525001286&hl=en&sa=X&d=6722182859117754225&ei=n9ZEaI6wK8y8ieoP89mfwQY&scisig=AAZF9b9KFn1ShKpuwlH3TF6FIP3m&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=1&folt=cit", "author": ["Xin ZHOU"], "ref": ["2 new citations to articles by Xin ZHOU"]}
{"title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning", "first_label": ["Vulnerabilities"], "second_label": [], "data": "L Chen, X Han, L Shen, J Bai, KF Wong\\xc2\\xa0- arXiv preprint arXiv:2506.03850, 2025\nHarmful fine-tuning (HFT), performed directly on open-source LLMs or through Fine-\ntuning-as-a-Service, breaks safety alignment and poses significant threats. Existing \nmethods aim to mitigate HFT risks by learning robust representation on alignment \ndata or making harmful data unlearnable, but they treat each data sample equally, \nleaving data vulnerability patterns understudied. In this work, we reveal that certain \nsubsets of alignment data are consistently more prone to forgetting during HFT\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03850&hl=en&sa=X&d=1311313001180143322&ei=n9ZEaJTiH_iJ6rQPkqSymQY&scisig=AAZF9b8lDPXobanPi9JuXZr1M1uy&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["1 new citation to articles by Richard Fang"]}
{"title": "Improving LLM-Based Fault Localization with External Memory and Project Context", "first_label": ["LLM", "Fault Localization"], "second_label": ["Localization"], "data": "I Yeo, D Ryu, J Baik\\xc2\\xa0- arXiv preprint arXiv:2506.03585, 2025\nFault localization, the process of identifying the software components responsible for \nfailures, is essential but often time-consuming. Recent advances in Large Language \nModels (LLMs) have enabled fault localization without extensive defect datasets or\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03585&hl=en&sa=X&d=6976682449842682697&ei=n9ZEaL2mIZLXieoP7MLq8Ak&scisig=AAZF9b9yWjShUCNBl2qtWzrOq6a8&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "\\xe2\\x80\\x9cWe've Met Some Problems\\xe2\\x80\\x9d: Developers' Issues with Privacy-Preserving Computation Techniques on Stack Overflow", "first_label": [], "second_label": [], "data": "P K\\xc3\\xbchtreiber, S Heimermann, S Schillinger\\xe2\\x80\\xa6\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSoftware developers must adhere to privacy regulations and apply privacy principles. \nHowever, developers may not be privacy experts and hence are likely to encounter \nissues when choosing, applying, and implementing the corresponding Privacy\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-92882-6_4&hl=en&sa=X&d=12170750192969461747&ei=n9ZEaKzGJOWs6rQPpf7X-Aw&scisig=AAZF9b-2VAlnCZ3VFsZgIT-IQ1ir&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=4&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "AI Powered Code Generation: The Next Era of Automated Software Development", "first_label": ["Code"], "second_label": ["Generation"], "data": "H Verma, S Choudhary, K Pandey\\xc2\\xa0- International Journal of Sciences and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn the ever-changing software engineering paradigm, artificial intelligence has \nbecome the revolutionary driver, changing the method by which code is generated, \noptimized, & deployed. The paper delves into the emergence of AI-enabled code\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.ijsci.com/index.php/home/article/download/167/143&hl=en&sa=X&d=5827903349629826673&ei=n9ZEaKzGJOWs6rQPpf7X-Aw&scisig=AAZF9b-P4SV97piIohZWqUsrg_jj&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "Fault Localisation and Repair for DL Systems: An Empirical Study with LLMs", "first_label": ["LLM"], "second_label": ["Repair"], "data": "J Kim, N Humbatova, G Jahangirova, S Yoo, P Tonella\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nNumerous Fault Localisation (FL) and repair techniques have been proposed to \naddress faults in Deep Learning (DL) models. However, their effectiveness in \npractical applications remains uncertain due to the reliance on pre-defined rules\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03396&hl=en&sa=X&d=2993196625760325493&ei=n9ZEaJ--HqalieoP4J3H2A8&scisig=AAZF9b8BjoHxJ7ENnG0lxho0lpNU&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=2&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}
{"title": "Evaluatiing the efficacy of LLM Safety Solutions: The Palit Benchmark Dataset", "first_label": ["LLM"], "second_label": [], "data": "S Palit, D Woods\\xc2\\xa0- arXiv preprint arXiv:2505.13028, 2025\nLarge Language Models (LLMs) are increasingly integrated into critical systems in \nindustries like healthcare and finance. Users can often submit queries to LLM-\nenabled chatbots, some of which can enrich responses with information retrieved\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13028%3F&hl=en&sa=X&d=1273460049730580919&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b9ByON4eaZyjRTpM1-cA_zU&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "first_label": ["LLM"], "second_label": ["Agent"], "data": "L Wang, W Wang, S Wang, Z Li, Z Ji, Z Lyu, D Wu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid advancement of Large Language Models (LLMs) has led to the \nemergence of Multi-Agent Systems (MAS) to perform complex tasks through \ncollaboration. However, the intricate nature of MAS, including their architecture and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12442&hl=en&sa=X&d=8825331941370095164&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b_j6hKF2Zy8dNMOx-tobM6E&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Agentxploit: End-to-end redteaming of black-box ai agents", "first_label": [], "second_label": ["Agent"], "data": "Z Wang, V Siu, Z Ye, T Shi, Y Nie, X Zhao, C Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe strong planning and reasoning capabilities of Large Language Models (LLMs) \nhave fostered the development of agent-based systems capable of leveraging \nexternal tools and interacting with increasingly complex environments. However\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.05849&hl=en&sa=X&d=13145039506086791936&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b_5aWzkLQOK0HBWClMmbpt1&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks", "first_label": [], "second_label": [], "data": "R Zhang, Y Shen, H Li, W Jiang, H Chen, Y Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent research highlights concerns about the trustworthiness of third-party Pre-\nTrained Language Models (PTLMs) due to potential backdoor attacks. These \nbackdoored PTLMs, however, are effective only for specific pre-defined downstream\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.11586&hl=en&sa=X&d=15157304470732077928&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b997v4XRj8-KVQslhBMZvqE&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "first_label": ["LLM"], "second_label": [], "data": "G Chen, F Song, Z Zhao, X Jia, Y Liu, Y Qiao, W Zhang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nJailbreak attacks to Large audio-language models (LALMs) are studied recently, but \nthey achieve suboptimal effectiveness, applicability, and practicability, particularly, \nassuming that the adversary can fully manipulate user prompts. In this work, we first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14103&hl=en&sa=X&d=12355582258955555442&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b84twuHlVXU1hPFciKO3AfT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Securing AI Agents with Information-Flow Control", "first_label": [], "second_label": ["Agent"], "data": "M Costa, B K\\xc3\\xb6pf, A Kolluri, A Paverd, M Russinovich\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs AI agents become increasingly autonomous and capable, ensuring their security \nagainst vulnerabilities such as prompt injection becomes critical. This paper explores \nthe use of information-flow control (IFC) to provide security guarantees for AI agents\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.23643%3F&hl=en&sa=X&d=4534448032257376326&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b-ruPZ7fbYUs0K6HirRZr2z&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World", "first_label": [], "second_label": [], "data": "J Guo, L Zhou, Z Wang, J He, Q Song, A Chen, W Jiang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn recent years, deep learning-based Monocular Depth Estimation (MDE) models \nhave been widely applied in fields such as autonomous driving and robotics. \nHowever, their vulnerability to backdoor attacks remains unexplored. To fill the gap in\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.16154%3F&hl=en&sa=X&d=5415910119131408953&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b_P0jMEgHvYQo3O-kEno08o&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "A4FL: Federated Adversarial Defense via Adversarial Training and Pruning Against Backdoor Attack", "first_label": [], "second_label": [], "data": "B Li, M Hamid, M Saleem, M Aman\\xc2\\xa0- IEEE Access, 2025\nBackdoor attacks threaten federated learning (FL) models, where malicious \nparticipants embed hidden triggers into local models during training. These triggers \ncan compromise crucial applications, such as autonomous systems, when they\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/10992684.pdf&hl=en&sa=X&d=4691259454218143835&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b-NVXh_0LVfWkgmGPYIRTtu&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "EntropyMark: Towards More Harmless Backdoor Watermark via Entropy-based Constraint for Open-source Dataset Copyright Protection", "first_label": [], "second_label": [], "data": "M Sun, R Wang, Z Zhu, L Jing, Y Guo\\xc2\\xa0- Proceedings of the Computer Vision and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nHigh-quality open-source datasets are essential for advancing deep neural \nnetworks. However, the unauthorized commercial use of these datasets has raised \nsignificant concerns about copyright protection. One promising approach is backdoor\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_EntropyMark_Towards_More_Harmless_Backdoor_Watermark_via_Entropy-based_Constraint_for_CVPR_2025_paper.pdf&hl=en&sa=X&d=3999981307599099721&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b-KVOWLvLNl_9VwbGvBpqDH&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "Z Xu, U Sanghi, M Kankanhalli\\xc2\\xa0- arXiv preprint arXiv:2505.12692, 2025\nLarge Language Models (LLMs) are increasingly deployed in interactions where \nthey are prompted to adopt personas. This paper investigates whether such persona \nconditioning affects model safety under bullying, an adversarial manipulation that\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12692%3F&hl=en&sa=X&d=3130006781540917286&ei=n9ZEaPXOLM6r6rQP3rzY-QM&scisig=AAZF9b-r43I7NUbwAxgd2MM-t4Q_&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "ETS Liu, A Wang, S Mateega, C Georgescu, D Tang\\xc2\\xa0- arXiv preprint arXiv:2505.19395, 2025\nEnsuring that large language models (LLMs) can effectively assess, detect, explain, \nand remediate software vulnerabilities is critical for building robust and secure \nsoftware systems. We introduce VADER, a human-evaluated benchmark designed\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.19395%3F&hl=en&sa=X&d=9714217024162720024&ei=n9ZEaKPIJ4qIieoPrumR8AU&scisig=AAZF9b_-FsHIQqysH23COzO04ZAM&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=2&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Accelerating Automated Program Verifiers by Automatic Proof Localization", "first_label": [], "second_label": ["Localization"], "data": "K Gopinathan, D Spiliopoulos, V Goyal, P M\\xc3\\xbcller\\xe2\\x80\\xa6\nAutomated program verifiers such as Dafny, F*, Verus, and Viper are now routinely \nused to verify real-world software. Unfortunately, the performance of the SMT solvers \nemployed by these tools is not always able to keep up with the increasing size and\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://ilyasergey.net/assets/pdf/papers/axolocl-cav25.pdf&hl=en&sa=X&d=12540905409007805483&ei=n9ZEaKPIJ4qIieoPrumR8AU&scisig=AAZF9b--TR062xcgaOy_C_2kqr9N&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=4&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "CODEMENV: Benchmarking Large Language Models on Code Migration", "first_label": ["LLM", "Code"], "second_label": [], "data": "K Cheng, X Shen, Y Yang, T Wang, Y Cao, MA Ali\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown remarkable capabilities across various \nsoftware engineering tasks; however, their effectiveness in code migration, adapting \ncode to run in different environments, remains insufficiently studied. In this work, we\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00894&hl=en&sa=X&d=11203773371204290692&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b9TDVO2Og3WPkp2gR-1hZyY&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences", "first_label": ["LLM"], "second_label": ["Localization"], "data": "M Saqib, S Chakraborty, S Karmaker\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLLM generated code often contains security issues. We address two key challenges \nin improving secure code generation. First, obtaining high quality training data \ncovering a broad set of security issues is critical. To address this, we introduce a\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00419&hl=en&sa=X&d=3471320712326210844&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b-zP6iuspafIpJ9gFQhBsZE&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=1&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Rethinking the effects of data contamination in Code Intelligence", "first_label": ["Code"], "second_label": [], "data": "Z Yang, H Lin, Y He, J Xu, Z Sun, S Liu, P Wang, Z Yu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIn recent years, code intelligence has gained increasing importance in the field of \nautomated software engineering. Meanwhile, the widespread adoption of Pretrained \nLanguage Models (PLMs) and Large Language Models (LLMs) has raised concerns\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02791&hl=en&sa=X&d=13914802631128746138&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b9FbJDsSBQv15gRiA2JZNoe&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "Xin ZHOU - new related research"]}
{"title": "Towards More Effective Fault Detection in LLM-Based Unit Test Generation", "first_label": ["LLM", "Software Testing"], "second_label": ["Detection", "Generation"], "data": "G Wang, Q Xu, LC Briand, K Liu\\xc2\\xa0- arXiv preprint arXiv:2506.02954, 2025\nUnit tests play a vital role in uncovering potential faults in software. While tools like \nEvoSuite focus on maximizing code coverage, recent advances in large language \nmodels (LLMs) have shifted attention toward LLM-based test generation. However\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02954&hl=en&sa=X&d=5602972151000617790&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b-teZsjTIJdP31OgBwjNz0D&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research", "Thanh Le-Cong - new related research"]}
{"title": "A Preference-Driven Methodology for High-Quality Solidity Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "Z Peng, X Yin, C Ying, C Ni, Y Luo\\xc2\\xa0- arXiv preprint arXiv:2506.03006, 2025\nWhile Large Language Models (LLMs) have demonstrated remarkable progress in \ngenerating functionally correct Solidity code, they continue to face critical challenges \nin producing gas-efficient and secure code, which are critical requirements for real\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03006&hl=en&sa=X&d=17726484644696653033&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b-kQj0MkmtCeWsZksOvGyp1&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=4&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "EvoTaint: Incremental Static Taint Analysis of Evolving Android Apps", "first_label": [], "second_label": [], "data": "J GUO, H CAI - 2025\nEvoTaint: Incremental Static Taint Analysis of Evolving Android Apps Page 1 EvoTaint: \n\\r\\nIncremental Static Taint Analysis of Evolving Android Apps JIAWEI GUO, University at \n\\r\\nBuffalo, SUNY, USA HAIPENG CAI\\xe2\\x88\\x97, University at Buffalo, SUNY, USA In the last\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Haipeng-Cai/publication/392269894_EvoTaint_Incremental_Static_Taint_Analysis_of_Evolving_Android_Apps/links/683bc581d1054b0207f8b234/EvoTaint-Incremental-Static-Taint-Analysis-of-Evolving-Android-Apps.pdf&hl=en&sa=X&d=11445709114946387781&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b_QhW6jcbhyhVy4Rs4l2Tz-&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=5&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation", "Agent"], "data": "Q Xu, G Wang, L Briand, K Liu\\xc2\\xa0- arXiv preprint arXiv:2506.02943, 2025\nUnit testing plays a critical role in ensuring software correctness. However, writing \nunit tests manually is laborious, especially for strong typed languages like Java, \nmotivating the need for automated approaches. Traditional methods primarily rely on\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02943&hl=en&sa=X&d=9080051622854454343&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b-W733SKwSoE4FIE6fi3cJw&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "Xin ZHOU - new related research", "Thanh Le-Cong - new related research"]}
{"title": "Exploring Prompt Patterns in AI-Assisted Code Generation: Towards Faster and More Effective Developer-AI Collaboration", "first_label": ["Code"], "second_label": ["Generation"], "data": "S DiCuffa, A Zambrana, P Yadav, S Madiraju, K Suman\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe growing integration of AI tools in software development, particularly Large \nLanguage Models (LLMs) such as ChatGPT, has revolutionized how developers \napproach coding tasks. However, achieving high-quality code often requires iterative\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.01604&hl=en&sa=X&d=2802840246323475232&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b_SsZ4-K7lqqE0zYDkae7M-&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=7&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM", "first_label": ["LLM"], "second_label": ["Graph"], "data": "Y Xiang, CP Chen, L Zeng, W Yin, X Liu, H Li, W Xu\\xc2\\xa0- arXiv preprint arXiv:2506.02490, 2025\nKubernetes, a notably complex and distributed system, utilizes an array of controllers \nto uphold cluster management logic through state reconciliation. Nevertheless, \nmaintaining state consistency presents significant challenges due to unexpected\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02490&hl=en&sa=X&d=5123869491118168077&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b941F9ak4iWL2XhEVmYDE88&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=8&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "7 new citations to articles by Abhik Roychoudhury"]}
{"title": "RIMFuzz: real-time impact-aware mutation for library API fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "X Wang, L Zhao\\xc2\\xa0- Journal of King Saud University Computer and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs libraries merely expose APIs to developers rather than directly handling user \ninput, applying fuzzing to libraries requires fuzz drivers to help process fuzzer-\nprovided input and invoke APIs. To reduce manual effort and avoid reliance on\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s44443-025-00050-1&hl=en&sa=X&d=11566396351270247841&ei=KWNDaMecD7XCieoPt6N7&scisig=AAZF9b9NzGapAGT-Gt26V3wejXNn&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=9&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges", "first_label": ["LLM"], "second_label": ["Agent", "Graph"], "data": "L Muzsai, D Imolai, A Luk\\xc3\\xa1cs\\xc2\\xa0- arXiv preprint arXiv:2506.02048, 2025\nLarge Language Models (LLMs) still struggle with the structured reasoning and tool-\nassisted computation needed for problem solving in cybersecurity applications. In \nthis work, we introduce\" random-crypto\", a cryptographic Capture-the-Flag (CTF) \nchallenge generator framework that we use to fine-tune a tool-augmented Llama-3.1-\n8B with Guided Reinforcement Prompt Optimisation (GRPO), allowing the agent to \niteratively write and execute Python inside an isolated REPL. GRPO yields a+ 53\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaTeams of llm agents can exploit zero-day vulnerabilities\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02048&hl=en&sa=X&d=18189145546159441726&ei=KWNDaPDDCZLXieoPwNDK0Qk&scisig=AAZF9b_5rlalF1mfqKk70xRHnoWh&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=0&folt=cit", "author": ["Richard Fang"], "ref": ["2 new citations to articles by Richard Fang"]}
{"title": "AI in Security", "first_label": [], "second_label": [], "data": "DP Sharma, A Habibi Lashkari, MD Firoozjaei\\xe2\\x80\\xa6\\xc2\\xa0- Understanding AI in\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith the rapid advancements in Artificial Intelligence (AI), its role in cybersecurity has \nbecome increasingly significant. AI is a double-edged sword, empowering both \ndefenders and attackers in the evolving threat landscape. This chapter explores the \ngeneral framework of AI in security, emphasizing its applications in threat detection, \nnetwork security, malware analysis, and fraud prevention. It highlights the challenges \nof building a universal AI security framework, addressing issues such as data\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLlm agents can autonomously hack websites\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-91524-6_3&hl=en&sa=X&d=446135133632427158&ei=KWNDaPDDCZLXieoPwNDK0Qk&scisig=AAZF9b_AfzJEslnJSMeBMWkvu9j2&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=1&folt=cit", "author": ["Richard Fang"], "ref": ["2 new citations to articles by Richard Fang"]}
{"title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "J Kong, H Fang, X Yang, K Gao, B Chen, ST Xia\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSupervised fine-tuning (SFT) aligns large language models (LLMs) with human \nintent by training them on labeled task-specific data. Recent studies have shown that \nmalicious attackers can inject backdoors into these models by embedding triggers\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.17601%3F&hl=en&sa=X&d=14932585774719166007&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b-bRcbQ7r2VSTXuEVUTZE69&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "first_label": ["LLM"], "second_label": [], "data": "U Maskey, M Dras, U Naseem\\xc2\\xa0- arXiv preprint arXiv:2506.02442, 2025\nThis paper presents a systematic evaluation of Large Language Models'(LLMs) \nbehavior on long-tail distributed (encrypted) texts and their safety implications. We \nintroduce a two-dimensional framework for assessing LLM safety:(1) instruction\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02442&hl=en&sa=X&d=15102350404536711280&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b_4gGVOvmiHUrKGvn2E4Vhh&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift", "first_label": ["LLM"], "second_label": [], "data": "S Liang, J Liang, T Pang, C Du, A Liu, M Zhu, X Cao\\xe2\\x80\\xa6\\xc2\\xa0- Proceedings of the\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nInstruction tuning enhances large vision-language models (LVLMs) but increases \ntheir vulnerability to backdoor attacks due to their open design. Unlike prior studies in \nstatic settings, this paper explores backdoor attacks in LVLM instruction tuning\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://openaccess.thecvf.com/content/CVPR2025/papers/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.pdf&hl=en&sa=X&d=5142527777954789704&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b_Zz7b9xsyiBVRqo7nDX3YO&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "first_label": ["LLM"], "second_label": [], "data": "S Cappelletti, T Poppi, S Poppi, ZX Yong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) are increasingly evaluated on multiple-choice \nquestion answering (MCQA) tasks using* first-token probability*(FTP), which selects \nthe answer option whose initial token has the highest likelihood. While efficient, FTP\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15323%3F&hl=en&sa=X&d=5513891085179250670&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b8ts6vJnVnPKgZRZk2J59vT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based Retrieval-Augmented Recommender Systems", "first_label": ["LLM"], "second_label": [], "data": "F Nazary, Y Deldjoo, T Di Noia, E Di Sciascio\\xc2\\xa0- arXiv preprint arXiv:2505.05196, 2025\nWe present a systematic study of provider-side data poisoning in retrieval-\naugmented recommender systems (RAG-based). By modifying only a small fraction \nof tokens within item descriptions--for instance, adding emotional keywords or\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.05196&hl=en&sa=X&d=7153996738322631678&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b9XYvMXz8xtJ6dU7EDwB2B7&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Lifelong Safety Alignment for Language Models", "first_label": ["LLM"], "second_label": [], "data": "H Wang, Z Qin, Y Zhao, C Du, M Lin, X Wang, T Pang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLLMs have made impressive progress, but their growing capabilities also expose \nthem to highly flexible jailbreaking attacks designed to bypass safety alignment. \nWhile many existing defenses focus on known types of attacks, it is more critical to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.20259%3F&hl=en&sa=X&d=16834372085988299596&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b9d_59vHSVRWjToyCIlG2f5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "PS Pandey, S Simko, K Pelrine, Z Jin\\xc2\\xa0- arXiv preprint arXiv:2505.16789, 2025\nAs large language models gain popularity, their vulnerability to adversarial attacks \nremains a primary concern. While fine-tuning models on domain-specific datasets is \noften employed to improve model performance, it can introduce vulnerabilities within\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.16789%3F&hl=en&sa=X&d=9827187220639785834&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b-hRa3CIybDfceUhaRlecWD&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution", "first_label": [], "second_label": [], "data": "J Guo, X Wen, W Jiang, C Huang, J Li, H Li\\xc2\\xa0- arXiv preprint arXiv:2505.15308, 2025\nWith the widespread application of super-resolution (SR) in various fields, \nresearchers have begun to investigate its security. Previous studies have \ndemonstrated that SR models can also be subjected to backdoor attacks through\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15308&hl=en&sa=X&d=13335437674086498390&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b8WmdvUXiuzvPutlRNCBmNP&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "first_label": ["LLM"], "second_label": [], "data": "HA Shairah, HAAK Hammoud, B Ghanem, G Turkiyyah\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) are typically aligned to comply with safety guidelines \nby refusing harmful instructions. A recent attack, termed abliteration, isolates and \nsuppresses the single latent direction most responsible for refusal behavior, enabling\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.19056&hl=en&sa=X&d=3737202939224050048&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b8QKV5yRTnfm0WbZVG9RAGO&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "H Luo, S Dai, C Ni, X Li, G Zhang, K Wang, T Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDespite the rapid advancement of LLM-based agents, the reliable evaluation of their \nsafety and security remains a significant challenge. Existing rule-based or LLM-\nbased evaluators often miss dangers in agents' step-by-step actions, overlook subtle\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00641&hl=en&sa=X&d=523652018719672976&ei=KWNDaIL9F-bGieoP1sTUqQ8&scisig=AAZF9b-gq3lghzRvyDwMaj5uZok5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models", "first_label": ["LLM", "Software Testing"], "second_label": ["Detection", "Generation"], "data": "M Srinivasan, J Abdel\\xc2\\xa0- arXiv preprint arXiv:2506.03024, 2025\nLarge Language Models (LLMs) are increasingly deployed in critical domains, yet \nthey often exhibit biases inherited from training data, leading to fairness concerns. \nThis work focuses on the problem of effectively detecting fairness violations\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03024&hl=en&sa=X&d=1395882172513838978&ei=KWNDaNuDG_CuieoP583H-QE&scisig=AAZF9b-w6t_kNtySNxVBUSKCIHCp&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=4&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "3 new citations to articles by Hong Jin Kang"]}
{"title": "An LLM Agent for Functional Bug Detection in Network Protocols", "first_label": ["LLM", "Bug"], "second_label": ["Detection", "Agent"], "data": "M Zheng, C Wang, X Liu, J Guo, S Feng, X Zhang\\xc2\\xa0- arXiv preprint arXiv:2506.00714, 2025\nFunctional correctness is critical for ensuring the reliability and security of network \nprotocol implementations. Functional bugs, instances where implementations \ndiverge from behaviors specified in RFC documents, can lead to severe\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00714&hl=en&sa=X&d=14025741833580725532&ei=KWNDaNuDG_CuieoP583H-QE&scisig=AAZF9b8vkBixnPr2fRq2ZYFDK47O&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=5&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Breakpoint: Scalable evaluation of system-level reasoning in LLM code agents", "first_label": ["LLM", "Code"], "second_label": ["Agent", "Reasoning"], "data": "K Hariharan, U Girit, A Wang, J Andreas\\xc2\\xa0- arXiv preprint arXiv:2506.00172, 2025\nBenchmarks for large language models (LLMs) have predominantly assessed short-\nhorizon, localized reasoning. Existing long-horizon suites (eg SWE-bench) rely on \nmanually curated issues, so expanding or tuning difficulty demands expensive\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00172&hl=en&sa=X&d=7723120566071680437&ei=KWNDaNuDG_CuieoP583H-QE&scisig=AAZF9b-MEatAoZUO5Dw1f9NQmtaH&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=6&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "Xin ZHOU - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study", "first_label": ["LLM", "Code"], "second_label": [], "data": "C Wang, Z Yang, Y Harel, D Lo\\xc2\\xa0- arXiv preprint arXiv:2506.01825, 2025\nCode LLMs are increasingly employed in software development. However, studies \nhave shown that they are vulnerable to backdoor attacks: when a trigger (a specific \ninput pattern) appears in the input, the backdoor will be activated and cause the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.01825&hl=en&sa=X&d=16166340572421918430&ei=KWNDaLzPGZLXieoPwNDK0Qk&scisig=AAZF9b-phsc8qq-f3lOMccCIs3x-&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "3 new citations to articles by Hong Jin Kang"]}
{"title": "Semi-supervised software vulnerability assessment via code lexical and structural information fusion", "first_label": ["Vulnerabilities", "Code"], "second_label": [], "data": "W Pei, Y Huang, X Chen, G Lu, Y Liu, C Ni\\xc2\\xa0- Automated Software Engineering, 2025\nIn recent years, data-driven approaches have become popular for software \nvulnerability assessment (SVA). However, these approaches need a large amount of \nlabeled SVA data to construct effective SVA models. This process demands security\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10515-025-00526-4&hl=en&sa=X&d=12150716079684109674&ei=KWNDaLzPGZLXieoPwNDK0Qk&scisig=AAZF9b8zXJiBu6kjaWbL323Dh99f&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=4&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "Quang-Cuong Bui - new related research"]}
{"title": "Automated Vulnerability Scanning and Security Audit Framework for Maritime Systems", "first_label": ["Vulnerabilities"], "second_label": [], "data": "A Vineetha Harish - 2025\nThe maritime industry is a well-established sector, with the shipping industry dating \nback hundreds of years. The devices in this sector differ from those in the Information \nTechnology (IT) sector in their usage, operations, hardware, andprotocols. \nTechnological advancements have introduced new challenges in the sector, \nincluding the increased risk of cyber attacks. To effectively identify, respond to, and \nanalyse these threats, it is essential to understand the cyber securityvulnerabilities in\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaComparison of static application security testing tools and large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://pearl.plymouth.ac.uk/cgi/viewcontent.cgi%3Farticle%3D1554%26context%3Dsecam-theses&hl=en&sa=X&d=2189587239999303496&ei=KWNDaIr4DbWv6rQPgJOzyAw&scisig=AAZF9b-OYqJbkX0x-Ao2H8TVwJmC&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["2 new citations to articles by Bach Le", "2 new citations to articles by Thanh Le-Cong", "4 new citations to articles by Xin ZHOU"]}
{"title": "Comparative Analysis of Static Application Security Testing Tools on Real-world Java Vulnerabilities", "first_label": ["Vulnerabilities", "Software Testing"], "second_label": [], "data": "W Ansgariusson, J St\\xc3\\xa5hl - 2025\nWith the increasing complexity and scale of modern software systems, ensuring \nsoftware security is more critical than ever. As projects grow, so does the likelihood of \nvulnerabilities being introduced. Static Application Security Testing (SAST) tools \nassist developers in identifying such vulnerabilities during development. In this study, \nfive Java SAST tools (Bearer, CodeQL, Horusec, Semgrep and SonarQube) were \nevaluated based primarily on their vulnerability detection rate and execution time to\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaComparison of static application security testing tools and large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://lup.lub.lu.se/student-papers/search/publication/9189955&hl=en&sa=X&d=13866902737926222762&ei=KWNDaIr4DbWv6rQPgJOzyAw&scisig=AAZF9b-y_7Nr_GjnxK9G2GTopJNh&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=1&folt=cit", "author": ["Bach Le"], "ref": ["2 new citations to articles by Bach Le", "2 new citations to articles by Thanh Le-Cong", "4 new citations to articles by Xin ZHOU"]}
{"title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution", "first_label": ["GitHub Issue"], "second_label": [], "data": "L Guo, W Tao, R Jiang, Y Wang, J Chen, X Liu, Y Ma\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe GitHub issue resolution task aims to resolve issues reported in repositories \nautomatically. With advances in large language models (LLMs), this task has gained \nincreasing attention, and several benchmarks are proposed to evaluate the issue\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.04606&hl=en&sa=X&d=17721785651242552030&ei=KWNDaLjzCqm7ieoP1JuLsA8&scisig=AAZF9b9vdcHDALBkXHSBcEUw295J&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Evaluation of LLMs for mathematical problem solving", "first_label": ["LLM"], "second_label": [], "data": "R Wang, R Wang, Y Shen, C Wu, Q Zhou, R Chandra\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have shown impressive performance on a range of \neducational tasks, but are still understudied for their potential to solve mathematical \nproblems. In this study, we compare three prominent LLMs, including GPT-4o, \nDeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying \ncomplexities (GSM8K, MATH500, and UNSW datasets). We take a five-dimensional \napproach based on the Structured Chain-of-Thought (SCoT) framework to assess\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomatic Programming: Large Language Models and Beyond\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00309&hl=en&sa=X&d=302127274804275545&ei=KWNDaPDvEIqIieoPj_7kqQE&scisig=AAZF9b9UKjKGP2_xDdhHHPY45-tx&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "How Programming Concepts and Neurons Are Shared in Code Language Models", "first_label": ["LLM", "Code"], "second_label": [], "data": "AH Kargaran, Y Liu, F Yvon, H Sch\\xc3\\xbctze\\xc2\\xa0- arXiv preprint arXiv:2506.01074, 2025\nSeveral studies have explored the mechanisms of large language models (LLMs) in \ncoding tasks, but most have focused on programming languages (PLs) in a \nmonolingual setting. In this paper, we investigate the relationship between multiple \nPLs and English in the concept space of LLMs. We perform a few-shot translation \ntask on 21 PL pairs using two Llama-based models. By decoding the embeddings of \nintermediate layers during this task, we observe that the concept space is closer to\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaBaishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, and\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.01074&hl=en&sa=X&d=3020487052088765627&ei=KWNDaPDvEIqIieoPj_7kqQE&scisig=AAZF9b963QyJEZqju52Zn2dNXpOn&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=1&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale", "first_label": ["Vulnerabilities"], "second_label": ["Agent"], "data": "Z Wang, T Shi, J He, M Cai, J Zhang, D Song\\xc2\\xa0- arXiv preprint arXiv:2506.02548, 2025\nLarge language model (LLM) agents are becoming increasingly skilled at handling \ncybersecurity tasks autonomously. Thoroughly assessing their cybersecurity \ncapabilities is critical and urgent, given the high stakes in this domain. However, \nexisting benchmarks fall short, often failing to capture real-world scenarios or being \nlimited in scope. To address this gap, we introduce CyberGym, a large-scale and \nhigh-quality cybersecurity evaluation framework featuring 1,507 real-world\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaDirected greybox fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02548&hl=en&sa=X&d=873453502677392460&ei=KWNDaPDvEIqIieoPj_7kqQE&scisig=AAZF9b8xjaPhE5MC9fd6Nl6Oq05-&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=4&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Enhancing Software Engineering Through AI: A Comprehensive Exploration", "first_label": [], "second_label": [], "data": "N Nwasra, A Alsalemi\\xc2\\xa0- 2025 1st International Conference on Computational\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe integration of Artificial Intelligence (AI) techniques into Software Engineering \n(SE) has opened new avenues for improving software development, testing, and \nmaintenance processes. This paper explores the transformative potential of AI-driven \napproaches in addressing some of the most persistent challenges in modern \nsoftware engineering workflows. Specifically, it highlights the application of AI for \nautomated bug detection and fixing in open-source software projects, which often\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutomated Program Repair\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11013170/&hl=en&sa=X&d=17143292876618282581&ei=KWNDaPDvEIqIieoPj_7kqQE&scisig=AAZF9b_SzfEf_pbw3uYLkporTJWc&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=5&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "LIBAFLGO: Evaluating and Advancing Directed Greybox Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "E Geretto, A Jemmett, C Giuffrida, H Bos\nWhile greybox fuzzing is routinely applied in production environments with great \nsuccess, directed greybox fuzzing has struggled to gain real-world adoption\\xe2\\x80\\x94\ndespite the great (intuitive) promise and the many optimizations proposed in \nliterature. In practice, directed fuzzers struggle for three critical issues. First, popular \nimplementations build on and compare to ancient baselines, often derived from \nAFLGo. Unfortunately, none of the optimizations that are essential for performance in\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaDirected greybox fuzzing\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://download.vusec.net/papers/libaflgo_eurosp25.pdf&hl=en&sa=X&d=3588514315333430002&ei=KWNDaPDvEIqIieoPj_7kqQE&scisig=AAZF9b9_dufLgPupNfMrhFNXyzrr&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=6&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Improving LLM-Generated Code Quality with GRPO", "first_label": ["LLM", "Code"], "second_label": [], "data": "M Robeyns, L Aitchison\\xc2\\xa0- arXiv preprint arXiv:2506.02211, 2025\nLarge Language Models (LLMs) are gaining widespread use for code generation. \nRecent training procedures use execution feedback as a reward signal, typically \nfocusing on the functional correctness of the code, using unit test pass rate as a \nreward signal. However, this reward signal fails to capture notions of maintainability, \nquality and safety of the code produced. We address this under-explored area and \ndevelop a comprehensive library to quantify various aspects of code quality, and use\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaACECode: A Reinforcement Learning Framework for Aligning\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.02211&hl=en&sa=X&d=16267907864718546505&ei=KWNDaPS8DI-UywTXk47YAw&scisig=AAZF9b9DhYkiDLvHKyPP6Kqiu6SI&oi=scholaralrt&hist=ylyK0_8AAAAJ:4851239734318863641:AAZF9b8LH3KLAxOt2g9Q0Um21N4o&html=&pos=0&folt=cit", "author": ["Hong Jin Kang"], "ref": ["3 new citations to articles by Hong Jin Kang"]}
{"title": "How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face", "first_label": [], "second_label": [], "data": "A Gonz\\xc3\\xa1lez, X Franch, D Lo, S Mart\\xc3\\xadnez-Fern\\xc3\\xa1ndez\\xc2\\xa0- arXiv preprint arXiv:2506.03013, 2025\nOpen-Source Pre-Trained Models (PTMs) provide extensive resources for various \nMachine Learning (ML) tasks, yet these resources lack a classification tailored to \nSoftware Engineering (SE) needs. To address this gap, we derive a taxonomy \nencompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a \npopular open-source ML repository, Hugging Face (HF). Our repository mining study \nbegan with a systematically gathered database of PTMs from the HF API, considering\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAssessing and Advancing Benchmarks for Evaluating Large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.03013&hl=en&sa=X&d=5692275965436272309&ei=KWNDaPLLFviJ6rQP4Lq-sAo&scisig=AAZF9b8sD4_NBzDXCoEcstmgjHYl&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["4 new citations to articles by Xin ZHOU"]}
{"title": "AutoMixAlign: Adaptive Data Mixing for Multi-Task Preference Optimization in LLMs", "first_label": ["LLM"], "second_label": [], "data": "NE Corrado, J Katz-Samuels, A Devraj, H Yun\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWhen aligning large language models (LLMs), their performance on various tasks \n(such as being helpful, harmless, and honest) depends heavily on the composition of \ntheir training data. However, selecting a data mixture that achieves strong \nperformance across all tasks is challenging. Existing approaches rely on large \nablation studies, heuristics, or human intuition, but these can be prohibitively \nexpensive and suboptimal. We study this problem in the setting of preference\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaCodeultrafeedback: An llm-as-a-judge dataset for aligning large\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2506.00569&hl=en&sa=X&d=13222213487580864623&ei=KWNDaPLLFviJ6rQP4Lq-sAo&scisig=AAZF9b_uLNZfPbv1B_2xW_9oNKMN&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=3&folt=cit", "author": ["Xin ZHOU"], "ref": ["4 new citations to articles by Xin ZHOU"]}

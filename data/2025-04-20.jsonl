{"title": "The Code Barrier: What LLMs Actually Understand?", "first_label": ["LLM", "Code"], "second_label": [], "data": "SL Nikiema, J Samhi, AK Kabor\\xc3\\xa9, J Klein\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUnderstanding code represents a core ability needed for automating software \ndevelopment tasks. While foundation models like LLMs show impressive results \nacross many software engineering challenges, the extent of their true semantic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.10557&hl=en&sa=X&d=16959771222072583625&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaebY9qDBs7QZu6gOkM2zOELP&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=0&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions", "first_label": ["LLM"], "second_label": [], "data": "P Dang, D Huang, D Li, K Chen, Y Wen, Q Guo, X Hu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nOut-of-tree kernel patches are essential for adapting the Linux kernel to new \nhardware or enabling specific functionalities. Maintaining and updating these \npatches across different kernel versions demands significant effort from experienced\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.09474&hl=en&sa=X&d=1960075332266941840&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeaDdG69Y1WbgolcX9e3sh7U&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=1&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Harnessing Large Language Models for Automated Software Testing: A Leap Towards Scalable Test Case Generation", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "S Rehan, B Al-Bander, A Al-Said Ahmad\\xc2\\xa0- Electronics, 2025\nSoftware testing is critical for ensuring software reliability, with test case generation \noften being resource-intensive and time-consuming. This study leverages the Llama-\n2 large language model (LLM) to automate unit test generation for Java focal\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Amro-Al-Said-Ahmad/publication/390492852_Harnessing_Large_Language_Models_for_Automated_Software_Testing_A_Leap_Towards_Scalable_Test_Case_Generation/links/67f0483c76d4923a1af63949/Harnessing-Large-Language-Models-for-Automated-Software-Testing-A-Leap-Towards-Scalable-Test-Case-Generation.pdf&hl=en&sa=X&d=1760969295279974751&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeZwjQ9Vuuu06NZk38sjpO70&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research", "Hong Jin Kang - new related research"]}
{"title": "DALO-APR: LLM-based automatic program repair with data augmentation and loss function optimization", "first_label": ["APR", "LLM"], "second_label": ["Repair"], "data": "S Wang, L Lu, S Qiu, Q Tian, H Lin\\xc2\\xa0- The Journal of Supercomputing, 2025\nAutomatic program repair (APR) has made significant strides with the advent of large \nlanguage models (LLMs) such as T5 and CodeT5. However, LLM-based APR \nmodels may rely on repetitive repair patterns due to limited training data diversity\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11227-025-07102-3&hl=en&sa=X&d=10480458481806281342&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeZgISIeIz-af4x6cYmuf3Oy&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "LLMigrate: Transforming\" Lazy\" Large Language Models into Efficient Source Code Migrators", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Liu, J Hu, Y Shan, G Li, Y Zou, Y Dong, T Xie\\xc2\\xa0- arXiv preprint arXiv:2503.23791, 2025\nRewriting C code in Rust provides stronger memory safety, yet migrating large \ncodebases such as the 32-million-line Linux kernel remains challenging. While rule-\nbased translators (eg, C2Rust) provide accurate yet largely unsafe Rust programs\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.23791&hl=en&sa=X&d=8803774605220961224&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeY4DQ4Us-xpoOzQ65ucSN24&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=4&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Exploiting Vision-Language Models in GUI Reuse", "first_label": [], "second_label": ["Exploit"], "data": "V Niu, W Alshammari, NM Iluru, PV Teeleti, N Niu\\xe2\\x80\\xa6\nGraphical user interface (GUI) prototyping helps to clarify requirements and keep \nstakeholders engaged in software development. While contemporary approaches \nretrieve GUIs relevant to a user's query, little support exists for the actual reuse, ie, for\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://homepages.uc.edu/~niunn/papers/ICSR25.pdf&hl=en&sa=X&d=474069904190286471&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeYkLwofQitxvRb0pDXhVL4q&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=5&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Enhancing Just-In-Time Defect Prediction Models with Developer-Centric Features", "first_label": [], "second_label": [], "data": "E Guglielmi, A D'Aguanno, R Oliveto, S Scalabrino\nEnsuring high software quality in development cycles with frequent updates is \ncritical, especially in Agile and CI/CD environments. Just-In-Time Software Defect \nPrediction (JIT-SDP) has emerged as a promising solution for finding bugs early, as it\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://emanuelaguglielmi.it/assets/PDF/JIT-DefectPrediction.pdf&hl=en&sa=X&d=14899542475839495411&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeYRKhCxzvAqAugqCHbDf6im&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "1 new citation to articles by Hong Jin Kang"]}
{"title": "The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Print Injections", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Agent"], "data": "C Chen, Z Zhang, B Guo, S Ma, I Khalilov\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nA Large Language Model (LLM) powered GUI agent is a specialized autonomous \nsystem that performs tasks on the user's behalf according to high-level instructions. It \ndoes so by perceiving and interpreting the graphical user interfaces (GUIs) of\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11281&hl=en&sa=X&d=4014599373074154039&ei=dw4EaOG2LceUywTViqN4&scisig=AFWwaeYmbtsf-Gsj0mCGBCJqi4Mz&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AFWwaeYRVjm7Uk5GklbyG-nM5aLh&html=&pos=7&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Compromising LLM Driven Embodied Agents with Contextual Backdoor Attacks", "first_label": ["LLM"], "second_label": ["Agent"], "data": "A Liu, Y Zhou, X Liu, T Zhang, S Liang, J Wang, Y Pu\\xe2\\x80\\xa6\\xc2\\xa0- IEEE Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have transformed the development of embodied \nintelligence. By providing a few contextual demonstrations (such as rationales and \nsolution examples) developers can utilize the extensive internal knowledge of LLMs\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10943262/&hl=en&sa=X&d=12719951651069852610&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeZpVIpHx7agj5Gt0-h7Qtt8&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks", "first_label": [], "second_label": ["Detection"], "data": "Y Liu, Y Jia, J Jia, D Song, NZ Gong\\xc2\\xa0- arXiv preprint arXiv:2504.11358, 2025\nLLM-integrated applications and agents are vulnerable to prompt injection attacks, \nwhere an attacker injects prompts into their inputs to induce attacker-desired outputs. \nA detection method aims to determine whether a given input is contaminated by an\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11358&hl=en&sa=X&d=10818935948995424871&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeZUQoMIWfZAE7jOkiDiHvss&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "FLUE: Streamlined Uncertainty Estimation for Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "S Gao, T Gong, Z Lin, R Xu, H Zhou, J Li\\xc2\\xa0- Proceedings of the AAAI Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nUncertainty estimation is essential for practical applications such as decision-\nmaking, risk assessment, and human-AI collaboration. However, Uncertainty \nestimation in open-ended question-answering (QA) tasks presents unique\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ojs.aaai.org/index.php/AAAI/article/download/33840/35995&hl=en&sa=X&d=12392981900850419093&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeagkoBV_iGHApqFraS2DENo&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Effective defense against physically embedded backdoor attacks via clustering-based filtering", "first_label": [], "second_label": [], "data": "M Kutbi\\xc2\\xa0- Complex & Intelligent Systems, 2025\nBackdoor attacks pose a severe threat to the integrity of machine learning models, \nespecially in real-world image classification tasks. In such attacks, adversaries \nembed malicious behaviors triggered by specific patterns in the training data\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s40747-025-01876-y&hl=en&sa=X&d=328651353869364145&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeYO2KhbA5NjMz2QINArnKei&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Exploring Backdoor Attack and Defense for LLM-empowered Recommendations", "first_label": ["LLM"], "second_label": [], "data": "L Ning, W Fan, Q Li\\xc2\\xa0- arXiv preprint arXiv:2504.11182, 2025\nThe fusion of Large Language Models (LLMs) with recommender systems (RecSys) \nhas dramatically advanced personalized recommendations and drawn extensive \nattention. Despite the impressive progress, the safety of LLM-based RecSys against\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.11182&hl=en&sa=X&d=9627500529575121155&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaea16SFNnrAFZnU2vjfGSO8v&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "aiXcoder-7B-v2: Training LLMs to Fully Utilize the Long Context in Repository-level Code Completion", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "J Li, H Zhu, H Liu, X Shi, H Zong, Y Dong, K Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRepository-level code completion aims to complete code based on the long contexts \nof the repository. Existing studies extract long contexts from the repository as inputs \nand leverage Large Language Models (LLMs) to generate code. However, we reveal\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15301&hl=en&sa=X&d=15788931866516891147&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeYi_7K3zAnZWQ5kRBngFmjW&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations", "first_label": ["LLM", "Software Testing"], "second_label": ["Agent"], "data": "A Lopez-Lira\\xc2\\xa0- arXiv preprint arXiv:2504.10789, 2025\nThis paper presents a realistic simulated stock market where large language models \n(LLMs) act as heterogeneous competing trading agents. The open-source framework \nincorporates a persistent order book with market and limit orders, partial fills\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.10789&hl=en&sa=X&d=7842209209030039764&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaebE9A5vGCkYHne57SHvEHs_&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "L4: Diagnosing large-scale llm training failures via automated log analysis", "first_label": ["LLM"], "second_label": [], "data": "Z Jiang, J Huang, Z Chen, Y Li, G Yu, C Feng, Y Yang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs Large Language Models (LLMs) show their capabilities across various \napplications, training customized LLMs has become essential for modern \nenterprises. However, due to the complexity of LLM training, which requires massive\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.20263&hl=en&sa=X&d=14583207368034027437&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaebRe6Hz8JgakljjX6ZP8ib_&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training", "first_label": ["LLM"], "second_label": [], "data": "BR Bartoldson, S Venkatraman, J Diffenderfer, M Jain\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nReinforcement learning (RL) is a critical component of large language model (LLM) \npost-training. However, existing on-policy algorithms used for post-training are \ninherently incompatible with the use of experience replay buffers, which can be\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.18929&hl=en&sa=X&d=9558060532125767644&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaeYoVzcxOA7NOnZtm52EYzED&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity", "first_label": ["LLM"], "second_label": [], "data": "C Zhang, K Du, S Liu, W Kwon, X Mo, Y Wang, X Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) are widely used but expensive to run, especially as \ninference workloads grow. To lower costs, maximizing the request batch size by \nmanaging GPU memory efficiently is crucial. While PagedAttention has recently been\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.18292&hl=en&sa=X&d=7739656753816907647&ei=dw4EaPijMIio6rQPnvrT8Q4&scisig=AFWwaebgcPGXUDU8gTuYj4zLJnGn&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AFWwaeaPsVnV5GguxDkLdcyPdvnA&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "RustEvo^ 2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "L Liang, J Gong, M Liu, C Wang, G Ou, Y Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) have become pivotal tools for automating code \ngeneration in software development. However, these models face significant \nchallenges in producing version-aware code for rapidly evolving languages like\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.16922%3F&hl=en&sa=X&d=16675224894932160996&ei=dw4EaNSwKvOy6rQP1tr_uA8&scisig=AFWwaea4g_0zH5bNDjn1WSqwrFGh&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "A Nouri, J Andersson, KDJ Hornig, Z Fei, E Knabe\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAutomated Driving System (ADS) is a safety-critical software system responsible for \nthe interpretation of the vehicle's environment and making decisions accordingly. \nThe unbounded complexity of the driving context, including unforeseeable events\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.02141&hl=en&sa=X&d=11030603002805149689&ei=dw4EaNSwKvOy6rQP1tr_uA8&scisig=AFWwaeYUmC_6TmFAo66Flf543eb1&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Enhancing Code LLM Training with Programmer Attention", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Zhang, C Huang, Z Karas, DT Nguyen, K Leach\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nHuman attention provides valuable yet underexploited signals for code LLM training, \noffering a perspective beyond purely machine-driven attention. Despite the \ncomplexity and cost of collecting eye-tracking data, there has also been limited\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.14936&hl=en&sa=X&d=18437738178966048662&ei=dw4EaNSwKvOy6rQP1tr_uA8&scisig=AFWwaeYqVjuSDbuyL315ib7n-age&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Smart Contract Vulnerability Detection Using Large Language Models and Graph Structural Analysis.", "first_label": ["Vulnerabilities", "Smart Contracts", "LLM"], "second_label": ["Detection"], "data": "RY Choi, Y Song, M Jang, T Kim, J Ahn, DH Im\\xc2\\xa0- Computers, Materials & Continua, 2025\nSmart contracts are self-executing programs on blockchains that manage complex \nbusiness logic with transparency and integrity. However, their immutability after \ndeployment makes programming errors particularly critical, as such errors can be\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://search.ebscohost.com/login.aspx%3Fdirect%3Dtrue%26profile%3Dehost%26scope%3Dsite%26authtype%3Dcrawler%26jrnl%3D15462218%26AN%3D184143386%26h%3DgBiCt2Vo1kRt%252FCK1%252BSWWfB4p5FKnCdgFxhLvS20Pk1p9Cza3sO62Nc8OZpQoxEZ8SeN%252BTYixYvZr7BkNZzwhoA%253D%253D%26crl%3Dc&hl=en&sa=X&d=4504820058459498391&ei=dw4EaNSwKvOy6rQP1tr_uA8&scisig=AFWwaebpASe2BZfIWxlYp5PyJgmc&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AFWwaeYwgMeQSPpxCfDXmGy5aE3n&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research", "Hong Jin Kang - new related research"]}
{"title": "Reasoning with LLMs for Zero-Shot Vulnerability Detection", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection", "Reasoning"], "data": "A Zibaeirad, M Vieira\\xc2\\xa0- arXiv preprint arXiv:2503.17885, 2025\nAutomating software vulnerability detection (SVD) remains a critical challenge in an \nera of increasingly complex and interdependent software systems. Despite \nsignificant advances in Large Language Models (LLMs) for code analysis, prevailing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17885&hl=en&sa=X&d=16151102993317123730&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaeaU1duZG60YHiU0D1fyotku&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=0&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Improving distributed learning-based vulnerability detection via multi-modal prompt tuning", "first_label": ["Vulnerabilities"], "second_label": ["Detection"], "data": "Z Ren, X Ju, X Chen, Y Qu\\xc2\\xa0- Journal of Systems and Software, 2025\nSoftware vulnerabilities pose significant threats to the integrity and reliability of \ncomplex systems, making their detection critical. In recent years, a growing body of \nresearch has explored deep learning-based approaches for identifying\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0164121225001104&hl=en&sa=X&d=5554212799419582198&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaebywSeUSc7QcKlN-bBm5POZ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Enhancing llm code generation with ensembles: A similarity-based selection approach", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "T Mahmud, B Duan, C Pasareanu, G Yang\\xc2\\xa0- arXiv preprint arXiv:2503.15838, 2025\nEnsemble learning has been widely used in machine learning to improve model \nrobustness, accuracy, and generalization, but has not yet been applied to code \ngeneration tasks with large language models (LLMs). We propose an ensemble\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.15838&hl=en&sa=X&d=6315180162277152648&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaeYw_s8WAmNdUs0l_UHZtKix&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Resolving Conditional Implicit Calls to Improve Static and Dynamic Analysis in Android Apps", "first_label": [], "second_label": [], "data": "J SAMHI, R JUST, MD ERNST, TF BISSYAND\\xc3\\x89\\xe2\\x80\\xa6\nSecurity and privacy in the Android ecosystem is of critical importance, because \nAndroid dominates the mobile market in number of devices [31] and number of apps \n[33]. Previous research has investigated static analysis [5, 17, 18, 34, 36, 51]\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://jordansamhi.com/files/papers/archer.pdf&hl=en&sa=X&d=4455344894030498548&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaeaw1tkz4SYZ5ffYKxMbPzmn&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=6&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "BESA: Extending Bugs Triggered by Runtime Testing via Static Analysis", "first_label": ["Bug", "Software Testing", "Static Analysis"], "second_label": [], "data": "JJ Bai\\xc2\\xa0- Proceedings of the Twentieth European Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDue to limited test cases and execution scenarios, runtime testing often has \ninsufficient code coverage and thus misses many real bugs. To tackle this problem, \nwe propose a novel idea that static analysis of the triggered bug in runtime testing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/abs/10.1145/3689031.3696089&hl=en&sa=X&d=11971501184870035235&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaebDNiyhkoXdOVlEYm5l6Q09&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=7&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Advanced code time complexity prediction approach using contrastive learning", "first_label": ["Code"], "second_label": [], "data": "S Park, J Hahn, E Orwig, SK Ko, YS Han\\xc2\\xa0- Engineering Applications of Artificial\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nIt is a crucial task to predict the algorithmic time complexity for estimating the \nefficiency of a software code. Since the problem is known to be undecidable in \ntheory, there is no 100% accurate tools to solve the problem. Even humans often\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0952197625006311&hl=en&sa=X&d=16349264168467483688&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaebzdf93mAmknfqCUiHXyi_o&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=8&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM", "first_label": ["LLM", "Code"], "second_label": [], "data": "L Team, W Cai, Y Cao, C Chen, C Chen, S Chen, Q Cui\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nRecent advancements in code large language models (LLMs) have demonstrated \nremarkable capabilities in code generation and understanding. It is still challenging \nto build a code LLM with comprehensive performance yet ultimate efficiency. Many\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2503.17793&hl=en&sa=X&d=6822469255660902021&ei=dw4EaLfKM9SyieoPgfbI2Qk&scisig=AFWwaeZ8vcQc_PT5mFMW0Uih-mYZ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AFWwaeaeIo1O_qAhRJzogmnex0DM&html=&pos=9&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair", "first_label": ["APR"], "second_label": ["Repair"], "data": "X Cai, L Jiang\\xc2\\xa0- arXiv preprint arXiv:2504.01523, 2025\nAutomated Program Repair (APR) aims to enhance software reliability by \nautomatically generating bug-fixing patches. Recent work has improved the state-of-\nthe-art of APR by fine-tuning pre-trained large language models (LLMs), such as\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.01523&hl=en&sa=X&d=9461921674059523797&ei=dw4EaM7jKIWlieoPqbWj8Q4&scisig=AFWwaebGEkQCoR5RF1sA1OAr_sCy&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AFWwaeZjZJIN-8rhXrY_SmCmGQgD&html=&pos=0&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research"]}

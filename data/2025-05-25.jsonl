{"title": "DuoReduce: Bug Isolation for Multi-Layer Extensible Compilation", "first_label": ["Bug"], "second_label": [], "data": "J Wang, Y Qiu, B Limpanukorn, HJ Kang, Q Zhang\\xe2\\x80\\xa6 - 2025\nIn recent years, the MLIR framework has had explosive growth due to the need for \nextensible deep learning compilers for hardware accelerators. Such examples \ninclude Triton [39], CIRCT [14], and ONNX-MLIR [22]. MLIR compilers introduce\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles written by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://par.nsf.gov/biblio/10592191&hl=en&sa=X&d=16704826441152364364&ei=mHAyaKqYLOG86rQP8bDQeA&scisig=AAZF9b8cZzXp6d7FxXcCX8_GEGxu&oi=scholaralrt&hist=ylyK0_8AAAAJ:2125936049491152889:AAZF9b-lPWWMN6t2xDAeKM1BPPMk&html=&pos=0&folt=art", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new articles"]}
{"title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "first_label": ["APR"], "second_label": ["Repair"], "data": "FV Ruiz, M Hort, L Moonen\\xc2\\xa0- arXiv preprint arXiv:2505.02931, 2025\nAutomatic program repair (APR) aims to reduce the manual efforts required to \nidentify and fix errors in source code. Before the rise of LLM-based agents, a \ncommon strategy was to increase the number of generated patches, sometimes to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.02931&hl=en&sa=X&d=16837479357373517474&ei=mHAyaKXENPCuieoP583H-QE&scisig=AAZF9b_oekcjeS14XjGNQo9mtAE5&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "CORECRISIS: Threat-Guided and Context-Aware Iterative Learning and Fuzzing of 5G Core Networks", "first_label": ["Fuzzing"], "second_label": [], "data": "Y Dong, T Yang, A Al Ishtiaq, SMM Rashid, A Ranjbar\\xe2\\x80\\xa6\nWe develop CORECRISIS, a stateful black-box fuzz-testing framework for 5G core \nnetwork (5GC) implementations. Unlike previous stateful security analysis efforts of \ncellular networks which rely on manually-crafted, static test inputs and are limited to\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-1292-dong-yilu.pdf&hl=en&sa=X&d=4291979592406815479&ei=mHAyaKXENPCuieoP583H-QE&scisig=AAZF9b8xMpoFy9xYsoZdVl1unvGx&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=1&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "JNFuzz-Droid: a lightweight fuzzing and taint analysis framework for native code of Android applications", "first_label": ["Fuzzing", "Code"], "second_label": [], "data": "J Cao, F Guo, Y Qu\\xc2\\xa0- Empirical Software Engineering, 2025\nThe need to account for native code in Android apps is becoming urgent as the \nusage of native code is growing in both benign and malicious apps. However, most \ncurrent state-of-the-art analysis tools cannot effectively analyze the data-flow\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10671-9&hl=en&sa=X&d=11975277543600467260&ei=mHAyaKXENPCuieoP583H-QE&scisig=AAZF9b-V-QJRIFsAWjireox8y_xR&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research", "David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "Fuzzing Space Communication Protocols", "first_label": ["Fuzzing"], "second_label": [], "data": "S Havermans, L Baumg\\xc3\\xa4rtner, J Roberts, M Wallum\\xe2\\x80\\xa6\nSpace systems are critical assets and protecting them against cyberattacks is a \nparamount challenge that has received limited attention. In particular, it is \nfundamental to secure spacecraft communications by identifying and removing\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://software.imdea.org/~juanca/papers/spacefuzz_spacesec25.pdf&hl=en&sa=X&d=964000194933772866&ei=mHAyaKXENPCuieoP583H-QE&scisig=AAZF9b9To-YtFkNH0V2NeHA65cZP&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=3&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "WildSync: Automated Fuzzing Harness Synthesis via Wild API Usage Recovery", "first_label": ["Fuzzing"], "second_label": [], "data": "WEIC WU, S NAGY, C HAUSER - 2025\nAuthors' Contact Information: Wei-Cheng Wu, Dartmouth College, Hanover, USA, wei-\ncheng. wu. gr@ dartmouth. edu; Stefan Nagy, University of Utah, Salt Lake City, USA, \nstefan. nagy@ utah. edu; Christophe Hauser, Dartmouth College, Hanover, USA\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://users.cs.utah.edu/~snagy/papers/25ISSTA.pdf&hl=en&sa=X&d=3125007074729669279&ei=mHAyaKXENPCuieoP583H-QE&scisig=AAZF9b8Pijs9BnZif9GfvBHcFCPy&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=4&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Wang, L Ding, TN Nguyen, S Wang, Y Zheng\\xc2\\xa0- arXiv preprint arXiv:2505.14759, 2025\nLarge Language Models for code often entail significant computational complexity, \nwhich grows significantly with the length of the input code sequence. We propose \nLeanCode for code simplification to reduce training and prediction time, leveraging\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14759&hl=en&sa=X&d=3248557237439334794&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b_zaxyG8JpihPx8XNA4sIet&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=0&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "David Lo - new related research", "Hong Jin Kang - new related research"]}
{"title": "RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry", "first_label": ["Code"], "second_label": ["Generation"], "data": "C Wang, Z Yang, S Gao, C Gao, T Peng, H Huang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode completion, a crucial practice in industrial settings, helps developers improve \nprogramming efficiency by automatically suggesting code snippets during \ndevelopment. With the emergence of Large Code Models (LCMs), this field has\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15179&hl=en&sa=X&d=6796322448260186262&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b-0hTJNlMgjhng-YBv3LBF1&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=1&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "David Lo - new related research"]}
{"title": "DS-Bench: A Realistic Benchmark for Data Science Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "S Ouyang, D Huang, J Guo, Z Sun, Q Zhu, JM Zhang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe introduce DS-bench, a new benchmark designed to evaluate large language \nmodels (LLMs) on complicated and realistic data science code generation tasks. DS-\nbench consists of 1,000 carefully constructed problems sourced from realistic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15621&hl=en&sa=X&d=7689829491268805054&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b8p3V31WIhnI4wv-RzXSsiv&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "David Lo - new related research", "Hong Jin Kang - new related research", "Bach Le - new related research"]}
{"title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement", "first_label": ["LLM"], "second_label": [], "data": "J Hu, J Zhang, Y Zhao, T Ringer\\xc2\\xa0- arXiv preprint arXiv:2505.15740, 2025\nFormal methods is pivotal for verifying the reliability of critical systems through \nrigorous mathematical proofs. However, its adoption is hindered by labor-intensive \nmanual proofs and the expertise required to use theorem provers. Recent\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15740&hl=en&sa=X&d=11638096528726351561&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b88iKMA_2buuMpNX2t8P1Kx&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=3&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research", "7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Are requirements really all you need? A case study of LLM-driven configuration code generation for automotive simulations", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "K Lebioda, N Petrovic, F Pan, V Zolfaghari\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) are taking many industries by storm. They possess \nimpressive reasoning capabilities and are capable of handling complex problems, as \nshown by their steadily improving scores on coding and mathematical benchmarks\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13263&hl=en&sa=X&d=8142159630564180976&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b8JjDXL2WTWCxGT05T_vG1H&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=4&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Secret Breach Detection in Source Code with Large Language Models", "first_label": ["LLM", "Code"], "second_label": ["Detection"], "data": "MN Rahman, S Ahmed, Z Wahab, SM Sohan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nBackground: Leaking sensitive information, such as API keys, tokens, and \ncredentials, in source code remains a persistent security threat. Traditional regex and \nentropy-based tools often generate high false positives due to limited contextual\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.18784&hl=en&sa=X&d=12009667987401074433&ei=mHAyaPvsOPel6rQP6dvK4Ao&scisig=AAZF9b8WC1VKgTvjxoIAkc7ukuBh&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=5&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "An Empirical Analysis of Vulnerability Detection Tools for Solidity Smart Contracts Using Line Level Manually Annotated Vulnerabilities", "first_label": ["Vulnerabilities", "Smart Contracts"], "second_label": ["Detection"], "data": "F Salzano, CK Antenucci, S Scalabrino, G Rosa\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid adoption of blockchain technology highlighted the importance of ensuring \nthe security of smart contracts due to their critical role in automated business logic \nexecution on blockchain platforms. This paper provides an empirical evaluation of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15756&hl=en&sa=X&d=8442828954099712047&ei=mHAyaPiaMKalieoP5f2nkAE&scisig=AAZF9b-EAcvYZ_J4_9XdhW2rqHOc&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=2&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Hong Jin Kang - new related research", "3 new citations to articles by Bach Le", "Quang-Cuong Bui - new related research"]}
{"title": "NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging", "first_label": ["Code", "Bug"], "second_label": ["Exploit"], "data": "W Zhang, Q Li, X Dai, J Chen, K Du, W Zhang, W Liu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDebugging is a critical aspect of LLM's coding ability. Early debugging efforts \nprimarily focused on code-level analysis, which often falls short when addressing \ncomplex programming errors that require a deeper understanding of algorithmic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15356&hl=en&sa=X&d=4143706178160146132&ei=mHAyaPiaMKalieoP5f2nkAE&scisig=AAZF9b-0DStOAacdkUcGc9ionRoT&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Bach Le - new related research"]}
{"title": "MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models", "first_label": ["LLM", "Code"], "second_label": ["Generation", "Agent"], "data": "A Rahman, V Cvetkovic, K Reece, A Walters, Y Hassan\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have transformed software development through \ncode generation capabilities, yet their effectiveness for high-performance computing \n(HPC) remains limited. HPC code requires specialized optimizations for parallelism\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.03906&hl=en&sa=X&d=17867515124636780677&ei=mHAyaPiaMKalieoP5f2nkAE&scisig=AAZF9b9SpUJHcH-TletqVNtFUBN6&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Towards a Science of Causal Interpretability in Deep Learning for Software Engineering", "first_label": [], "second_label": [], "data": "DN Palacio\\xc2\\xa0- arXiv preprint arXiv:2505.15023, 2025\nThis dissertation addresses achieving causal interpretability in Deep Learning for \nSoftware Engineering (DL4SE). While Neural Code Models (NCMs) show strong \nperformance in automating software tasks, their lack of transparency in causal\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15023&hl=en&sa=X&d=539836201692368647&ei=mHAyaPiaMKalieoP5f2nkAE&scisig=AAZF9b8R8Qo6Le76nXXW1THn9HE-&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=7&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Cross-Level Requirements Tracing Based on Large Language Models", "first_label": ["LLM"], "second_label": [], "data": "C Ge, TT Wang, XT Yang, C Treude\\xc2\\xa0- IEEE Transactions on Software Engineering, 2025\nCross-level requirements traceability, linking high-level requirements (HLRs) and \nlow-level requirements (LLRs), is essential for maintaining relationships and \nconsistency in software development. However, the manual creation of requirements\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/journal/ts/5555/01/11008781/26TEcLRNtvy&hl=en&sa=X&d=13934667537654443335&ei=mHAyaPiaMKalieoP5f2nkAE&scisig=AAZF9b864Be_md4Iip2bVKxL4wMh&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=8&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Leveraging Ethical Narratives to Enhance LLM\\xe2\\x80\\x90AutoML Generated Machine Learning Models", "first_label": ["LLM"], "second_label": [], "data": "J Nelson, M Pavlidis, A Fish, N Polatidis\\xe2\\x80\\xa6\\xc2\\xa0- Expert Systems, 2025\nThe growing popularity of generative AI and large language models (LLMs) has \nsparked innovation alongside debate, particularly around issues of plagiarism and \nintellectual property law. However, a less\\xe2\\x80\\x90discussed concern is the quality of code \ngenerated by these models, which often contains errors and encourages poor \nprogramming practices. This paper proposes a novel solution by integrating LLMs \nwith automated machine learning (AutoML). By leveraging AutoML's strengths in\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRefining chatgpt-generated code: Characterizing and mitigating\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.70072&hl=en&sa=X&d=17665266423314408285&ei=mHAyaNTtKtHSieoP4tatoQU&scisig=AAZF9b8F1FH2vCrQZfK3GxxA6GL2&oi=scholaralrt&hist=ylyK0_8AAAAJ:1164437029242115036:AAZF9b9cZXgBuh9nrxFB6U5Br4kf&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["1 new citation to articles by Thanh Le-Cong", "3 new citations to articles by Bach Le"]}
{"title": "LELANTE: LEveraging LLM for Automated ANdroid TEsting", "first_label": ["LLM", "Software Testing"], "second_label": [], "data": "S Fatin, MH Al-Quvi, HS Shahgir, S Barua, A Iqbal\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nGiven natural language test case description for an Android application, existing \ntesting approaches require developers to manually write scripts using tools such as \nAppium and Espresso to execute the corresponding test case. This process is labor\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.20896&hl=en&sa=X&d=17615025726171243016&ei=mHAyaP6POp-mieoPoOrjuQU&scisig=AAZF9b8-TWsDerYtYXgkjb3Dz2BH&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "PRIMG: Efficient LLM-driven Test Generation Using Mutant Prioritization", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "MS Bouafif, M Hamdaqa, E Zulkoski\\xc2\\xa0- arXiv preprint arXiv:2505.05584, 2025\nMutation testing is a widely recognized technique for assessing and enhancing the \neffectiveness of software test suites by introducing deliberate code mutations. \nHowever, its application often results in overly large test suites, as developers\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.05584%3F&hl=en&sa=X&d=11403797073407320761&ei=mHAyaP6POp-mieoPoOrjuQU&scisig=AAZF9b_8rRCKKaF4lP39iEYsaKlR&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=4&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "FuzzFlesh: Randomised Testing of Decompilers Via Control Flow Graph-based Program Generation", "first_label": ["Fuzzing", "Software Testing", "Static Analysis"], "second_label": ["Generation", "Graph"], "data": "A Gorzynski, AF Donaldson\nDecompilation is the process of translating compiled code into high-level code. \nControl flow recovery is a challenging part of the process.'Misdecompilations' can \noccur, whereby the decompiled code does not accurately represent the semantics of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.doc.ic.ac.uk/~afd/papers/2025/ECOOP-FuzzFlesh.pdf&hl=en&sa=X&d=904693152685957307&ei=mHAyaP6POp-mieoPoOrjuQU&scisig=AAZF9b8Lv9KaxB9WgdBF01r9IOYL&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=6&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Poster: Towards Understanding Bug Bounties for AI/ML Open-Source Vulnerabilities in GitHub Repositories", "first_label": ["Vulnerabilities", "Bug"], "second_label": [], "data": "J Ayala, J Garcia\nRecently, specialized bug bounty platforms, such as huntr, have emerged to \nincentivize the discovery of vulnerabilities in open-source software (OSS) that power \nAI (Artificial Intelligence) and ML (Machine Learning) systems. In this extended\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://sp2025.ieee-security.org/downloads/posters/sp25posters-final9.pdf&hl=en&sa=X&d=4514136328140428675&ei=mHAyaP6POp-mieoPoOrjuQU&scisig=AAZF9b-P5QwQGR52cbjrqe0OzmpI&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=7&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Can test generation and program repair inform automated assessment of programming projects?", "first_label": ["APR", "Software Testing"], "second_label": ["Repair", "Generation"], "data": "R Gu, JM Rojas, D Shin\\xc2\\xa0- 2025 IEEE Conference on Software Testing\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nComputer Science educators assessing student programming assignments are \ntypically responsible for two challenging tasks: grading and providing feedback. \nProducing grades that are fair and feedback that is useful to students is a goal \ncommon to most educators. In this context, automated test generation and program \nrepair offer promising solutions for detecting bugs and suggesting corrections in \nstudents' code which could be leveraged to inform grading and feedback generation\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaHistory driven program repair\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://www.computer.org/csdl/proceedings-article/icst/2025/10988955/26S4ITvftoA&hl=en&sa=X&d=15304299180329145691&ei=mHAyaMbxLubGieoP1sTUqQ8&scisig=AAZF9b_1qN3yoEG9pKZt3_SvMG2c&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=2&folt=cit", "author": ["Bach Le"], "ref": ["3 new citations to articles by Bach Le", "7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Fully Randomized Pointers", "first_label": [], "second_label": [], "data": "SD Phaye, GJ Duck, RHC Yap, TE Carlson - 2025\nMemory errors continue to be a critical concern for programs written in low-level \nprogramming languages such as C and C++. Many different memory error defenses \nhave been proposed, each with varying trade-offs in terms of overhead, compatibility, \nand attack resistance. Some defenses are highly compatible but only provide \nminimal protection, and can be easily bypassed by knowledgeable attackers. On the \nother end of the spectrum, capability systems offer very strong (unforgeable)\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaEfficient Greybox Fuzzing to Detect Memory Errors\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.comp.nus.edu.sg/~tcarlson/pdfs/phaye2025frp.pdf&hl=en&sa=X&d=2878539714208456769&ei=mHAyaPTPMeW16rQPsqVy&scisig=AAZF9b_ZpxPIQ2Ha2sPPXdUow8f7&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=0&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Increasing The Practicality Of Verification Using Incomplete Solutions", "first_label": ["Verification"], "second_label": [], "data": "E Goldweber - 2025\nBuilding correct software systems is challenging. To ensure correctness, developers \nhave increasingly turned to formal verification to help achieve this goal. Verification \nprovides strong assurances of correctness by construction. Developers write proof \nannotations to show that their implementation adheres to desired properties captured \nin a formal specification. Checking if the implementation meets the specification is \nautomated by encoding the proof, specification, and implementation as a satisfiability\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaFuzzing: Challenges and Reflections\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://deepblue.lib.umich.edu/bitstream/handle/2027.42/197340/edgoldwe_1.pdf%3Fsequence%3D1&hl=en&sa=X&d=7975383174051443547&ei=mHAyaPTPMeW16rQPsqVy&scisig=AAZF9b_BuKyDXXqxq4vyqUc4UWbm&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=2&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "ASIRDetector: Scheduling-driven, asynchronous execution to discover asynchronous improper releases bug in linux kernel", "first_label": ["Bug"], "second_label": ["Detection"], "data": "J Zhao, Q Wei, X Li, Y Wang, X Li\\xc2\\xa0- Computers & Security, 2025\nAsynchronous operations are the cornerstone of modern operating systems, \nenabling high-performance task scheduling and efficient resource management. \nHowever, if the asynchronous mechanism releases resources at incorrect times, it \nwill pose significant security risks to the Linux kernel, such as high-risk vulnerabilities \nlike use-after-free and null pointer dereferencing. Due to the indirect triggerability of \nasynchronous operations by users, existing methods for detecting kernel\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaDirected greybox fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0167404825002196&hl=en&sa=X&d=4899331987769959650&ei=mHAyaPTPMeW16rQPsqVy&scisig=AAZF9b836z7fK9Nih3g6eqTjufOC&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=3&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "ComplementaryRepair: Enhancing Small Open-Source Large Language Models for Repairing Introductory Student Code with Prompt Diversity", "first_label": ["LLM", "Code"], "second_label": ["Repair"], "data": "J Yang - 2025\nAbstract Automated Program Repair (APR) could enhance introductory programming \neducation by repairing errors in student code efficiently. Beyond simply providing \nsolutions, APR can offer partial repairs as hints, aid instructors in identifying errors, \nand serve as a basis for generating automated feedback. Moreover, repaired student \ncode can offer a more personalized and effective reference for post-submission \nlearning. Recent advancements in Large Language Models (LLMs) have\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRe-factoring based Program Repair applied to Programming\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://aaltodoc.aalto.fi/bitstreams/0bc5e811-58e1-41fe-aefc-cd1120c1c89c/download&hl=en&sa=X&d=3337997423257487216&ei=mHAyaPTPMeW16rQPsqVy&scisig=AAZF9b-K8mr28ijF0lmRZJ0eXLeT&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=5&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Probabilistic Analysis of Time to Discover Paths in Fuzzing", "first_label": ["Fuzzing"], "second_label": [], "data": "R Shevade - 2025\nFuzzing is a popular program testing technique in the software security community. It \noperates by creating unexpected inputs for the program that may cause bugs and \nmonitors the program for crashes or abnormal behavior. The inputs created by the \nfuzzer is the product of random mutations on an initial input (seed) or a set of seeds \n(corpus). Greybox fuzzer, a specific type of fuzzer, strategically pick seeds which, it \nbelieves is more likely to lead to a discovery of a bug. This selection is guided by\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaDirected greybox fuzzing\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://etda.libraries.psu.edu/files/final_submissions/32659&hl=en&sa=X&d=1441640927391494910&ei=mHAyaPTPMeW16rQPsqVy&scisig=AAZF9b8TO34pbUZDtpnPsdhEOyZ_&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=6&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Leveraging Open-Source LLMs for Zero-Shot Vulnerability Detection: A Comparative Analysis", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection"], "data": "N Capuano, V Carletti, P Foggia, G Parrella, M Vento\\xc2\\xa0- International Conference on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid expansion of the Internet of Things (IoT) has brought significant security \nchallenges, primarily due to vulnerabilities in the firmware of IoT and network \ndevices, which is predominantly written in low-level programming languages such as\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-87775-9_2&hl=en&sa=X&d=8724089268434184607&ei=mHAyaIiTM4Ct6rQP9sPUwAg&scisig=AAZF9b-ahkowI222f9SeGE9jq2lb&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=0&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Tighter Clusters, Safer Code? Improving Vulnerability Detection with Enhanced Contrastive Loss", "first_label": ["Vulnerabilities", "Code"], "second_label": ["Detection"], "data": "P Kapparad, BR Mohan\\xc2\\xa0- Proceedings of the 2025 Conference of the Nations of\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nDistinguishing vulnerable code from non-vulnerable code is challenging due to high \ninter-class similarity. Supervised contrastive learning (SCL) improves embedding \nseparation but struggles with intra-class clustering, especially when variations within\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://aclanthology.org/2025.naacl-srw.24.pdf&hl=en&sa=X&d=568853348667603882&ei=mHAyaIiTM4Ct6rQP9sPUwAg&scisig=AAZF9b81gkouWADoP2Iw2ABqDW_S&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=2&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research"]}
{"title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "first_label": ["LLM"], "second_label": [], "data": "S Cappelletti, T Poppi, S Poppi, ZX Yong\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLMs) are increasingly evaluated on multiple-choice \nquestion answering (MCQA) tasks using* first-token probability*(FTP), which selects \nthe answer option whose initial token has the highest likelihood. While efficient, FTP\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15323&hl=en&sa=X&d=5513891085179250670&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b9edBmGRix6lx3Ht8Bnn61V&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SafeQuant: LLM Safety Analysis via Quantized Gradient Inspection", "first_label": ["LLM"], "second_label": [], "data": "S Padakandla, S Babar, M Kaul\\xc2\\xa0- Proceedings of the 2025 Conference of the\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nContemporary jailbreak attacks on Large Language Models (LLMs) employ \nsophisticated techniques with obfuscated content to bypass safety guardrails. \nExisting defenses either use computationally intensive LLM verification or require\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://aclanthology.org/2025.naacl-long.127.pdf&hl=en&sa=X&d=15183737478241495930&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b-IAmGkHDwpPHasWJdSh6QL&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition", "first_label": [], "second_label": [], "data": "S Yuan, G Xu, H Li, R Zhang, X Qian, W Jiang, H Cao\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nTraffic sign recognition (TSR) systems are crucial for autonomous driving but are \nvulnerable to backdoor attacks. Existing physical backdoor attacks either lack stealth, \nprovide inflexible attack control, or ignore emerging Vision-Large-Language-Models\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12045&hl=en&sa=X&d=486471126133404257&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b9uZDwtYCinn-s4WZ6ahuHJ&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Advancing LLM Safe Alignment with Safety Representation Ranking", "first_label": ["LLM"], "second_label": [], "data": "T Du, Z Wei, Q Chen, C Zhang, Y Wang\\xc2\\xa0- arXiv preprint arXiv:2505.15710, 2025\nThe rapid advancement of large language models (LLMs) has demonstrated \nmilestone success in a variety of tasks, yet their potential for generating harmful \ncontent has raised significant safety concerns. Existing safety evaluation approaches\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15710&hl=en&sa=X&d=9147050430051665415&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b_EiI4FmCbxurZY4HFEhWA5&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses", "first_label": ["LLM"], "second_label": [], "data": "X Yang, B Stevanoski, M Meeus, YA de Montjoye\\xc2\\xa0- arXiv preprint arXiv:2505.15738, 2025\nLarge language models (LLMs) are rapidly deployed in real-world applications \nranging from chatbots to agentic systems. Alignment is one of the main approaches \nused to defend against attacks such as prompt injection and jailbreaks. Recent\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.15738&hl=en&sa=X&d=15949587169759547530&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b9jW2tzEb1jjKg1wddBnl-L&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Layer Frozen Multi-Net & latent space feature-concealed backdoor samples detection", "first_label": [], "second_label": ["Detection"], "data": "J Li, S Luo, L Pan, C Zhang, Z Zhang, C Lu\\xc2\\xa0- Neural Networks, 2025\nIdentifying feature-concealed backdoor samples that entangle with benign semantics \nof target-class or possess dynamic triggers challenges backdoor attack detection. \nExisting methods focus on sample distribution differences in latent space of victim\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0893608025003764&hl=en&sa=X&d=9440568821815382276&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b9KQWzgsrGxwNibQ2QDGZPT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Invisible trigger image: A dynamic neural backdoor attack based on hidden feature", "first_label": [], "second_label": [], "data": "X Chen, M Li, Y Sun, Z Tian\\xc2\\xa0- Neurocomputing, 2025\nNeural backdoor is one of the most significant supply chain threat faced by deep \nneural networks (DNNs). A common attack method for implanting backdoors in DNNs \ninvolves injecting poisoned training data that contains triggers. Triggers are\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0925231225009683&hl=en&sa=X&d=17933955967710027848&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b8vcZJvcttnhTa_udTLOVjg&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion", "first_label": ["LLM"], "second_label": [], "data": "T Cui, Y Mao, P Liu, C Liu, D You\\xc2\\xa0- arXiv preprint arXiv:2505.14316, 2025\nAlthough large language models (LLMs) have achieved remarkable advancements, \ntheir security remains a pressing concern. One major threat is jailbreak attacks, \nwhere adversarial prompts bypass model safeguards to generate harmful or\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14316&hl=en&sa=X&d=4317580650000119700&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b9krK_d1hcPwfwGDIEjENim&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation", "first_label": ["LLM"], "second_label": [], "data": "R Romijnders, S Laskaridis, AS Shamsabadi\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge Language Models (LLM) are typically trained on vast amounts of data from \nvarious sources. Even when designed modularly (eg, Mixture-of-Experts), LLMs can \nleak privacy on their sources. Conversely, training such models in isolation arguably\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.18147%3F&hl=en&sa=X&d=5115743027930617572&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b-zZHmB78f-Ya4d-6GmF3AY&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Revisiting Data Auditing in Large Vision-Language Models", "first_label": ["LLM"], "second_label": [], "data": "H Zhu, S Liang, W Wang, B Li, T Yuan, F Li, SL Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith the surge of large language models (LLMs), Large Vision-Language Models \n(VLMs)--which integrate vision encoders with LLMs for accurate visual grounding--\nhave shown great potential in tasks like generalist agents and robotic control\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.18349&hl=en&sa=X&d=928423691795550423&ei=mHAyaOCvN7eC6rQP9sj2mQ8&scisig=AAZF9b99grWlNGgtJWvNrjQ6l87h&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "N Maloyan, B Ashinov, D Namiot\\xc2\\xa0- arXiv preprint arXiv:2505.13348, 2025\nLarge Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-\nJudge) for assessing the quality of machine-generated text. This paradigm offers \nscalability and cost-effectiveness compared to human annotation. However, the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13348&hl=en&sa=X&d=5272614435720234126&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b__v284anbHznTasg-AYLVM&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=0&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Evaluatiing the efficacy of LLM Safety Solutions: The Palit Benchmark Dataset", "first_label": ["LLM"], "second_label": [], "data": "S Palit, D Woods\\xc2\\xa0- arXiv preprint arXiv:2505.13028, 2025\nLarge Language Models (LLMs) are increasingly integrated into critical systems in \nindustries like healthcare and finance. Users can often submit queries to LLM-\nenabled chatbots, some of which can enrich responses with information retrieved\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13028&hl=en&sa=X&d=1273460049730580919&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b_-vHL06cauSDcPg_fRL8FD&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=1&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "JULI: Jailbreak Large Language Models by Self-Introspection", "first_label": ["LLM"], "second_label": [], "data": "J Wang, Z Hu, D Wagner\\xc2\\xa0- arXiv preprint arXiv:2505.11790, 2025\nLarge Language Models (LLMs) are trained with safety alignment to prevent \ngenerating malicious content. Although some attacks have highlighted vulnerabilities \nin these safety-aligned LLMs, they typically have limitations, such as necessitating\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.11790&hl=en&sa=X&d=3926434229328693064&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b9nZiBnexyosm99TRU_ULp7&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=2&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "first_label": ["LLM"], "second_label": ["Agent"], "data": "L Wang, W Wang, S Wang, Z Li, Z Ji, Z Lyu, D Wu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThe rapid advancement of Large Language Models (LLMs) has led to the \nemergence of Multi-Agent Systems (MAS) to perform complex tasks through \ncollaboration. However, the intricate nature of MAS, including their architecture and\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12442&hl=en&sa=X&d=8825331941370095164&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b_j6hKF2Zy8dNMOx-tobM6E&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=3&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models", "first_label": ["LLM"], "second_label": [], "data": "G Chen, F Song, Z Zhao, X Jia, Y Liu, Y Qiao, W Zhang\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nJailbreak attacks to Large audio-language models (LALMs) are studied recently, but \nthey achieve suboptimal effectiveness, applicability, and practicability, particularly, \nassuming that the adversary can fully manipulate user prompts. In this work, we first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14103&hl=en&sa=X&d=12355582258955555442&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b84twuHlVXU1hPFciKO3AfT&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=4&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks", "first_label": ["LLM"], "second_label": [], "data": "A Djanibekov, N Mukhituly, K Inui, H Aldarmaki\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nSpeech Language Models (SLMs) enable natural interactions via spoken \ninstructions, which more effectively capture user intent by detecting nuances in \nspeech. The richer speech signal introduces new security risks compared to text\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13541&hl=en&sa=X&d=14264422034074649514&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b_cqBc09Ng0S0WhdfXSIHSr&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=5&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "Z Xu, U Sanghi, M Kankanhalli\\xc2\\xa0- arXiv preprint arXiv:2505.12692, 2025\nLarge Language Models (LLMs) are increasingly deployed in interactions where \nthey are prompted to adopt personas. This paper investigates whether such persona \nconditioning affects model safety under bullying, an adversarial manipulation that\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12692&hl=en&sa=X&d=3130006781540917286&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b_aZXNRBIY5lJUtsG2cllBY&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=6&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "first_label": ["LLM"], "second_label": [], "data": "MM Tanjim, Y In, X Chen, VS Bursztyn, RA Rossi\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAmbiguity remains a fundamental challenge in Natural Language Processing (NLP) \ndue to the inherent complexity and flexibility of human language. With the advent of \nLarge Language Models (LLMs), addressing ambiguity has become even more\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12543&hl=en&sa=X&d=10098855731850359041&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b-vOVUAwttokXYr2ZwaP0ql&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=7&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations", "first_label": ["LLM"], "second_label": [], "data": "Y Li, H Xu, F Tian\\xc2\\xa0- arXiv preprint arXiv:2505.12237, 2025\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) have \ndemonstrated remarkable reasoning and generalization capabilities in video \nunderstanding; however, their application in video editing remains largely\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12237&hl=en&sa=X&d=10505763597250854428&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b90gQPI6R4RwX9k19MeWwCR&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=8&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Steering the CensorShip: Uncovering Representation Vectors for LLM\" Thought\" Control", "first_label": ["LLM"], "second_label": [], "data": "H Cyberey, D Evans\\xc2\\xa0- arXiv preprint arXiv:2504.17130, 2025\nLarge language models (LLMs) have transformed the way we access information. \nThese models are often tuned to refuse to comply with requests that are considered \nharmful and to produce responses that better align with the preferences of those who\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.17130&hl=en&sa=X&d=15371871009015193723&ei=cMcwaLefGsmx6rQP-rCh4AE&scisig=AAZF9b8EW3QbsTc18pFbrVjbazAC&oi=scholaralrt&hist=ylyK0_8AAAAJ:15287030194885030172:AAZF9b9ZGN1vUuxfG1GbOlvhloTS&html=&pos=9&folt=rel", "author": ["Richard Fang"], "ref": ["Richard Fang - new related research"]}
{"title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair", "first_label": ["APR", "Bug"], "second_label": ["Repair"], "data": "H Zheng, I Shumailov, T Fan, A Hall, M Payer\\xc2\\xa0- arXiv preprint arXiv:2505.13103, 2025\nThe rapid advancement of bug-finding techniques has led to the discovery of more \nvulnerabilities than developers can reasonably fix, creating an urgent need for \neffective Automated Program Repair (APR) methods. However, the complexity of\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13103&hl=en&sa=X&d=13398758609778638920&ei=cMcwaIXGFOfRieoP8p3muQQ&scisig=AAZF9b-mwPS59VllzciyI8t93968&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=0&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research", "Bach Le - new related research", "Hong Jin Kang - new related research", "Thanh Le-Cong - new related research", "David Lo - new related research", "7 new citations to articles by Abhik Roychoudhury", "3 new citations to articles by Xin ZHOU"]}
{"title": "Adversarial Reasoning for Repair Based on Inferred Program Intent", "first_label": [], "second_label": ["Repair", "Reasoning"], "data": "H Ye, AZH Yang, C Hu, Y Wang, T Zhang, CL Goues\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAutomated program repair (APR) has shown promising results, particularly with the \nuse of neural networks. Currently, most APR tools focus on code transformations \nspecified by test suites, rather than reasoning about the program intent and the high\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nQuang-Cuong Bui\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13008&hl=en&sa=X&d=69655204533362550&ei=cMcwaIXGFOfRieoP8p3muQQ&scisig=AAZF9b8GEF76NVs7pjmf0XUNAGB8&oi=scholaralrt&hist=ylyK0_8AAAAJ:11088443020050739259:AAZF9b_dlaF_l6JD6R93aQP1v_a_&html=&pos=1&folt=rel", "author": ["Quang-Cuong Bui"], "ref": ["Quang-Cuong Bui - new related research", "2 new citations to articles by Bach Le", "Thanh Le-Cong - new related research", "Xin ZHOU - new related research", "7 new citations to articles by Abhik Roychoudhury"]}
{"title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Qing, B Zhu, M Du, Z Guo, TY Zhuo, Q Zhang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nExisting code generation benchmarks primarily evaluate functional correctness, with \nlimited focus on code efficiency and often restricted to a single language like Python. \nTo address this gap, we introduce EffiBench-X, the first multi-language benchmark\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nBach Le\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13004&hl=en&sa=X&d=12028887143426091594&ei=cMcwaNnxC-WBieoP0PLYiAU&scisig=AAZF9b8YAoU1OnIvBgJUbMPYVZ2h&oi=scholaralrt&hist=ylyK0_8AAAAJ:4328508672846969495:AAZF9b9vPVpCbQIEUDOQKatBd4_T&html=&pos=1&folt=rel", "author": ["Bach Le"], "ref": ["Bach Le - new related research", "Hong Jin Kang - new related research", "David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "Identifying Root Cause of bugs by Capturing Changed Code Lines with Relational Graph Neural Networks", "first_label": ["Code", "Bug"], "second_label": ["Detection", "Graph"], "data": "J Zhang, S Guo, H Li, C Li, Y Chai, R Chen\\xc2\\xa0- arXiv preprint arXiv:2505.00990, 2025\nThe Just-In-Time defect prediction model helps development teams improve software \nquality and efficiency by assessing whether code changes submitted by developers \nare likely to introduce defects in real-time, allowing timely identification of potential\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.00990&hl=en&sa=X&d=17996198631274398381&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b_S66ntoRzUw7soStgrJChb&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=1&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "Stateful Analysis and Fuzzing of Commercial Baseband Firmware", "first_label": ["Fuzzing"], "second_label": [], "data": "A Ranjbar, T Yang, K Tu, S Khalilollahi, SR Hussain\\xc2\\xa0- 2025 IEEE Symposium on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nBaseband firmware plays a critical role in cellular communication, yet its proprietary, \nclosed-source nature and complex, stateful processing logic make systematic \nsecurity testing challenging. Existing methods often fail to account for the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://syed-rafiul-hussain.github.io/wp-content/uploads/2025/05/Loris_baseband_fuzzing_sp25.pdf&hl=en&sa=X&d=12942867024371270911&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b-Ze-vPOALeUo-ikpj0sFws&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=3&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "Abhik Roychoudhury - new related research"]}
{"title": "LLM-assisted Bug Identification and Correction for Verilog HDL", "first_label": ["LLM", "Bug"], "second_label": [], "data": "K Qayyum, CK Jha, S Ahmadi-Pour, M Hassan\\xe2\\x80\\xa6\\xc2\\xa0- ACM Transactions on\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAs technology continues to advance, it becomes increasingly integrated into daily life \nfacilitating complex tasks across a range of environments. While some applications \nsuch as smartphones and smartwatches are less critical, others like healthcare\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3733237&hl=en&sa=X&d=17416623168000537614&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b8EupnBeyQszHacv5cfujCQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=4&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "An Empirical Study of Multi-Language Security Patches in Open Source Software", "first_label": [], "second_label": [], "data": "S Sun, Y Xing, G Zou, X Wang, K Sun\nVulnerabilities in software repositories written in multiple programming languages \npresent a major challenge to modern software quality assurance, especially those \nresulting from interactions between different languages. Existing static and dynamic\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://yunlongxing.github.io/publications/dimva25_Empirical.pdf&hl=en&sa=X&d=10326866748066170289&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b-FrAQMzcNeCYBMoioph8Ze&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=5&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "first_label": [], "second_label": [], "data": "H Wu, R Ming, J Gao, H Zhao, X Chen, Y Yang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) achieve remarkable performance in code generation \ntasks. However, a significant performance disparity persists between popular \nprogramming languages (eg, Python, C++) and others. To address this capability\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12723&hl=en&sa=X&d=13717591830345234396&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b-Rv548ZrJGZne-1MZyXh7v&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=6&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research", "David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "Benchmarking and Revisiting Code Generation Assessment: A Mutation-Based Approach", "first_label": ["Code"], "second_label": ["Generation"], "data": "L Wang, T Li, X Xie, Y Zhi, J Wang, C Shen\\xc2\\xa0- arXiv preprint arXiv:2505.06880, 2025\nCode Large Language Models (CLLMs) have exhibited outstanding performance in \nprogram synthesis, attracting the focus of the research community. The evaluation of \nCLLM's program synthesis capability has generally relied on manually curated\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nHong Jin Kang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.06880&hl=en&sa=X&d=14741036642597810521&ei=cMcwaKONHeWBieoP0PLYiAU&scisig=AAZF9b-XtwkjbmK5M8jMYEwh_fbQ&oi=scholaralrt&hist=ylyK0_8AAAAJ:17903213248891513419:AAZF9b_VfgOz15WyMBkfhPNIM3wy&html=&pos=7&folt=rel", "author": ["Hong Jin Kang"], "ref": ["Hong Jin Kang - new related research"]}
{"title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "first_label": ["LLM"], "second_label": [], "data": "S Fang, W Ding, B Xu\\xc2\\xa0- arXiv preprint arXiv:2505.12185, 2025\nAssessing the programming capabilities of Large Language Models (LLMs) is crucial \nfor their effective use in software engineering. Current evaluations, however, \npredominantly measure the accuracy of generated code on static benchmarks, \nneglecting the critical aspect of model robustness during programming tasks. While \nadversarial attacks offer insights on model robustness, their effectiveness is limited \nand evaluation could be constrained. Current adversarial attack methods for\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAdversarial attacks on code models with discriminative graph\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12185&hl=en&sa=X&d=10000522040676713077&ei=cMcwaI-GELeC6rQP9sj2mQ8&scisig=AAZF9b9hKHunVPRN-AH800zkjV73&oi=scholaralrt&hist=ylyK0_8AAAAJ:4974034551180671527:AAZF9b-CBry8NrRagz6L4gSOAv5X&html=&pos=0&folt=cit", "author": ["Bach Le"], "ref": ["2 new citations to articles by Bach Le", "David Lo - new related research"]}
{"title": "On the Applicability of Code Language Models to Scientific Computing Programs", "first_label": ["LLM", "Code"], "second_label": [], "data": "Q Zhao, F Liu, X Long, C Wu, L Zhang\\xc2\\xa0- IEEE Transactions on Software Engineering, 2025\nScientific Computing Programming Languages (SCPLs), like MATLAB and R, are \npopular and widely used for computational mathematics. In recent years, pre-trained \ncode language models (CLMs) have automated many code-related tasks, covering\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/10977820/&hl=en&sa=X&d=15239791575915616252&ei=cMcwaMTHDqm7ieoP1JuLsA8&scisig=AAZF9b_lsWqv_Sky29QCVRfDTuma&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Context-Enhanced Vulnerability Detection Based on Large Language Model", "first_label": ["Vulnerabilities", "LLM"], "second_label": ["Detection"], "data": "Y Yang, B Xu, X Gao, H Sun\\xc2\\xa0- arXiv preprint arXiv:2504.16877, 2025\nVulnerability detection is a critical aspect of software security. Accurate detection is \nessential to prevent potential security breaches and protect software systems from \nmalicious attacks. Recently, vulnerability detection methods leveraging deep\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nThanh Le-Cong\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2504.16877&hl=en&sa=X&d=18311446874543132824&ei=cMcwaMTHDqm7ieoP1JuLsA8&scisig=AAZF9b8vxEu3pMo1vQ7vVJj2XwJH&oi=scholaralrt&hist=ylyK0_8AAAAJ:4812769200119993430:AAZF9b_1MT--9phVV-34dqGZeQFI&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - new related research"]}
{"title": "Exploring Fine-Grained Bug Report Categorization with Large Language Models and Prompt Engineering: An Empirical Study", "first_label": ["LLM", "Bug"], "second_label": [], "data": "A Koyuncu\\xc2\\xa0- ACM Transactions on Software Engineering and\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nAccurate classification of issues is essential for effective project management and \ntimely responses, as the volume of issue reports continues to grow. Manual \nclassification is labor-intensive and error-prone, necessitating automated solutions\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3736408&hl=en&sa=X&d=1221326668322558367&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b94qGcVVVtNwHBRf1PSh6nk&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=3&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research", "Xin ZHOU - new related research"]}
{"title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals", "first_label": ["LLM", "Code"], "second_label": [], "data": "X Luo, Q Zhu, Z Zhang, M Xu, T Cheng, Y Wang, Z Chu\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nCode Sensitivity refers to the ability of Code LLMs to recognize and respond to \ndetails changes in problem descriptions. While current code benchmarks and \ninstruction data focus on difficulty and diversity, sensitivity is overlooked. We first\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14597&hl=en&sa=X&d=3060624652656954460&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b_0rbuD18fQZ1qyyZ10pqAd&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=4&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Automated Vulnerability Discovery Generative AI in Offensive Security", "first_label": ["Vulnerabilities"], "second_label": [], "data": "M Alauthman, A Almomani, S Aoudi, A Al-Qerem\\xe2\\x80\\xa6\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0Risks Produced by\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThis chapter investigates how generative AI techniques transform offensive security. \nIt explores automated methods that identify software flaws, generate targeted \nexploits, and support penetration testers in analyzing systems. Emphasis is placed\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://www.igi-global.com/chapter/automated-vulnerability-discovery-generative-ai-in-offensive-security/378288&hl=en&sa=X&d=6142877101358339902&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b900jvcuzxSKRS8GTRV_VL2&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=6&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Automated Vulnerability Discovery Generative Al", "first_label": ["Vulnerabilities"], "second_label": [], "data": "A Al-Qerem, A Aldweesh\\xc2\\xa0- \\xe2\\x80\\xa6\\xc2\\xa0Cybersecurity Risks Produced by Generative AI, 2025\nThis chapter investigates how generative Al techniques transform offensive security. \nIt explores automated methods that identify software flaws, generate targeted \nexploits, and support penetration testers in analyzing systems. Emphasis is placed\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DzORbEQAAQBAJ%26oi%3Dfnd%26pg%3DPA309%26ots%3DVbsP-qp8xv%26sig%3D1IZyz9DVbXZoKPRRB-NIS6_DQWo&hl=en&sa=X&d=17594705992911668952&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b9pqFy2tqVqR-JKv6Zb1gXz&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=7&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Selective Code Generation for Functional Guarantees", "first_label": ["Code"], "second_label": ["Generation"], "data": "J Jeong, T Kim, S Park\\xc2\\xa0- arXiv preprint arXiv:2505.13553, 2025\nLarge language models (LLMs) show human-level performance and their \nspecialized descendants, code generation models, play core roles in solving \ncomplex tasks, including mathematical reasoning and software development. On the\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13553&hl=en&sa=X&d=10618887701165566141&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b88mIfSM5NIHQuMquWHVKnm&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=8&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "Manifesto from Dagstuhl Perspectives Workshop 24452--Reframing Technical Debt", "first_label": [], "second_label": [], "data": "P Avgeriou, I Ozkaya, H Koziolek, Z Codabux, N Ernst\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nThis is the Dagstuhl Perspectives Workshop 24452 manifesto on Reframing \nTechnical Debt. The manifesto begins with a one-page summary of Values, Beliefs, \nand Principles. It then elaborates on each Value, Belief, and Principle to explain their\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nDavid Lo\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13009&hl=en&sa=X&d=13515691248470767835&ei=cMcwaPi2EZ-mieoPoOrjuQU&scisig=AAZF9b_-qPhAs9uhZovsiWcLcd4a&oi=scholaralrt&hist=ylyK0_8AAAAJ:5865787842749446205:AAZF9b9CiGf-firBvlixUlAEJTz9&html=&pos=9&folt=rel", "author": ["David Lo"], "ref": ["David Lo - new related research"]}
{"title": "SVA-ICL: Improving LLM-based Software Vulnerability Assessment via In-Context Learning and Information Fusion", "first_label": ["Vulnerabilities", "LLM"], "second_label": [], "data": "C Gao, X Chen, G Zhang\\xc2\\xa0- arXiv preprint arXiv:2505.10008, 2025\nContext: Software vulnerability assessment (SVA) is critical for identifying, evaluating, \nand prioritizing security weaknesses in software applications. Objective: Despite the \nincreasing application of large language models (LLMs) in various software\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.10008&hl=en&sa=X&d=1264413811272727540&ei=cMcwaOfaG4PFieoPyOahgAU&scisig=AAZF9b8ALX06m_riBYkcPk3CfsTb&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=2&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "CLEVER: A Curated Benchmark for Formally Verified Code Generation", "first_label": ["Code"], "second_label": ["Generation"], "data": "A Thakur, J Lee, G Tsoukalas, M Sistla, M Zhao\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWe introduce ${\\\\rm C {\\\\small LEVER}} $, a high-quality, curated benchmark of 161 \nproblems for end-to-end verified code generation in Lean. Each problem consists of \n(1) the task of generating a specification that matches a held-out ground-truth\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13938&hl=en&sa=X&d=11192071992810668718&ei=cMcwaOfaG4PFieoPyOahgAU&scisig=AAZF9b-C-NTbh5R9b-swAyggAxAH&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=4&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "RTL++: Graph-enhanced LLM for RTL Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation", "Graph"], "data": "M Akyash, K Azar, H Kamali\\xc2\\xa0- arXiv preprint arXiv:2505.13479, 2025\nAs hardware design complexity escalates, there is an urgent need for advanced \nautomation in electronic design automation (EDA). Traditional register transfer level \n(RTL) design methods are manual, time-consuming, and prone to errors. While\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nXin ZHOU\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13479&hl=en&sa=X&d=6042755301333836943&ei=cMcwaOfaG4PFieoPyOahgAU&scisig=AAZF9b91iYjfy-c_GO2nFCC4zu2K&oi=scholaralrt&hist=ylyK0_8AAAAJ:16898579961534012346:AAZF9b8mlk_JgC2UbDdCdga5r9UH&html=&pos=6&folt=rel", "author": ["Xin ZHOU"], "ref": ["Xin ZHOU - new related research"]}
{"title": "Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents", "first_label": [], "second_label": ["Agent", "Search"], "data": "K Zainullina, A Golubev, M Trofimova, S Polezhaev\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have recently achieved remarkable results in \ncomplex multi-step tasks, such as mathematical reasoning and agentic software \nengineering. However, they often struggle to maintain consistent performance across \nmultiple solution attempts. One effective approach to narrow the gap between \naverage-case and best-case performance is guided test-time search, which explores \nmultiple solution paths to identify the most promising one. Unfortunately, effective\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAutoCodeRover: Autonomous program improvement\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.13652&hl=en&sa=X&d=16639264652215845731&ei=cMcwaIOGE_ynieoPhK2buAM&scisig=AAZF9b8rosT55wzCqZ8cohQw-jIG&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=2&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "An Automated Blackbox Noncompliance Checker for QUIC Server Implementations", "first_label": [], "second_label": [], "data": "KK Ang, G Farrelly, C Pope, DC Ranasinghe\\xc2\\xa0- arXiv preprint arXiv:2505.12690, 2025\nWe develop QUICtester, an automated approach for uncovering non-compliant \nbehaviors in the ratified QUIC protocol implementations (RFC 9000/9001). \nQUICtester leverages active automata learning to abstract the behavior of a QUIC \nimplementation into a finite state machine (FSM) representation. Unlike prior \nnoncompliance checking methods, to help uncover state dependencies on event \ntiming, QUICtester introduces the idea of state learning with event timing variations\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model guided protocol fuzzing\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12690&hl=en&sa=X&d=4685541144643323507&ei=cMcwaIOGE_ynieoPhK2buAM&scisig=AAZF9b987F0IhMXLHOwmMTOqWaqI&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=3&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "From LLMs to Randomness: Analyzing Program Input Efficacy with Resource and Language Metrics", "first_label": ["LLM"], "second_label": [], "data": "G Black, E Yocam, V Vaidyan, G Comert, Y Wang\\xc2\\xa0- IEEE Access, 2025\nSecurity-focused program testing typically focuses on crash detection and code \ncoverage while overlooking additional system behaviors that can impact program \nconfidentiality and availability. To address this gap, we propose a statistical \nframework that combines embedding-based anomaly detection, resource usage \nmetrics, and resource-state distance measures to systematically profile software \nbehaviors beyond traditional coverage-based methods. Leveraging over 5 million\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaFuzzing: Challenges and Reflections\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/11006641.pdf&hl=en&sa=X&d=18274979556241197973&ei=cMcwaIOGE_ynieoPhK2buAM&scisig=AAZF9b8kBdUN_IJqKKkeBSLHiFuo&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=4&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Understanding the Sneaky Patterns of Pop-up Windows in the Mobile Ecosystem", "first_label": [], "second_label": [], "data": "D Wu, Y Nan, S Wang, J Wang, L Li, X Wang\\xc2\\xa0- arXiv preprint arXiv:2505.12056, 2025\nIn mobile applications, Pop-up window (PoW) plays a crucial role in improving user \nexperience, guiding user actions, and delivering key information. Unfortunately, the \nexcessive use of PoWs severely degrades the user experience. These PoWs often \nsneakily mislead users in their choices, employing tactics that subtly manipulate \ndecision-making processes. In this paper, we provide the first in-depth study on the \nSneaky patterns in the mobile ecosystem. Our research first highlights five distinct\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaTime-travel Testing of Android Apps\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12056&hl=en&sa=X&d=16052199622639594585&ei=cMcwaIOGE_ynieoPhK2buAM&scisig=AAZF9b_rQ0aUIDzZ_5nB_pjqshQV&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=5&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "Bringing fuzzing capabilities to the Genode Framework", "first_label": ["Fuzzing"], "second_label": [], "data": "S Meier - 2025\nThe Genode OS framework represents a novel operating system architecture that \nhas been developed to address the challenges posed by complexity. It is an open-\nsource tool kit for building highly secure componentbased operating systems and its \nfunctionality extends across a wide range of devices, from those intended for use in \nembedded systems to those designed for dynamic general-purpose computing. \nDespite Genode's selfcharacterisation as a security-oriented operating system, it is\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaCoverage-based greybox fuzzing as markov chain\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://silasmeier.ch/master_thesis/Bringing_Fuzzing_Capabilites_to_the_Genode_Framework.pdf&hl=en&sa=X&d=14511365399994014207&ei=cMcwaIOGE_ynieoPhK2buAM&scisig=AAZF9b9rtZ3uMiuOs-End2tKBmdX&oi=scholaralrt&hist=ylyK0_8AAAAJ:10071049626428824134:AAZF9b8j6D2HAFt59uW8wFlKdfsL&html=&pos=6&folt=cit", "author": ["Abhik Roychoudhury"], "ref": ["7 new citations to articles by Abhik Roychoudhury"]}
{"title": "CHIMERA: Fuzzing P4 Network Infrastructure for Multi-Plane Bug Detection and Vulnerability Discovery", "first_label": ["Vulnerabilities", "Fuzzing", "Bug"], "second_label": ["Detection"], "data": "J Kim, DJ Tian, BE Ujcich\\xc2\\xa0- 2025 IEEE Symposium on Security and Privacy (SP), 2025\nProgrammable network data planes, such as P4, offer flexibility in defining network \nforwarding behavior. However, such programmability introduces a new attack \nsurface for bugs and security vulnerabilities. Most P4 security research has focused\\xc2\\xa0\\xe2\\x80\\xa6", "link": "https://scholar.google.com/scholar_url?url=https://kjw6855.github.io/assets/pdf/sp25_kim_chimera.pdf&hl=en&sa=X&d=6094077905886690143&ei=cMcwaKOOFtHSieoP4tatoQU&scisig=AAZF9b86dvBdClyFEIFpgKdNzAJ4&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=0&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Revisiting Defects4J for Fault Localization in Diverse Development Scenarios", "first_label": ["Fault Localization", "Software Defect"], "second_label": ["Localization"], "data": "MN Rafi, AR Chen, THP Chen, S Wang\nDefects4J stands out as a leading benchmark dataset for software testing research, \nproviding a controlled environment to study real bugs from prominent open-source \nsystems. While Defects4J provides a clean and valuable dataset, we aim to explore\\xc2\\xa0\\xe2\\x80\\xa6\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new articles related to research by \nAbhik Roychoudhury\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://petertsehsun.github.io/papers/MSR25_Defects4j.pdf&hl=en&sa=X&d=17769356187159404178&ei=cMcwaKOOFtHSieoP4tatoQU&scisig=AAZF9b9Jow3u953pAdt-vrSlDqrb&oi=scholaralrt&hist=ylyK0_8AAAAJ:12723761785867032729:AAZF9b9l_z1CTdTcNTkZbRX9RLem&html=&pos=2&folt=rel", "author": ["Abhik Roychoudhury"], "ref": ["Abhik Roychoudhury - new related research"]}
{"title": "Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks", "first_label": ["LLM"], "second_label": ["Agent"], "data": "M Xu, J Fan, X Huang, C Zhou, J Kang, D Niyato\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nWith the continuous evolution of Large Language Models (LLMs), LLM-based agents \nhave advanced beyond passive chatbots to become autonomous cyber entities \ncapable of performing complex tasks, including web browsing, malicious code and \ndeceptive content generation, and decision-making. By significantly reducing the \ntime, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based \nagents have led to a phenomenon termed Cyber Threat Inflation, characterized by a\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLarge language model for vulnerability detection and repair\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12786&hl=en&sa=X&d=5089822906395003663&ei=cMcwaITUGNyZieoP4ti52Q4&scisig=AAZF9b-0xl4uFh8D1fX1kOA-bKoS&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=0&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU", "6 new citations to articles by Richard Fang"]}
{"title": "Do Code LLMs Do Static Analysis?", "first_label": ["LLM", "Code", "Static Analysis"], "second_label": [], "data": "CY Su, C McMillan\\xc2\\xa0- arXiv preprint arXiv:2505.12118, 2025\nThis paper investigates code LLMs' capability of static analysis during code \nintelligence tasks such as code summarization and generation. Code LLMs are now \nhousehold names for their abilities to do some programming tasks that have \nheretofore required people. The process that people follow to do programming tasks \nhas long been understood to require static analysis. For example, human \nprogrammers navigate the call graph of large programs to comprehend the different\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaExploring Parameter-Efficient Fine-Tuning Techniques for Code\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12118&hl=en&sa=X&d=1588101499003976139&ei=cMcwaITUGNyZieoP4ti52Q4&scisig=AAZF9b_BbsCh-WUpc8HvOPjR0t-y&oi=scholaralrt&hist=ylyK0_8AAAAJ:15035864585353249078:AAZF9b__fNdZeFj1p33oPi7SBv6G&html=&pos=1&folt=cit", "author": ["Xin ZHOU"], "ref": ["3 new citations to articles by Xin ZHOU"]}
{"title": "EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection", "first_label": [], "second_label": ["Agent"], "data": "Y Lu, T Ju, M Zhao, X Ma, Y Guo, ZS Zhang\\xc2\\xa0- arXiv preprint arXiv:2505.14289, 2025\nAs multimodal agents are increasingly trained to operate graphical user interfaces \n(GUIs) to complete user tasks, they face a growing threat from indirect prompt \ninjection, attacks in which misleading instructions are embedded into the agent's \nvisual environment, such as popups or chat messages, and misinterpreted as part of \nthe intended task. A typical example is environmental injection, in which GUI \nelements are manipulated to influence agent behavior without directly modifying the\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaAdaptive attacks break defenses against indirect prompt injection\\xc2\\xa0\\xe2\\x80\\xa6\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14289&hl=en&sa=X&d=16078611100021680803&ei=cMcwaISXDdqX6rQP0Kmy0QM&scisig=AAZF9b_L3wIXA3whGse7NRJcxI6N&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=1&folt=cit", "author": ["Richard Fang"], "ref": ["6 new citations to articles by Richard Fang"]}
{"title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets", "first_label": ["LLM"], "second_label": [], "data": "N Lu, S Liu, J Wu, W Chen, Z Zhang, YS Ong, Q Wang\\xe2\\x80\\xa6\\xc2\\xa0- arXiv preprint arXiv\\xc2\\xa0\\xe2\\x80\\xa6, 2025\nLarge language models (LLMs) have shown great potential as general-purpose AI \nassistants across various domains. To fully leverage this potential in specific \napplications, many companies provide fine-tuning API services, enabling users to \nupload their own data for LLM customization. However, fine-tuning services \nintroduce a new safety threat: user-uploaded data, whether harmful or benign, can \nbreak the model's alignment, leading to unsafe outputs. Moreover, existing defense\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12038&hl=en&sa=X&d=11004474244674046700&ei=cMcwaISXDdqX6rQP0Kmy0QM&scisig=AAZF9b8rI7QyhYZq2MBRUtZ3eJeU&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=2&folt=cit", "author": ["Richard Fang"], "ref": ["6 new citations to articles by Richard Fang"]}
{"title": "Automated Profile Inference with Language Model Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "Y Du, Z Li, B Ding, Y Li, H Xiao, J Zhou, N Li\\xc2\\xa0- arXiv preprint arXiv:2505.12402, 2025\nImpressive progress has been made in automated problem-solving by the \ncollaboration of large language models (LLMs) based agents. However, these \nautomated capabilities also open avenues for malicious applications. In this paper, \nwe study a new threat that LLMs pose to online pseudonymity, called automated \nprofile inference, where an adversary can instruct LLMs to automatically scrape and \nextract sensitive personal attributes from publicly visible user activities on\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLlm agents can autonomously hack websites\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.12402&hl=en&sa=X&d=3098044590675759444&ei=cMcwaISXDdqX6rQP0Kmy0QM&scisig=AAZF9b_VFVTNesGvipUIGp2Ayfnv&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=3&folt=cit", "author": ["Richard Fang"], "ref": ["6 new citations to articles by Richard Fang"]}
{"title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem", "first_label": ["LLM"], "second_label": ["Agent"], "data": "F Liu, Z Yang, C Liu, T Song, X Gao, H Liu\\xc2\\xa0- arXiv preprint arXiv:2505.14148, 2025\nMathematical modeling is a cornerstone of scientific discovery and engineering \npractice, enabling the translation of real-world problems into formal systems across \ndomains such as physics, biology, and economics. Unlike mathematical reasoning, \nwhich assumes a predefined formulation, modeling requires open-ended problem \nanalysis, abstraction, and principled formalization. While Large Language Models \n(LLMs) have shown strong reasoning capabilities, they fall short in rigorous model\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaLlm agents can autonomously hack websites\\xe2\\x80\\xac", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14148&hl=en&sa=X&d=3052781000511687798&ei=cMcwaISXDdqX6rQP0Kmy0QM&scisig=AAZF9b8jSgczWE3UNkaupWwf4NV0&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=4&folt=cit", "author": ["Richard Fang"], "ref": ["6 new citations to articles by Richard Fang"]}
{"title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "first_label": [], "second_label": [], "data": "K Ponkshe, S Shah, R Singhal, P Vepakomma\\xc2\\xa0- arXiv preprint arXiv:2505.14185, 2025\nLarge Language Models (LLMs) rely on safety alignment to produce socially \nacceptable responses. This is typically achieved through instruction tuning and \nreinforcement learning from human feedback. However, this alignment is known to \nbe brittle: further fine-tuning, even on benign or lightly contaminated data, can \ndegrade safety and reintroduce harmful behaviors. A growing body of work suggests \nthat alignment may correspond to identifiable geometric directions in weight space\\xc2\\xa0\\xe2\\x80\\xa6\n\\xe2\\x80\\xa2\nCites: \\xe2\\x80\\xaaRemoving rlhf protections in gpt-4 via fine-tuning\\xe2\\x80\\xac\u00a0\u00a0\n\u00a0\nThis message was sent by Google Scholar because you\\'re following new citations to articles written by \nRichard Fang\n.\nList alerts\nCancel alert\n\\r\\n'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2505.14185&hl=en&sa=X&d=8896531855132265957&ei=cMcwaISXDdqX6rQP0Kmy0QM&scisig=AAZF9b8zIJ0U5rhDxwMqHE3xvznx&oi=scholaralrt&hist=ylyK0_8AAAAJ:4436498698466669065:AAZF9b-6dRec6PGUxNGKd2t3_e20&html=&pos=5&folt=cit", "author": ["Richard Fang"], "ref": ["6 new citations to articles by Richard Fang"]}

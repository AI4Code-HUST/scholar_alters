# Scholar Alters
Parse unread emails from Google Scholar alerts and update README.md

## Setting for first use
You should need to use [Gmail API Client Library](https://developers.google.com/gmail/api/quickstart/python) and create
credentials.json as explained in the link.

## Run
Run from your CLI:
```
bash script.sh
```
## Papers

| Topic | Branch | Papers |
| --- | --- | --- |
| Vulnerabilities, Code | Detection | [Enhancing Security in Third-Party Library Reuse--Comprehensive Detection of 1-day Vulnerability through Code Patch Analysis](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.19648&hl=vi&sa=X&d=16777664686099468318&ei=64BRZ9a7K6Ox6rQPwdys2AM&scisig=AFWwaeaLU453tSQ105Kdshh4-2a5&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=0&folt=cit) |
| Smart Contracts, Code |  | [EAOS: Exposing attacks in smart contracts through analyzing opcode sequences with operands](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S1389128624007916&hl=vi&sa=X&d=13123050235184174189&ei=64BRZ9a7K6Ox6rQPwdys2AM&scisig=AFWwaeYwb5mJxGqng8L_9YkZw-Oq&oi=scholaralrt&hist=apJ4fD8AAAAJ:13534924455939102554:AFWwaeZN-y-gtbFtywJ0Xio3nYxl&html=&pos=1&folt=cit) |
|  |  | [Sneaking Syntax into Transformer Language Models with Tree Regularization](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.18885&hl=en&sa=X&d=876159909466854783&ei=64BRZ7rEJJWA6rQP8Kz0OQ&scisig=AFWwaeatOZlc_ZumFp6w21NLU_AY&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel) |
|  |  | [Metaaligner: Towards generalizable multi-objective alignment of language models](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DdIVb5C0QFf&hl=en&sa=X&d=13438162666525277483&ei=64BRZ7rEJJWA6rQP8Kz0OQ&scisig=AFWwaeZUV7KhqsVT3QwRCZRb--XN&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel) |
| Large Language Models |  | [Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.02063&hl=en&sa=X&d=11598431771733194121&ei=64BRZ7rEJJWA6rQP8Kz0OQ&scisig=AFWwaeYDwJN2F3ZIMRF_Ewl77sfi&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel) |
| Large Language Models, Code |  | [On the Adversarial Robustness of Instruction-Tuned Large Language Models for Code](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.19508&hl=en&sa=X&d=11437267188537543815&ei=64BRZ7rEJJWA6rQP8Kz0OQ&scisig=AFWwaeZEQUlB1X3h6khTP49R2C0J&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel) |
| Large Language Models, Code | Generation | [VeCoGen: Automating Generation of Formally Verified C Code with Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.19275&hl=en&sa=X&d=8618617223234416721&ei=64BRZ9DEKO6N6rQP7qvQsA4&scisig=AFWwaeb1yIpDGU3gzsPnpKX5f-RR&oi=scholaralrt&hist=apJ4fD8AAAAJ:10695555881282652625:AFWwaeakbu5Ta3HmdjfVean1AXL4&html=&pos=0&folt=cit) |
| Large Language Models |  | [Marconi: Prefix Caching for the Era of Hybrid LLMs](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.19379&hl=en&sa=X&d=18318239950053593333&ei=64BRZ5bzLNyj6rQP9eTrsQo&scisig=AFWwaea-yDc67Oc-ybwy5K4FI06_&oi=scholaralrt&hist=apJ4fD8AAAAJ:16237994392044955269:AFWwaebaLgrVcMkfKx1Gjt1mqPQn&html=&pos=0&folt=cit) |

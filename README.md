# Scholar Alters
Parse unread emails from Google Scholar alerts and update README.md

## Setting for first use
You should need to use [Gmail API Client Library](https://developers.google.com/gmail/api/quickstart/python) and create
credentials.json as explained in the link.

## Run
Run from your CLI:
```
bash script.sh
```
## Papers

| Topic | Branch | Papers |
| --- | --- | --- |
| Large Language Models |  | [Developer Challenges on Large Language Models: A Study of Stack Overflow and OpenAI Developer Forum Posts](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.10873&hl=en&sa=X&d=4802779475487097878&ei=lSlQZ5ihL76_y9YP2MmluAw&scisig=AFWwaeZmcQsLh4FU9RIP0gysCkgQ&oi=scholaralrt&hist=apJ4fD8AAAAJ:8900472388513427833:AFWwaeZM7Y6I9R2ROVLnk31jdyVz&html=&pos=0&folt=rel) |
|  |  | [Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.05193&hl=en&sa=X&d=10279417018827333719&ei=lSlQZ9_aLdXEy9YPxYiS6QU&scisig=AFWwaebTIZpRDM32NyzjnnOI4XBk&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=0&folt=rel) |
|  |  | [Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2411.04282%3F&hl=en&sa=X&d=13952632409504715879&ei=lSlQZ9_aLdXEy9YPxYiS6QU&scisig=AFWwaebCHXUOq4TE9RtaFXUspR_q&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=1&folt=rel) |
|  |  | [How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider and MoE Transformers](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D67tRrjgzsh&hl=en&sa=X&d=13078570161380630146&ei=lSlQZ9_aLdXEy9YPxYiS6QU&scisig=AFWwaebgwbljL9gmYkzCkudOlKGB&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=2&folt=rel) |
|  |  | [Accelerating Blockwise Parallel Language Models with Draft Refinement](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DKT6F5Sw0eg&hl=en&sa=X&d=18375017579044240923&ei=lSlQZ9_aLdXEy9YPxYiS6QU&scisig=AFWwaeb_4I1STgj0KmHx5o1tyN0B&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=3&folt=rel) |
| Large Language Models |  | [SparseLLM: Towards Global Pruning of Pre-trained Language Models](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DoXHyYHp4Zb&hl=en&sa=X&d=2570638658173911419&ei=lSlQZ9_aLdXEy9YPxYiS6QU&scisig=AFWwaebxUUzGN-j6qmF6-vPv-sND&oi=scholaralrt&hist=apJ4fD8AAAAJ:3096313017463695374:AFWwaeb8R4GEV1B4xk_Cz2b6H7gj&html=&pos=4&folt=rel) |
